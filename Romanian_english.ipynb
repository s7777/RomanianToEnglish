{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNeOAY2ZT1pY",
        "outputId": "911bfa42-11b2-49e6-98fc-8d84f1566944"
      },
      "source": [
        "import string\n",
        "import re\n",
        "from pickle import dump\n",
        "from unicodedata import normalize\n",
        "from numpy import array\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, mode='rt', encoding='utf-8')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "# split a loaded document into sentences\n",
        "def to_pairs(doc):\n",
        "\tlines = doc.strip().split('\\n')\n",
        "\tpairs = [line.split('\\t') for line in  lines]\n",
        "\treturn pairs\n",
        "\n",
        "# clean a list of lines\n",
        "def clean_pairs(lines):\n",
        "\tcleaned = list()\n",
        "\t# prepare regex for char filtering\n",
        "\tre_print = re.compile('[^%s]' % re.escape(string.printable))\n",
        "\t# prepare translation table for removing punctuation\n",
        "\ttable = str.maketrans('', '', string.punctuation)\n",
        "\tfor pair in lines:\n",
        "\t\tclean_pair = list()\n",
        "\t\tfor line in pair:\n",
        "\t\t\t# normalize unicode characters\n",
        "\t\t\tline = normalize('NFD', line).encode('ascii', 'ignore')\n",
        "\t\t\tline = line.decode('UTF-8')\n",
        "\t\t\t# tokenize on white space\n",
        "\t\t\tline = line.split()\n",
        "\t\t\t# convert to lowercase\n",
        "\t\t\tline = [word.lower() for word in line]\n",
        "\t\t\t# remove punctuation from each token\n",
        "\t\t\tline = [word.translate(table) for word in line]\n",
        "\t\t\t# remove non-printable chars form each token\n",
        "\t\t\tline = [re_print.sub('', w) for w in line]\n",
        "\t\t\t# remove tokens with numbers in them\n",
        "\t\t\tline = [word for word in line if word.isalpha()]\n",
        "\t\t\t# store as string\n",
        "\t\t\tclean_pair.append(' '.join(line))\n",
        "\t\tcleaned.append(clean_pair)\n",
        "\treturn array(cleaned)\n",
        "\n",
        "# save a list of clean sentences to file\n",
        "def save_clean_data(sentences, filename):\n",
        "\tdump(sentences, open(filename, 'wb'))\n",
        "\tprint('Saved: %s' % filename)\n",
        "\n",
        "# load dataset\n",
        "filename = 'ron.txt'\n",
        "doc = load_doc(filename)\n",
        "# split into english-german pairs\n",
        "pairs = to_pairs(doc)\n",
        "# clean sentences\n",
        "clean_pairs = clean_pairs(pairs)\n",
        "# save clean pairs to file\n",
        "save_clean_data(clean_pairs, 'english-romanian.pkl')\n",
        "# spot check\n",
        "for i in range(100):\n",
        "\tprint('[%s] => [%s]' % (clean_pairs[i,0], clean_pairs[i,1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved: english-romanian.pkl\n",
            "[hi] => [buna]\n",
            "[run] => [fugi]\n",
            "[who] => [cine]\n",
            "[fire] => [foc]\n",
            "[help] => [ajutor]\n",
            "[jump] => [sari]\n",
            "[stop] => [oprestete]\n",
            "[wait] => [asteapta]\n",
            "[hello] => [salut]\n",
            "[hello] => [buna]\n",
            "[hurry] => [haide]\n",
            "[hurry] => [grabestete]\n",
            "[relax] => [calmeazate]\n",
            "[smile] => [zambeste]\n",
            "[attack] => [ataca]\n",
            "[attack] => [atacati]\n",
            "[cheers] => [noroc]\n",
            "[cheers] => [sanatate]\n",
            "[cheers] => [noroc bun]\n",
            "[freeze] => [stai pe loc]\n",
            "[get up] => [ridicate]\n",
            "[really] => [serios]\n",
            "[ask tom] => [intreabal pe tom]\n",
            "[awesome] => [maxim]\n",
            "[awesome] => [excelent]\n",
            "[call me] => [sunama]\n",
            "[get out] => [pleaca de aici]\n",
            "[get out] => [pleaca de aici]\n",
            "[go away] => [pleaca]\n",
            "[go away] => [lasama]\n",
            "[goodbye] => [la revedere]\n",
            "[hold on] => [rezista]\n",
            "[i agree] => [sunt de acord]\n",
            "[im tom] => [sunt tom]\n",
            "[im ill] => [sunt bolnav]\n",
            "[im sad] => [sunt trist]\n",
            "[its ok] => [e ok]\n",
            "[keep it] => [pastreazo]\n",
            "[keep it] => [pastreazal]\n",
            "[open up] => [deschide]\n",
            "[perfect] => [perfect]\n",
            "[tell me] => [spunemi]\n",
            "[why not] => [de ce nu]\n",
            "[grab him] => [apucate de el]\n",
            "[how cute] => [ce dragut]\n",
            "[how cute] => [ce simpatic]\n",
            "[hurry up] => [grabestete]\n",
            "[ill pay] => [eu voi plati]\n",
            "[im back] => [mam intors]\n",
            "[im back] => [am revenit]\n",
            "[im calm] => [sunt calm]\n",
            "[im free] => [eu sunt liber]\n",
            "[im here] => [sunt aici]\n",
            "[im here] => [eu sunt aici]\n",
            "[im home] => [sunt acasa]\n",
            "[im numb] => [sunt indiferent]\n",
            "[im numb] => [sunt indiferenta]\n",
            "[im sick] => [sunt bolnav]\n",
            "[it hurts] => [ma doare]\n",
            "[its tom] => [este tom]\n",
            "[keep out] => [nu intra]\n",
            "[marry me] => [casatorestete cu mine]\n",
            "[may i go] => [pot sa merg]\n",
            "[may i go] => [pot sa ma duc]\n",
            "[terrific] => [maxim]\n",
            "[terrific] => [teribil]\n",
            "[tom fled] => [tom a fugit]\n",
            "[tom left] => [tom a plecat]\n",
            "[too late] => [prea tarziu]\n",
            "[trust me] => [ai incredere in mine]\n",
            "[use this] => [utilizeaza asta]\n",
            "[who fell] => [cine a cazut]\n",
            "[who paid] => [cine a platit]\n",
            "[bless you] => [noroc]\n",
            "[bless you] => [sanatate]\n",
            "[bless you] => [noroc bun]\n",
            "[calm down] => [calmeazate]\n",
            "[come back] => [intoarcete]\n",
            "[come back] => [revino]\n",
            "[come back] => [reveniti]\n",
            "[come back] => [intoarcetiva]\n",
            "[come home] => [vino acasa]\n",
            "[dont ask] => [nu intreba]\n",
            "[fantastic] => [maxim]\n",
            "[he is ill] => [el este bolnav]\n",
            "[hes fast] => [el e rapid]\n",
            "[i am sick] => [sunt bolnav]\n",
            "[i beg you] => [va rog]\n",
            "[i can ski] => [pot sa schiez]\n",
            "[i can win] => [eu pot castiga]\n",
            "[i hope so] => [sper]\n",
            "[i will go] => [eu voi merge]\n",
            "[ill lose] => [voi pierde]\n",
            "[im bored] => [sunt plictisit]\n",
            "[im bored] => [mam plictisit]\n",
            "[im drunk] => [sunt beat]\n",
            "[im happy] => [sunt fericit]\n",
            "[im sorry] => [imi pare rau]\n",
            "[im tired] => [sunt obosit]\n",
            "[im tired] => [sunt obosita]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKNbh6zgVXjb",
        "outputId": "943fdd61-e0af-4a21-caa2-e0c675cb2697"
      },
      "source": [
        "from pickle import load\n",
        "from pickle import dump\n",
        "from numpy.random import rand\n",
        "from numpy.random import shuffle\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# save a list of clean sentences to file\n",
        "def save_clean_data(sentences, filename):\n",
        "\tdump(sentences, open(filename, 'wb'))\n",
        "\tprint('Saved: %s' % filename)\n",
        "\n",
        "# load dataset\n",
        "raw_dataset = load_clean_sentences('english-romanian.pkl')\n",
        "\n",
        "# reduce dataset size\n",
        "n_sentences = 10000\n",
        "dataset = raw_dataset[:n_sentences, :]\n",
        "# random shuffle\n",
        "shuffle(dataset)\n",
        "# split into train/test\n",
        "train, test = dataset[:9000], dataset[1000:]\n",
        "# save\n",
        "save_clean_data(dataset, 'english-romanian-both.pkl')\n",
        "save_clean_data(train, 'english-romanian-train.pkl')\n",
        "save_clean_data(test, 'english-romanian-test.pkl')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved: english-romanian-both.pkl\n",
            "Saved: english-romanian-train.pkl\n",
            "Saved: english-romanian-test.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6NqPWK-VryP",
        "outputId": "7429242a-05be-4151-d9d8-7b2215c6106f"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import RepeatVector\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# one hot encode target sequence\n",
        "def encode_output(sequences, vocab_size):\n",
        "\tylist = list()\n",
        "\tfor sequence in sequences:\n",
        "\t\tencoded = to_categorical(sequence, num_classes=vocab_size)\n",
        "\t\tylist.append(encoded)\n",
        "\ty = array(ylist)\n",
        "\ty = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
        "\treturn y\n",
        "\n",
        "# define NMT model\n",
        "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
        "\tmodel.add(LSTM(n_units))\n",
        "\tmodel.add(RepeatVector(tar_timesteps))\n",
        "\tmodel.add(LSTM(n_units, return_sequences=True))\n",
        "\tmodel.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
        "\treturn model\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-romanian-both.pkl')\n",
        "train = load_clean_sentences('english-romanian-train.pkl')\n",
        "test = load_clean_sentences('english-romanian-test.pkl')\n",
        "\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:,0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
        "print('English Max Length: %d' % (eng_length))\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "print('Romanian Vocabulary Size: %d' % ger_vocab_size)\n",
        "print('Romanian  Max Length: %d' % (ger_length))\n",
        "\n",
        "# prepare training data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
        "trainY = encode_output(trainY, eng_vocab_size)\n",
        "# prepare validation data\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
        "testY = encode_output(testY, eng_vocab_size)\n",
        "\n",
        "# define model\n",
        "model = define_model(ger_vocab_size, eng_vocab_size, ger_length, eng_length, 256)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "# summarize defined model\n",
        "print(model.summary())\n",
        "plot_model(model, to_file='model.png', show_shapes=True)\n",
        "# fit model\n",
        "filename = 'model.h5'\n",
        "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "model.fit(trainX, trainY, epochs=300, batch_size=64, validation_data=(testX, testY), callbacks=[checkpoint], verbose=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English Vocabulary Size: 4297\n",
            "English Max Length: 10\n",
            "Romanian Vocabulary Size: 6121\n",
            "Romanian  Max Length: 14\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 14, 256)           1566976   \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 256)               525312    \n",
            "_________________________________________________________________\n",
            "repeat_vector (RepeatVector) (None, 10, 256)           0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 10, 256)           525312    \n",
            "_________________________________________________________________\n",
            "time_distributed (TimeDistri (None, 10, 4297)          1104329   \n",
            "=================================================================\n",
            "Total params: 3,721,929\n",
            "Trainable params: 3,721,929\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/300\n",
            "141/141 - 67s - loss: 4.1603 - val_loss: 3.4185\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 3.41850, saving model to model.h5\n",
            "Epoch 2/300\n",
            "141/141 - 60s - loss: 3.3641 - val_loss: 3.2516\n",
            "\n",
            "Epoch 00002: val_loss improved from 3.41850 to 3.25161, saving model to model.h5\n",
            "Epoch 3/300\n",
            "141/141 - 60s - loss: 3.2394 - val_loss: 3.1604\n",
            "\n",
            "Epoch 00003: val_loss improved from 3.25161 to 3.16036, saving model to model.h5\n",
            "Epoch 4/300\n",
            "141/141 - 60s - loss: 3.1685 - val_loss: 3.1307\n",
            "\n",
            "Epoch 00004: val_loss improved from 3.16036 to 3.13071, saving model to model.h5\n",
            "Epoch 5/300\n",
            "141/141 - 61s - loss: 3.0848 - val_loss: 3.0271\n",
            "\n",
            "Epoch 00005: val_loss improved from 3.13071 to 3.02709, saving model to model.h5\n",
            "Epoch 6/300\n",
            "141/141 - 61s - loss: 3.0328 - val_loss: 2.9959\n",
            "\n",
            "Epoch 00006: val_loss improved from 3.02709 to 2.99586, saving model to model.h5\n",
            "Epoch 7/300\n",
            "141/141 - 61s - loss: 2.9993 - val_loss: 2.9628\n",
            "\n",
            "Epoch 00007: val_loss improved from 2.99586 to 2.96277, saving model to model.h5\n",
            "Epoch 8/300\n",
            "141/141 - 60s - loss: 2.9641 - val_loss: 2.9333\n",
            "\n",
            "Epoch 00008: val_loss improved from 2.96277 to 2.93332, saving model to model.h5\n",
            "Epoch 9/300\n",
            "141/141 - 60s - loss: 2.9294 - val_loss: 2.8994\n",
            "\n",
            "Epoch 00009: val_loss improved from 2.93332 to 2.89945, saving model to model.h5\n",
            "Epoch 10/300\n",
            "141/141 - 61s - loss: 2.8864 - val_loss: 2.8530\n",
            "\n",
            "Epoch 00010: val_loss improved from 2.89945 to 2.85298, saving model to model.h5\n",
            "Epoch 11/300\n",
            "141/141 - 60s - loss: 2.8328 - val_loss: 2.7956\n",
            "\n",
            "Epoch 00011: val_loss improved from 2.85298 to 2.79562, saving model to model.h5\n",
            "Epoch 12/300\n",
            "141/141 - 61s - loss: 2.7689 - val_loss: 2.7269\n",
            "\n",
            "Epoch 00012: val_loss improved from 2.79562 to 2.72687, saving model to model.h5\n",
            "Epoch 13/300\n",
            "141/141 - 60s - loss: 2.6960 - val_loss: 2.6547\n",
            "\n",
            "Epoch 00013: val_loss improved from 2.72687 to 2.65472, saving model to model.h5\n",
            "Epoch 14/300\n",
            "141/141 - 60s - loss: 2.6316 - val_loss: 2.6158\n",
            "\n",
            "Epoch 00014: val_loss improved from 2.65472 to 2.61583, saving model to model.h5\n",
            "Epoch 15/300\n",
            "141/141 - 61s - loss: 2.5830 - val_loss: 2.5523\n",
            "\n",
            "Epoch 00015: val_loss improved from 2.61583 to 2.55234, saving model to model.h5\n",
            "Epoch 16/300\n",
            "141/141 - 61s - loss: 2.5202 - val_loss: 2.4926\n",
            "\n",
            "Epoch 00016: val_loss improved from 2.55234 to 2.49256, saving model to model.h5\n",
            "Epoch 17/300\n",
            "141/141 - 61s - loss: 2.4561 - val_loss: 2.4318\n",
            "\n",
            "Epoch 00017: val_loss improved from 2.49256 to 2.43178, saving model to model.h5\n",
            "Epoch 18/300\n",
            "141/141 - 61s - loss: 2.4015 - val_loss: 2.3888\n",
            "\n",
            "Epoch 00018: val_loss improved from 2.43178 to 2.38876, saving model to model.h5\n",
            "Epoch 19/300\n",
            "141/141 - 61s - loss: 2.3438 - val_loss: 2.3468\n",
            "\n",
            "Epoch 00019: val_loss improved from 2.38876 to 2.34681, saving model to model.h5\n",
            "Epoch 20/300\n",
            "141/141 - 61s - loss: 2.2825 - val_loss: 2.2735\n",
            "\n",
            "Epoch 00020: val_loss improved from 2.34681 to 2.27354, saving model to model.h5\n",
            "Epoch 21/300\n",
            "141/141 - 61s - loss: 2.2214 - val_loss: 2.2137\n",
            "\n",
            "Epoch 00021: val_loss improved from 2.27354 to 2.21367, saving model to model.h5\n",
            "Epoch 22/300\n",
            "141/141 - 61s - loss: 2.1600 - val_loss: 2.1555\n",
            "\n",
            "Epoch 00022: val_loss improved from 2.21367 to 2.15555, saving model to model.h5\n",
            "Epoch 23/300\n",
            "141/141 - 61s - loss: 2.0926 - val_loss: 2.0878\n",
            "\n",
            "Epoch 00023: val_loss improved from 2.15555 to 2.08784, saving model to model.h5\n",
            "Epoch 24/300\n",
            "141/141 - 61s - loss: 2.0182 - val_loss: 2.0318\n",
            "\n",
            "Epoch 00024: val_loss improved from 2.08784 to 2.03182, saving model to model.h5\n",
            "Epoch 25/300\n",
            "141/141 - 61s - loss: 1.9473 - val_loss: 1.9530\n",
            "\n",
            "Epoch 00025: val_loss improved from 2.03182 to 1.95302, saving model to model.h5\n",
            "Epoch 26/300\n",
            "141/141 - 61s - loss: 1.8799 - val_loss: 1.8987\n",
            "\n",
            "Epoch 00026: val_loss improved from 1.95302 to 1.89875, saving model to model.h5\n",
            "Epoch 27/300\n",
            "141/141 - 61s - loss: 1.8124 - val_loss: 1.8315\n",
            "\n",
            "Epoch 00027: val_loss improved from 1.89875 to 1.83150, saving model to model.h5\n",
            "Epoch 28/300\n",
            "141/141 - 60s - loss: 1.7404 - val_loss: 1.7661\n",
            "\n",
            "Epoch 00028: val_loss improved from 1.83150 to 1.76611, saving model to model.h5\n",
            "Epoch 29/300\n",
            "141/141 - 60s - loss: 1.6710 - val_loss: 1.7100\n",
            "\n",
            "Epoch 00029: val_loss improved from 1.76611 to 1.70998, saving model to model.h5\n",
            "Epoch 30/300\n",
            "141/141 - 60s - loss: 1.6060 - val_loss: 1.6528\n",
            "\n",
            "Epoch 00030: val_loss improved from 1.70998 to 1.65277, saving model to model.h5\n",
            "Epoch 31/300\n",
            "141/141 - 61s - loss: 1.5403 - val_loss: 1.5926\n",
            "\n",
            "Epoch 00031: val_loss improved from 1.65277 to 1.59262, saving model to model.h5\n",
            "Epoch 32/300\n",
            "141/141 - 61s - loss: 1.4817 - val_loss: 1.5503\n",
            "\n",
            "Epoch 00032: val_loss improved from 1.59262 to 1.55028, saving model to model.h5\n",
            "Epoch 33/300\n",
            "141/141 - 61s - loss: 1.4227 - val_loss: 1.4931\n",
            "\n",
            "Epoch 00033: val_loss improved from 1.55028 to 1.49310, saving model to model.h5\n",
            "Epoch 34/300\n",
            "141/141 - 61s - loss: 1.3581 - val_loss: 1.4312\n",
            "\n",
            "Epoch 00034: val_loss improved from 1.49310 to 1.43122, saving model to model.h5\n",
            "Epoch 35/300\n",
            "141/141 - 61s - loss: 1.2988 - val_loss: 1.3802\n",
            "\n",
            "Epoch 00035: val_loss improved from 1.43122 to 1.38016, saving model to model.h5\n",
            "Epoch 36/300\n",
            "141/141 - 61s - loss: 1.2478 - val_loss: 1.3582\n",
            "\n",
            "Epoch 00036: val_loss improved from 1.38016 to 1.35819, saving model to model.h5\n",
            "Epoch 37/300\n",
            "141/141 - 61s - loss: 1.1906 - val_loss: 1.2793\n",
            "\n",
            "Epoch 00037: val_loss improved from 1.35819 to 1.27932, saving model to model.h5\n",
            "Epoch 38/300\n",
            "141/141 - 60s - loss: 1.1323 - val_loss: 1.2358\n",
            "\n",
            "Epoch 00038: val_loss improved from 1.27932 to 1.23578, saving model to model.h5\n",
            "Epoch 39/300\n",
            "141/141 - 61s - loss: 1.0876 - val_loss: 1.2167\n",
            "\n",
            "Epoch 00039: val_loss improved from 1.23578 to 1.21668, saving model to model.h5\n",
            "Epoch 40/300\n",
            "141/141 - 61s - loss: 1.0450 - val_loss: 1.1628\n",
            "\n",
            "Epoch 00040: val_loss improved from 1.21668 to 1.16278, saving model to model.h5\n",
            "Epoch 41/300\n",
            "141/141 - 61s - loss: 0.9918 - val_loss: 1.1204\n",
            "\n",
            "Epoch 00041: val_loss improved from 1.16278 to 1.12038, saving model to model.h5\n",
            "Epoch 42/300\n",
            "141/141 - 61s - loss: 0.9405 - val_loss: 1.0760\n",
            "\n",
            "Epoch 00042: val_loss improved from 1.12038 to 1.07600, saving model to model.h5\n",
            "Epoch 43/300\n",
            "141/141 - 61s - loss: 0.8925 - val_loss: 1.0428\n",
            "\n",
            "Epoch 00043: val_loss improved from 1.07600 to 1.04279, saving model to model.h5\n",
            "Epoch 44/300\n",
            "141/141 - 61s - loss: 0.8493 - val_loss: 1.0125\n",
            "\n",
            "Epoch 00044: val_loss improved from 1.04279 to 1.01254, saving model to model.h5\n",
            "Epoch 45/300\n",
            "141/141 - 61s - loss: 0.8108 - val_loss: 0.9762\n",
            "\n",
            "Epoch 00045: val_loss improved from 1.01254 to 0.97623, saving model to model.h5\n",
            "Epoch 46/300\n",
            "141/141 - 61s - loss: 0.7714 - val_loss: 0.9399\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.97623 to 0.93986, saving model to model.h5\n",
            "Epoch 47/300\n",
            "141/141 - 61s - loss: 0.7318 - val_loss: 0.9150\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.93986 to 0.91501, saving model to model.h5\n",
            "Epoch 48/300\n",
            "141/141 - 60s - loss: 0.6971 - val_loss: 0.8994\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.91501 to 0.89937, saving model to model.h5\n",
            "Epoch 49/300\n",
            "141/141 - 60s - loss: 0.6656 - val_loss: 0.8582\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.89937 to 0.85820, saving model to model.h5\n",
            "Epoch 50/300\n",
            "141/141 - 61s - loss: 0.6281 - val_loss: 0.8267\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.85820 to 0.82669, saving model to model.h5\n",
            "Epoch 51/300\n",
            "141/141 - 60s - loss: 0.5963 - val_loss: 0.8038\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.82669 to 0.80377, saving model to model.h5\n",
            "Epoch 52/300\n",
            "141/141 - 61s - loss: 0.5721 - val_loss: 0.7845\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.80377 to 0.78452, saving model to model.h5\n",
            "Epoch 53/300\n",
            "141/141 - 60s - loss: 0.5434 - val_loss: 0.7589\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.78452 to 0.75892, saving model to model.h5\n",
            "Epoch 54/300\n",
            "141/141 - 61s - loss: 0.5153 - val_loss: 0.7389\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.75892 to 0.73892, saving model to model.h5\n",
            "Epoch 55/300\n",
            "141/141 - 61s - loss: 0.4866 - val_loss: 0.7228\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.73892 to 0.72278, saving model to model.h5\n",
            "Epoch 56/300\n",
            "141/141 - 61s - loss: 0.4609 - val_loss: 0.7017\n",
            "\n",
            "Epoch 00056: val_loss improved from 0.72278 to 0.70172, saving model to model.h5\n",
            "Epoch 57/300\n",
            "141/141 - 62s - loss: 0.4407 - val_loss: 0.6911\n",
            "\n",
            "Epoch 00057: val_loss improved from 0.70172 to 0.69109, saving model to model.h5\n",
            "Epoch 58/300\n",
            "141/141 - 61s - loss: 0.4216 - val_loss: 0.6697\n",
            "\n",
            "Epoch 00058: val_loss improved from 0.69109 to 0.66973, saving model to model.h5\n",
            "Epoch 59/300\n",
            "141/141 - 61s - loss: 0.3986 - val_loss: 0.6498\n",
            "\n",
            "Epoch 00059: val_loss improved from 0.66973 to 0.64980, saving model to model.h5\n",
            "Epoch 60/300\n",
            "141/141 - 61s - loss: 0.3749 - val_loss: 0.6375\n",
            "\n",
            "Epoch 00060: val_loss improved from 0.64980 to 0.63746, saving model to model.h5\n",
            "Epoch 61/300\n",
            "141/141 - 60s - loss: 0.3563 - val_loss: 0.6160\n",
            "\n",
            "Epoch 00061: val_loss improved from 0.63746 to 0.61602, saving model to model.h5\n",
            "Epoch 62/300\n",
            "141/141 - 61s - loss: 0.3343 - val_loss: 0.6041\n",
            "\n",
            "Epoch 00062: val_loss improved from 0.61602 to 0.60414, saving model to model.h5\n",
            "Epoch 63/300\n",
            "141/141 - 60s - loss: 0.3164 - val_loss: 0.5904\n",
            "\n",
            "Epoch 00063: val_loss improved from 0.60414 to 0.59037, saving model to model.h5\n",
            "Epoch 64/300\n",
            "141/141 - 60s - loss: 0.2962 - val_loss: 0.5774\n",
            "\n",
            "Epoch 00064: val_loss improved from 0.59037 to 0.57743, saving model to model.h5\n",
            "Epoch 65/300\n",
            "141/141 - 60s - loss: 0.2845 - val_loss: 0.5826\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.57743\n",
            "Epoch 66/300\n",
            "141/141 - 60s - loss: 0.2807 - val_loss: 0.5664\n",
            "\n",
            "Epoch 00066: val_loss improved from 0.57743 to 0.56638, saving model to model.h5\n",
            "Epoch 67/300\n",
            "141/141 - 61s - loss: 0.2695 - val_loss: 0.5601\n",
            "\n",
            "Epoch 00067: val_loss improved from 0.56638 to 0.56009, saving model to model.h5\n",
            "Epoch 68/300\n",
            "141/141 - 60s - loss: 0.2502 - val_loss: 0.5390\n",
            "\n",
            "Epoch 00068: val_loss improved from 0.56009 to 0.53901, saving model to model.h5\n",
            "Epoch 69/300\n",
            "141/141 - 60s - loss: 0.2304 - val_loss: 0.5286\n",
            "\n",
            "Epoch 00069: val_loss improved from 0.53901 to 0.52855, saving model to model.h5\n",
            "Epoch 70/300\n",
            "141/141 - 60s - loss: 0.2142 - val_loss: 0.5190\n",
            "\n",
            "Epoch 00070: val_loss improved from 0.52855 to 0.51896, saving model to model.h5\n",
            "Epoch 71/300\n",
            "141/141 - 60s - loss: 0.2021 - val_loss: 0.5078\n",
            "\n",
            "Epoch 00071: val_loss improved from 0.51896 to 0.50779, saving model to model.h5\n",
            "Epoch 72/300\n",
            "141/141 - 60s - loss: 0.1946 - val_loss: 0.5066\n",
            "\n",
            "Epoch 00072: val_loss improved from 0.50779 to 0.50664, saving model to model.h5\n",
            "Epoch 73/300\n",
            "141/141 - 61s - loss: 0.1855 - val_loss: 0.4996\n",
            "\n",
            "Epoch 00073: val_loss improved from 0.50664 to 0.49955, saving model to model.h5\n",
            "Epoch 74/300\n",
            "141/141 - 61s - loss: 0.1744 - val_loss: 0.4923\n",
            "\n",
            "Epoch 00074: val_loss improved from 0.49955 to 0.49234, saving model to model.h5\n",
            "Epoch 75/300\n",
            "141/141 - 61s - loss: 0.1711 - val_loss: 0.4950\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.49234\n",
            "Epoch 76/300\n",
            "141/141 - 61s - loss: 0.1635 - val_loss: 0.4846\n",
            "\n",
            "Epoch 00076: val_loss improved from 0.49234 to 0.48460, saving model to model.h5\n",
            "Epoch 77/300\n",
            "141/141 - 61s - loss: 0.1569 - val_loss: 0.4860\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.48460\n",
            "Epoch 78/300\n",
            "141/141 - 62s - loss: 0.1510 - val_loss: 0.4810\n",
            "\n",
            "Epoch 00078: val_loss improved from 0.48460 to 0.48103, saving model to model.h5\n",
            "Epoch 79/300\n",
            "141/141 - 61s - loss: 0.1418 - val_loss: 0.4679\n",
            "\n",
            "Epoch 00079: val_loss improved from 0.48103 to 0.46787, saving model to model.h5\n",
            "Epoch 80/300\n",
            "141/141 - 61s - loss: 0.1326 - val_loss: 0.4631\n",
            "\n",
            "Epoch 00080: val_loss improved from 0.46787 to 0.46311, saving model to model.h5\n",
            "Epoch 81/300\n",
            "141/141 - 61s - loss: 0.1245 - val_loss: 0.4596\n",
            "\n",
            "Epoch 00081: val_loss improved from 0.46311 to 0.45962, saving model to model.h5\n",
            "Epoch 82/300\n",
            "141/141 - 61s - loss: 0.1169 - val_loss: 0.4514\n",
            "\n",
            "Epoch 00082: val_loss improved from 0.45962 to 0.45139, saving model to model.h5\n",
            "Epoch 83/300\n",
            "141/141 - 62s - loss: 0.1106 - val_loss: 0.4502\n",
            "\n",
            "Epoch 00083: val_loss improved from 0.45139 to 0.45018, saving model to model.h5\n",
            "Epoch 84/300\n",
            "141/141 - 61s - loss: 0.1045 - val_loss: 0.4473\n",
            "\n",
            "Epoch 00084: val_loss improved from 0.45018 to 0.44730, saving model to model.h5\n",
            "Epoch 85/300\n",
            "141/141 - 61s - loss: 0.1010 - val_loss: 0.4480\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.44730\n",
            "Epoch 86/300\n",
            "141/141 - 61s - loss: 0.0990 - val_loss: 0.4468\n",
            "\n",
            "Epoch 00086: val_loss improved from 0.44730 to 0.44684, saving model to model.h5\n",
            "Epoch 87/300\n",
            "141/141 - 61s - loss: 0.0976 - val_loss: 0.4453\n",
            "\n",
            "Epoch 00087: val_loss improved from 0.44684 to 0.44529, saving model to model.h5\n",
            "Epoch 88/300\n",
            "141/141 - 62s - loss: 0.0961 - val_loss: 0.4454\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.44529\n",
            "Epoch 89/300\n",
            "141/141 - 61s - loss: 0.0957 - val_loss: 0.4496\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.44529\n",
            "Epoch 90/300\n",
            "141/141 - 61s - loss: 0.1004 - val_loss: 0.4539\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.44529\n",
            "Epoch 91/300\n",
            "141/141 - 61s - loss: 0.1103 - val_loss: 0.4635\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.44529\n",
            "Epoch 92/300\n",
            "141/141 - 61s - loss: 0.1095 - val_loss: 0.4547\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.44529\n",
            "Epoch 93/300\n",
            "141/141 - 61s - loss: 0.1012 - val_loss: 0.4458\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.44529\n",
            "Epoch 94/300\n",
            "141/141 - 61s - loss: 0.0851 - val_loss: 0.4339\n",
            "\n",
            "Epoch 00094: val_loss improved from 0.44529 to 0.43393, saving model to model.h5\n",
            "Epoch 95/300\n",
            "141/141 - 61s - loss: 0.0728 - val_loss: 0.4356\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.43393\n",
            "Epoch 96/300\n",
            "141/141 - 60s - loss: 0.0688 - val_loss: 0.4306\n",
            "\n",
            "Epoch 00096: val_loss improved from 0.43393 to 0.43057, saving model to model.h5\n",
            "Epoch 97/300\n",
            "141/141 - 61s - loss: 0.0680 - val_loss: 0.4287\n",
            "\n",
            "Epoch 00097: val_loss improved from 0.43057 to 0.42869, saving model to model.h5\n",
            "Epoch 98/300\n",
            "141/141 - 62s - loss: 0.0642 - val_loss: 0.4259\n",
            "\n",
            "Epoch 00098: val_loss improved from 0.42869 to 0.42593, saving model to model.h5\n",
            "Epoch 99/300\n",
            "141/141 - 62s - loss: 0.0596 - val_loss: 0.4246\n",
            "\n",
            "Epoch 00099: val_loss improved from 0.42593 to 0.42458, saving model to model.h5\n",
            "Epoch 100/300\n",
            "141/141 - 61s - loss: 0.0585 - val_loss: 0.4245\n",
            "\n",
            "Epoch 00100: val_loss improved from 0.42458 to 0.42454, saving model to model.h5\n",
            "Epoch 101/300\n",
            "141/141 - 61s - loss: 0.0559 - val_loss: 0.4246\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.42454\n",
            "Epoch 102/300\n",
            "141/141 - 61s - loss: 0.0559 - val_loss: 0.4239\n",
            "\n",
            "Epoch 00102: val_loss improved from 0.42454 to 0.42392, saving model to model.h5\n",
            "Epoch 103/300\n",
            "141/141 - 61s - loss: 0.0564 - val_loss: 0.4265\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.42392\n",
            "Epoch 104/300\n",
            "141/141 - 61s - loss: 0.0557 - val_loss: 0.4293\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.42392\n",
            "Epoch 105/300\n",
            "141/141 - 61s - loss: 0.0625 - val_loss: 0.4388\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.42392\n",
            "Epoch 106/300\n",
            "141/141 - 61s - loss: 0.0942 - val_loss: 0.4829\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.42392\n",
            "Epoch 107/300\n",
            "141/141 - 61s - loss: 0.1496 - val_loss: 0.4898\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.42392\n",
            "Epoch 108/300\n",
            "141/141 - 61s - loss: 0.1189 - val_loss: 0.4580\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.42392\n",
            "Epoch 109/300\n",
            "141/141 - 61s - loss: 0.0867 - val_loss: 0.4397\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.42392\n",
            "Epoch 110/300\n",
            "141/141 - 61s - loss: 0.0617 - val_loss: 0.4257\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.42392\n",
            "Epoch 111/300\n",
            "141/141 - 60s - loss: 0.0510 - val_loss: 0.4203\n",
            "\n",
            "Epoch 00111: val_loss improved from 0.42392 to 0.42033, saving model to model.h5\n",
            "Epoch 112/300\n",
            "141/141 - 61s - loss: 0.0465 - val_loss: 0.4210\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.42033\n",
            "Epoch 113/300\n",
            "141/141 - 61s - loss: 0.0448 - val_loss: 0.4207\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.42033\n",
            "Epoch 114/300\n",
            "141/141 - 61s - loss: 0.0448 - val_loss: 0.4221\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.42033\n",
            "Epoch 115/300\n",
            "141/141 - 60s - loss: 0.0447 - val_loss: 0.4218\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.42033\n",
            "Epoch 116/300\n",
            "141/141 - 61s - loss: 0.0444 - val_loss: 0.4238\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.42033\n",
            "Epoch 117/300\n",
            "141/141 - 60s - loss: 0.0434 - val_loss: 0.4230\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.42033\n",
            "Epoch 118/300\n",
            "141/141 - 60s - loss: 0.0433 - val_loss: 0.4234\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.42033\n",
            "Epoch 119/300\n",
            "141/141 - 61s - loss: 0.0433 - val_loss: 0.4246\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.42033\n",
            "Epoch 120/300\n",
            "141/141 - 61s - loss: 0.0425 - val_loss: 0.4251\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.42033\n",
            "Epoch 121/300\n",
            "141/141 - 61s - loss: 0.0425 - val_loss: 0.4266\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.42033\n",
            "Epoch 122/300\n",
            "141/141 - 61s - loss: 0.0435 - val_loss: 0.4289\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.42033\n",
            "Epoch 123/300\n",
            "141/141 - 61s - loss: 0.0444 - val_loss: 0.4262\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.42033\n",
            "Epoch 124/300\n",
            "141/141 - 60s - loss: 0.0466 - val_loss: 0.4337\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.42033\n",
            "Epoch 125/300\n",
            "141/141 - 61s - loss: 0.0642 - val_loss: 0.4962\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.42033\n",
            "Epoch 126/300\n",
            "141/141 - 60s - loss: 0.1314 - val_loss: 0.5190\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.42033\n",
            "Epoch 127/300\n",
            "141/141 - 60s - loss: 0.1380 - val_loss: 0.4665\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.42033\n",
            "Epoch 128/300\n",
            "141/141 - 60s - loss: 0.0858 - val_loss: 0.4414\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.42033\n",
            "Epoch 129/300\n",
            "141/141 - 60s - loss: 0.0569 - val_loss: 0.4297\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.42033\n",
            "Epoch 130/300\n",
            "141/141 - 60s - loss: 0.0464 - val_loss: 0.4277\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.42033\n",
            "Epoch 131/300\n",
            "141/141 - 60s - loss: 0.0442 - val_loss: 0.4256\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.42033\n",
            "Epoch 132/300\n",
            "141/141 - 60s - loss: 0.0413 - val_loss: 0.4253\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.42033\n",
            "Epoch 133/300\n",
            "141/141 - 60s - loss: 0.0403 - val_loss: 0.4238\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.42033\n",
            "Epoch 134/300\n",
            "141/141 - 60s - loss: 0.0386 - val_loss: 0.4251\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.42033\n",
            "Epoch 135/300\n",
            "141/141 - 60s - loss: 0.0384 - val_loss: 0.4248\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.42033\n",
            "Epoch 136/300\n",
            "141/141 - 60s - loss: 0.0380 - val_loss: 0.4249\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.42033\n",
            "Epoch 137/300\n",
            "141/141 - 60s - loss: 0.0380 - val_loss: 0.4253\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.42033\n",
            "Epoch 138/300\n",
            "141/141 - 61s - loss: 0.0382 - val_loss: 0.4249\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.42033\n",
            "Epoch 139/300\n",
            "141/141 - 60s - loss: 0.0376 - val_loss: 0.4263\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.42033\n",
            "Epoch 140/300\n",
            "141/141 - 60s - loss: 0.0382 - val_loss: 0.4260\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.42033\n",
            "Epoch 141/300\n",
            "141/141 - 60s - loss: 0.0380 - val_loss: 0.4270\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.42033\n",
            "Epoch 142/300\n",
            "141/141 - 60s - loss: 0.0387 - val_loss: 0.4273\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.42033\n",
            "Epoch 143/300\n",
            "141/141 - 60s - loss: 0.0389 - val_loss: 0.4278\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.42033\n",
            "Epoch 144/300\n",
            "141/141 - 60s - loss: 0.0406 - val_loss: 0.4324\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.42033\n",
            "Epoch 145/300\n",
            "141/141 - 60s - loss: 0.0501 - val_loss: 0.4530\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.42033\n",
            "Epoch 146/300\n",
            "141/141 - 61s - loss: 0.0921 - val_loss: 0.4958\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.42033\n",
            "Epoch 147/300\n",
            "141/141 - 61s - loss: 0.1074 - val_loss: 0.4700\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.42033\n",
            "Epoch 148/300\n",
            "141/141 - 60s - loss: 0.0778 - val_loss: 0.4477\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.42033\n",
            "Epoch 149/300\n",
            "141/141 - 60s - loss: 0.0560 - val_loss: 0.4389\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.42033\n",
            "Epoch 150/300\n",
            "141/141 - 60s - loss: 0.0454 - val_loss: 0.4334\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.42033\n",
            "Epoch 151/300\n",
            "141/141 - 61s - loss: 0.0410 - val_loss: 0.4319\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.42033\n",
            "Epoch 152/300\n",
            "141/141 - 60s - loss: 0.0382 - val_loss: 0.4284\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.42033\n",
            "Epoch 153/300\n",
            "141/141 - 61s - loss: 0.0366 - val_loss: 0.4290\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.42033\n",
            "Epoch 154/300\n",
            "141/141 - 60s - loss: 0.0360 - val_loss: 0.4310\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.42033\n",
            "Epoch 155/300\n",
            "141/141 - 60s - loss: 0.0357 - val_loss: 0.4288\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.42033\n",
            "Epoch 156/300\n",
            "141/141 - 61s - loss: 0.0355 - val_loss: 0.4290\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.42033\n",
            "Epoch 157/300\n",
            "141/141 - 62s - loss: 0.0355 - val_loss: 0.4296\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.42033\n",
            "Epoch 158/300\n",
            "141/141 - 63s - loss: 0.0355 - val_loss: 0.4294\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.42033\n",
            "Epoch 159/300\n",
            "141/141 - 62s - loss: 0.0361 - val_loss: 0.4300\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.42033\n",
            "Epoch 160/300\n",
            "141/141 - 62s - loss: 0.0357 - val_loss: 0.4305\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.42033\n",
            "Epoch 161/300\n",
            "141/141 - 63s - loss: 0.0359 - val_loss: 0.4307\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.42033\n",
            "Epoch 162/300\n",
            "141/141 - 61s - loss: 0.0365 - val_loss: 0.4325\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.42033\n",
            "Epoch 163/300\n",
            "141/141 - 63s - loss: 0.0370 - val_loss: 0.4318\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.42033\n",
            "Epoch 164/300\n",
            "141/141 - 61s - loss: 0.0386 - val_loss: 0.4369\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.42033\n",
            "Epoch 165/300\n",
            "141/141 - 62s - loss: 0.0522 - val_loss: 0.4608\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.42033\n",
            "Epoch 166/300\n",
            "141/141 - 60s - loss: 0.0881 - val_loss: 0.4864\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.42033\n",
            "Epoch 167/300\n",
            "141/141 - 60s - loss: 0.0986 - val_loss: 0.4666\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.42033\n",
            "Epoch 168/300\n",
            "141/141 - 61s - loss: 0.0836 - val_loss: 0.4508\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.42033\n",
            "Epoch 169/300\n",
            "141/141 - 61s - loss: 0.0522 - val_loss: 0.4401\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.42033\n",
            "Epoch 170/300\n",
            "141/141 - 62s - loss: 0.0415 - val_loss: 0.4328\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.42033\n",
            "Epoch 171/300\n",
            "141/141 - 63s - loss: 0.0366 - val_loss: 0.4329\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.42033\n",
            "Epoch 172/300\n",
            "141/141 - 63s - loss: 0.0355 - val_loss: 0.4336\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.42033\n",
            "Epoch 173/300\n",
            "141/141 - 64s - loss: 0.0345 - val_loss: 0.4321\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.42033\n",
            "Epoch 174/300\n",
            "141/141 - 63s - loss: 0.0339 - val_loss: 0.4323\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.42033\n",
            "Epoch 175/300\n",
            "141/141 - 63s - loss: 0.0340 - val_loss: 0.4329\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.42033\n",
            "Epoch 176/300\n",
            "141/141 - 64s - loss: 0.0340 - val_loss: 0.4327\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.42033\n",
            "Epoch 177/300\n",
            "141/141 - 64s - loss: 0.0339 - val_loss: 0.4339\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.42033\n",
            "Epoch 178/300\n",
            "141/141 - 64s - loss: 0.0338 - val_loss: 0.4333\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.42033\n",
            "Epoch 179/300\n",
            "141/141 - 63s - loss: 0.0338 - val_loss: 0.4329\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.42033\n",
            "Epoch 180/300\n",
            "141/141 - 63s - loss: 0.0344 - val_loss: 0.4336\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.42033\n",
            "Epoch 181/300\n",
            "141/141 - 64s - loss: 0.0344 - val_loss: 0.4329\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.42033\n",
            "Epoch 182/300\n",
            "141/141 - 63s - loss: 0.0345 - val_loss: 0.4343\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.42033\n",
            "Epoch 183/300\n",
            "141/141 - 64s - loss: 0.0346 - val_loss: 0.4345\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.42033\n",
            "Epoch 184/300\n",
            "141/141 - 61s - loss: 0.0346 - val_loss: 0.4356\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.42033\n",
            "Epoch 185/300\n",
            "141/141 - 60s - loss: 0.0360 - val_loss: 0.4396\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.42033\n",
            "Epoch 186/300\n",
            "141/141 - 61s - loss: 0.0434 - val_loss: 0.4588\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.42033\n",
            "Epoch 187/300\n",
            "141/141 - 61s - loss: 0.0858 - val_loss: 0.4875\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.42033\n",
            "Epoch 188/300\n",
            "141/141 - 61s - loss: 0.1013 - val_loss: 0.4722\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.42033\n",
            "Epoch 189/300\n",
            "141/141 - 60s - loss: 0.0720 - val_loss: 0.4534\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.42033\n",
            "Epoch 190/300\n",
            "141/141 - 60s - loss: 0.0473 - val_loss: 0.4392\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.42033\n",
            "Epoch 191/300\n",
            "141/141 - 61s - loss: 0.0380 - val_loss: 0.4359\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.42033\n",
            "Epoch 192/300\n",
            "141/141 - 61s - loss: 0.0345 - val_loss: 0.4330\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.42033\n",
            "Epoch 193/300\n",
            "141/141 - 61s - loss: 0.0330 - val_loss: 0.4338\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.42033\n",
            "Epoch 194/300\n",
            "141/141 - 60s - loss: 0.0327 - val_loss: 0.4339\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.42033\n",
            "Epoch 195/300\n",
            "141/141 - 60s - loss: 0.0327 - val_loss: 0.4339\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.42033\n",
            "Epoch 196/300\n",
            "141/141 - 60s - loss: 0.0326 - val_loss: 0.4345\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.42033\n",
            "Epoch 197/300\n",
            "141/141 - 61s - loss: 0.0323 - val_loss: 0.4339\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.42033\n",
            "Epoch 198/300\n",
            "141/141 - 61s - loss: 0.0327 - val_loss: 0.4338\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.42033\n",
            "Epoch 199/300\n",
            "141/141 - 61s - loss: 0.0325 - val_loss: 0.4346\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.42033\n",
            "Epoch 200/300\n",
            "141/141 - 60s - loss: 0.0331 - val_loss: 0.4351\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.42033\n",
            "Epoch 201/300\n",
            "141/141 - 60s - loss: 0.0332 - val_loss: 0.4361\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.42033\n",
            "Epoch 202/300\n",
            "141/141 - 61s - loss: 0.0334 - val_loss: 0.4358\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.42033\n",
            "Epoch 203/300\n",
            "141/141 - 60s - loss: 0.0333 - val_loss: 0.4361\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.42033\n",
            "Epoch 204/300\n",
            "141/141 - 60s - loss: 0.0335 - val_loss: 0.4363\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.42033\n",
            "Epoch 205/300\n",
            "141/141 - 61s - loss: 0.0336 - val_loss: 0.4380\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.42033\n",
            "Epoch 206/300\n",
            "141/141 - 60s - loss: 0.0336 - val_loss: 0.4364\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.42033\n",
            "Epoch 207/300\n",
            "141/141 - 61s - loss: 0.0340 - val_loss: 0.4375\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.42033\n",
            "Epoch 208/300\n",
            "141/141 - 61s - loss: 0.0348 - val_loss: 0.4371\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.42033\n",
            "Epoch 209/300\n",
            "141/141 - 60s - loss: 0.0350 - val_loss: 0.4384\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.42033\n",
            "Epoch 210/300\n",
            "141/141 - 61s - loss: 0.0361 - val_loss: 0.4413\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.42033\n",
            "Epoch 211/300\n",
            "141/141 - 60s - loss: 0.0522 - val_loss: 0.4834\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.42033\n",
            "Epoch 212/300\n",
            "141/141 - 61s - loss: 0.1012 - val_loss: 0.4941\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.42033\n",
            "Epoch 213/300\n",
            "141/141 - 61s - loss: 0.0886 - val_loss: 0.4616\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.42033\n",
            "Epoch 214/300\n",
            "141/141 - 61s - loss: 0.0562 - val_loss: 0.4501\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.42033\n",
            "Epoch 215/300\n",
            "141/141 - 60s - loss: 0.0440 - val_loss: 0.4419\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.42033\n",
            "Epoch 216/300\n",
            "141/141 - 60s - loss: 0.0385 - val_loss: 0.4381\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.42033\n",
            "Epoch 217/300\n",
            "141/141 - 60s - loss: 0.0343 - val_loss: 0.4387\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.42033\n",
            "Epoch 218/300\n",
            "141/141 - 61s - loss: 0.0328 - val_loss: 0.4361\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.42033\n",
            "Epoch 219/300\n",
            "141/141 - 60s - loss: 0.0319 - val_loss: 0.4363\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.42033\n",
            "Epoch 220/300\n",
            "141/141 - 60s - loss: 0.0318 - val_loss: 0.4367\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.42033\n",
            "Epoch 221/300\n",
            "141/141 - 61s - loss: 0.0316 - val_loss: 0.4371\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.42033\n",
            "Epoch 222/300\n",
            "141/141 - 61s - loss: 0.0315 - val_loss: 0.4375\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.42033\n",
            "Epoch 223/300\n",
            "141/141 - 61s - loss: 0.0315 - val_loss: 0.4372\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.42033\n",
            "Epoch 224/300\n",
            "141/141 - 60s - loss: 0.0314 - val_loss: 0.4385\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.42033\n",
            "Epoch 225/300\n",
            "141/141 - 60s - loss: 0.0317 - val_loss: 0.4390\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.42033\n",
            "Epoch 226/300\n",
            "141/141 - 60s - loss: 0.0319 - val_loss: 0.4395\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.42033\n",
            "Epoch 227/300\n",
            "141/141 - 61s - loss: 0.0321 - val_loss: 0.4395\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.42033\n",
            "Epoch 228/300\n",
            "141/141 - 61s - loss: 0.0322 - val_loss: 0.4383\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.42033\n",
            "Epoch 229/300\n",
            "141/141 - 61s - loss: 0.0326 - val_loss: 0.4394\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.42033\n",
            "Epoch 230/300\n",
            "141/141 - 60s - loss: 0.0326 - val_loss: 0.4389\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.42033\n",
            "Epoch 231/300\n",
            "141/141 - 60s - loss: 0.0327 - val_loss: 0.4400\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.42033\n",
            "Epoch 232/300\n",
            "141/141 - 61s - loss: 0.0331 - val_loss: 0.4388\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.42033\n",
            "Epoch 233/300\n",
            "141/141 - 61s - loss: 0.0332 - val_loss: 0.4401\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.42033\n",
            "Epoch 234/300\n",
            "141/141 - 61s - loss: 0.0332 - val_loss: 0.4412\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.42033\n",
            "Epoch 235/300\n",
            "141/141 - 61s - loss: 0.0351 - val_loss: 0.4441\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.42033\n",
            "Epoch 236/300\n",
            "141/141 - 61s - loss: 0.0505 - val_loss: 0.4874\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.42033\n",
            "Epoch 237/300\n",
            "141/141 - 61s - loss: 0.0910 - val_loss: 0.4945\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.42033\n",
            "Epoch 238/300\n",
            "141/141 - 61s - loss: 0.0874 - val_loss: 0.4649\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.42033\n",
            "Epoch 239/300\n",
            "141/141 - 61s - loss: 0.0570 - val_loss: 0.4479\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.42033\n",
            "Epoch 240/300\n",
            "141/141 - 61s - loss: 0.0392 - val_loss: 0.4407\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.42033\n",
            "Epoch 241/300\n",
            "141/141 - 61s - loss: 0.0342 - val_loss: 0.4406\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.42033\n",
            "Epoch 242/300\n",
            "141/141 - 61s - loss: 0.0324 - val_loss: 0.4400\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.42033\n",
            "Epoch 243/300\n",
            "141/141 - 61s - loss: 0.0316 - val_loss: 0.4407\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.42033\n",
            "Epoch 244/300\n",
            "141/141 - 62s - loss: 0.0313 - val_loss: 0.4410\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.42033\n",
            "Epoch 245/300\n",
            "141/141 - 61s - loss: 0.0312 - val_loss: 0.4408\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.42033\n",
            "Epoch 246/300\n",
            "141/141 - 61s - loss: 0.0311 - val_loss: 0.4419\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.42033\n",
            "Epoch 247/300\n",
            "141/141 - 61s - loss: 0.0315 - val_loss: 0.4409\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.42033\n",
            "Epoch 248/300\n",
            "141/141 - 61s - loss: 0.0314 - val_loss: 0.4412\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.42033\n",
            "Epoch 249/300\n",
            "141/141 - 61s - loss: 0.0313 - val_loss: 0.4436\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.42033\n",
            "Epoch 250/300\n",
            "141/141 - 60s - loss: 0.0314 - val_loss: 0.4433\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.42033\n",
            "Epoch 251/300\n",
            "141/141 - 61s - loss: 0.0317 - val_loss: 0.4421\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.42033\n",
            "Epoch 252/300\n",
            "141/141 - 61s - loss: 0.0317 - val_loss: 0.4439\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.42033\n",
            "Epoch 253/300\n",
            "141/141 - 61s - loss: 0.0317 - val_loss: 0.4443\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.42033\n",
            "Epoch 254/300\n",
            "141/141 - 61s - loss: 0.0322 - val_loss: 0.4430\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.42033\n",
            "Epoch 255/300\n",
            "141/141 - 61s - loss: 0.0321 - val_loss: 0.4436\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.42033\n",
            "Epoch 256/300\n",
            "141/141 - 61s - loss: 0.0321 - val_loss: 0.4441\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.42033\n",
            "Epoch 257/300\n",
            "141/141 - 61s - loss: 0.0326 - val_loss: 0.4446\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.42033\n",
            "Epoch 258/300\n",
            "141/141 - 61s - loss: 0.0330 - val_loss: 0.4436\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.42033\n",
            "Epoch 259/300\n",
            "141/141 - 61s - loss: 0.0335 - val_loss: 0.4429\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.42033\n",
            "Epoch 260/300\n",
            "141/141 - 60s - loss: 0.0338 - val_loss: 0.4463\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.42033\n",
            "Epoch 261/300\n",
            "141/141 - 60s - loss: 0.0354 - val_loss: 0.4514\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.42033\n",
            "Epoch 262/300\n",
            "141/141 - 60s - loss: 0.0716 - val_loss: 0.5165\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.42033\n",
            "Epoch 263/300\n",
            "141/141 - 60s - loss: 0.0976 - val_loss: 0.4754\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.42033\n",
            "Epoch 264/300\n",
            "141/141 - 61s - loss: 0.0632 - val_loss: 0.4571\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.42033\n",
            "Epoch 265/300\n",
            "141/141 - 60s - loss: 0.0439 - val_loss: 0.4495\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.42033\n",
            "Epoch 266/300\n",
            "141/141 - 60s - loss: 0.0362 - val_loss: 0.4446\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.42033\n",
            "Epoch 267/300\n",
            "141/141 - 60s - loss: 0.0329 - val_loss: 0.4443\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.42033\n",
            "Epoch 268/300\n",
            "141/141 - 60s - loss: 0.0325 - val_loss: 0.4451\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.42033\n",
            "Epoch 269/300\n",
            "141/141 - 60s - loss: 0.0317 - val_loss: 0.4441\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.42033\n",
            "Epoch 270/300\n",
            "141/141 - 61s - loss: 0.0308 - val_loss: 0.4440\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.42033\n",
            "Epoch 271/300\n",
            "141/141 - 61s - loss: 0.0305 - val_loss: 0.4450\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.42033\n",
            "Epoch 272/300\n",
            "141/141 - 61s - loss: 0.0305 - val_loss: 0.4445\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.42033\n",
            "Epoch 273/300\n",
            "141/141 - 61s - loss: 0.0310 - val_loss: 0.4452\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.42033\n",
            "Epoch 274/300\n",
            "141/141 - 61s - loss: 0.0309 - val_loss: 0.4448\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.42033\n",
            "Epoch 275/300\n",
            "141/141 - 61s - loss: 0.0306 - val_loss: 0.4454\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.42033\n",
            "Epoch 276/300\n",
            "141/141 - 60s - loss: 0.0309 - val_loss: 0.4453\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.42033\n",
            "Epoch 277/300\n",
            "141/141 - 60s - loss: 0.0308 - val_loss: 0.4465\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.42033\n",
            "Epoch 278/300\n",
            "141/141 - 60s - loss: 0.0312 - val_loss: 0.4474\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.42033\n",
            "Epoch 279/300\n",
            "141/141 - 61s - loss: 0.0314 - val_loss: 0.4462\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.42033\n",
            "Epoch 280/300\n",
            "141/141 - 60s - loss: 0.0316 - val_loss: 0.4467\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.42033\n",
            "Epoch 281/300\n",
            "141/141 - 60s - loss: 0.0319 - val_loss: 0.4479\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.42033\n",
            "Epoch 282/300\n",
            "141/141 - 60s - loss: 0.0315 - val_loss: 0.4454\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.42033\n",
            "Epoch 283/300\n",
            "141/141 - 60s - loss: 0.0315 - val_loss: 0.4483\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.42033\n",
            "Epoch 284/300\n",
            "141/141 - 60s - loss: 0.0319 - val_loss: 0.4463\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.42033\n",
            "Epoch 285/300\n",
            "141/141 - 60s - loss: 0.0322 - val_loss: 0.4474\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.42033\n",
            "Epoch 286/300\n",
            "141/141 - 60s - loss: 0.0338 - val_loss: 0.4536\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.42033\n",
            "Epoch 287/300\n",
            "141/141 - 60s - loss: 0.0690 - val_loss: 0.5086\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.42033\n",
            "Epoch 288/300\n",
            "141/141 - 60s - loss: 0.0906 - val_loss: 0.4766\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.42033\n",
            "Epoch 289/300\n",
            "141/141 - 60s - loss: 0.0572 - val_loss: 0.4572\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.42033\n",
            "Epoch 290/300\n",
            "141/141 - 60s - loss: 0.0402 - val_loss: 0.4473\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.42033\n",
            "Epoch 291/300\n",
            "141/141 - 61s - loss: 0.0333 - val_loss: 0.4470\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.42033\n",
            "Epoch 292/300\n",
            "141/141 - 60s - loss: 0.0321 - val_loss: 0.4474\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.42033\n",
            "Epoch 293/300\n",
            "141/141 - 60s - loss: 0.0312 - val_loss: 0.4466\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.42033\n",
            "Epoch 294/300\n",
            "141/141 - 60s - loss: 0.0304 - val_loss: 0.4467\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.42033\n",
            "Epoch 295/300\n",
            "141/141 - 60s - loss: 0.0305 - val_loss: 0.4465\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.42033\n",
            "Epoch 296/300\n",
            "141/141 - 61s - loss: 0.0301 - val_loss: 0.4487\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.42033\n",
            "Epoch 297/300\n",
            "141/141 - 61s - loss: 0.0305 - val_loss: 0.4478\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.42033\n",
            "Epoch 298/300\n",
            "141/141 - 60s - loss: 0.0305 - val_loss: 0.4477\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.42033\n",
            "Epoch 299/300\n",
            "141/141 - 60s - loss: 0.0303 - val_loss: 0.4474\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.42033\n",
            "Epoch 300/300\n",
            "141/141 - 60s - loss: 0.0302 - val_loss: 0.4486\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.42033\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f1944d38ef0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJBwK47nOCkK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3515e4e0-1371-478d-9ab8-1b518336753f"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# map an integer to a word\n",
        "def word_for_id(integer, tokenizer):\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == integer:\n",
        "\t\t\treturn word\n",
        "\treturn None\n",
        "\n",
        "# generate target given source sequence\n",
        "def predict_sequence(model, tokenizer, source):\n",
        "\tprediction = model.predict(source, verbose=0)[0]\n",
        "\tintegers = [argmax(vector) for vector in prediction]\n",
        "\ttarget = list()\n",
        "\tfor i in integers:\n",
        "\t\tword = word_for_id(i, tokenizer)\n",
        "\t\tif word is None:\n",
        "\t\t\tbreak\n",
        "\t\ttarget.append(word)\n",
        "\treturn ' '.join(target)\n",
        "\n",
        "# evaluate the skill of the model\n",
        "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
        "\tactual, predicted = list(), list()\n",
        "\tfor i, source in enumerate(sources):\n",
        "\t\t# translate encoded source text\n",
        "\t\tsource = source.reshape((1, source.shape[0]))\n",
        "\t\ttranslation = predict_sequence(model, eng_tokenizer, source)\n",
        "\t\traw_target, raw_src,test = raw_dataset[i]\n",
        "\t\tif i < 10:\n",
        "\t\t\tprint('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
        "\t\tactual.append([raw_target.split()])\n",
        "\t\tpredicted.append(translation.split())\n",
        "\t# calculate BLEU score\n",
        "\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-romanian-both.pkl')\n",
        "train = load_clean_sentences('english-romanian-train.pkl')\n",
        "test = load_clean_sentences('english-romanian-test.pkl')\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "# prepare data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "\n",
        "# load model\n",
        "model = load_model('model.h5')\n",
        "# test on some training sequences\n",
        "print('train')\n",
        "evaluate_model(model, eng_tokenizer, trainX, train)\n",
        "# test on some test sequences\n",
        "print('test')\n",
        "evaluate_model(model, eng_tokenizer, testX, test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train\n",
            "src=[nu pot sa o gasesc], target=[i cant find it], predicted=[i cant find it]\n",
            "src=[acest ceas trebuie reparat], target=[this watch needs to be fixed], predicted=[this watch needs to be fixed]\n",
            "src=[ajutor], target=[help], predicted=[help]\n",
            "src=[tom a mancat prea multa vata de zahar], target=[tom ate too much cotton candy], predicted=[tom ate too much cotton candy]\n",
            "src=[supa este groasa], target=[the soup is thick], predicted=[the soup is thick]\n",
            "src=[stai deoparte de acel loc], target=[keep away from there], predicted=[keep away from there]\n",
            "src=[cainele a venit alergand la ea], target=[the dog came running to her], predicted=[the dog came running to her]\n",
            "src=[nu trebuie sa o pierzi], target=[you ought not to miss it], predicted=[you ought not to miss it]\n",
            "src=[ai grija de tine], target=[take care], predicted=[take care]\n",
            "src=[casa aceea este a mea], target=[that house is mine], predicted=[that house is mine]\n",
            "BLEU-1: 0.979353\n",
            "BLEU-2: 0.971890\n",
            "BLEU-3: 0.966778\n",
            "BLEU-4: 0.943036\n",
            "test\n",
            "src=[asta e o propozitie ciudata], target=[this is a weird sentence], predicted=[this is a strange sentence]\n",
            "src=[lasama sa intru], target=[let me come in], predicted=[let me come in]\n",
            "src=[mam nascut in kyoto in], target=[i was born in kyoto in], predicted=[i was born in kyoto in]\n",
            "src=[sunt uimita de indrazneala ta], target=[i am amazed at your audacity], predicted=[i am amazed at your audacity]\n",
            "src=[cat de adanc este lacul asta], target=[how deep is this lake], predicted=[how deep is this lake]\n",
            "src=[totul este o greseala], target=[this is all a mistake], predicted=[this is all a mistake]\n",
            "src=[preturile variaza in functie de locatie], target=[prices vary by location], predicted=[prices vary by location]\n",
            "src=[voi conduce chiar eu], target=[ill drive myself], predicted=[ill drive myself]\n",
            "src=[inveti ceva nou in fiecare zi], target=[you learn something new every day], predicted=[you learn something new every day]\n",
            "src=[imi pare rau], target=[im sorry], predicted=[im sorry]\n",
            "BLEU-1: 0.913682\n",
            "BLEU-2: 0.895274\n",
            "BLEU-3: 0.891656\n",
            "BLEU-4: 0.858230\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkK4Ms3vVyxk"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}