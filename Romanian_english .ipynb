{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNeOAY2ZT1pY",
        "outputId": "911bfa42-11b2-49e6-98fc-8d84f1566944"
      },
      "source": [
        "import string\n",
        "import re\n",
        "from pickle import dump\n",
        "from unicodedata import normalize\n",
        "from numpy import array\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, mode='rt', encoding='utf-8')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "# split a loaded document into sentences\n",
        "def to_pairs(doc):\n",
        "\tlines = doc.strip().split('\\n')\n",
        "\tpairs = [line.split('\\t') for line in  lines]\n",
        "\treturn pairs\n",
        "\n",
        "# clean a list of lines\n",
        "def clean_pairs(lines):\n",
        "\tcleaned = list()\n",
        "\t# prepare regex for char filtering\n",
        "\tre_print = re.compile('[^%s]' % re.escape(string.printable))\n",
        "\t# prepare translation table for removing punctuation\n",
        "\ttable = str.maketrans('', '', string.punctuation)\n",
        "\tfor pair in lines:\n",
        "\t\tclean_pair = list()\n",
        "\t\tfor line in pair:\n",
        "\t\t\t# normalize unicode characters\n",
        "\t\t\tline = normalize('NFD', line).encode('ascii', 'ignore')\n",
        "\t\t\tline = line.decode('UTF-8')\n",
        "\t\t\t# tokenize on white space\n",
        "\t\t\tline = line.split()\n",
        "\t\t\t# convert to lowercase\n",
        "\t\t\tline = [word.lower() for word in line]\n",
        "\t\t\t# remove punctuation from each token\n",
        "\t\t\tline = [word.translate(table) for word in line]\n",
        "\t\t\t# remove non-printable chars form each token\n",
        "\t\t\tline = [re_print.sub('', w) for w in line]\n",
        "\t\t\t# remove tokens with numbers in them\n",
        "\t\t\tline = [word for word in line if word.isalpha()]\n",
        "\t\t\t# store as string\n",
        "\t\t\tclean_pair.append(' '.join(line))\n",
        "\t\tcleaned.append(clean_pair)\n",
        "\treturn array(cleaned)\n",
        "\n",
        "# save a list of clean sentences to file\n",
        "def save_clean_data(sentences, filename):\n",
        "\tdump(sentences, open(filename, 'wb'))\n",
        "\tprint('Saved: %s' % filename)\n",
        "\n",
        "# load dataset\n",
        "filename = 'ron.txt'\n",
        "doc = load_doc(filename)\n",
        "# split into english-german pairs\n",
        "pairs = to_pairs(doc)\n",
        "# clean sentences\n",
        "clean_pairs = clean_pairs(pairs)\n",
        "# save clean pairs to file\n",
        "save_clean_data(clean_pairs, 'english-romanian.pkl')\n",
        "# spot check\n",
        "for i in range(100):\n",
        "\tprint('[%s] => [%s]' % (clean_pairs[i,0], clean_pairs[i,1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved: english-romanian.pkl\n",
            "[hi] => [buna]\n",
            "[run] => [fugi]\n",
            "[who] => [cine]\n",
            "[fire] => [foc]\n",
            "[help] => [ajutor]\n",
            "[jump] => [sari]\n",
            "[stop] => [oprestete]\n",
            "[wait] => [asteapta]\n",
            "[hello] => [salut]\n",
            "[hello] => [buna]\n",
            "[hurry] => [haide]\n",
            "[hurry] => [grabestete]\n",
            "[relax] => [calmeazate]\n",
            "[smile] => [zambeste]\n",
            "[attack] => [ataca]\n",
            "[attack] => [atacati]\n",
            "[cheers] => [noroc]\n",
            "[cheers] => [sanatate]\n",
            "[cheers] => [noroc bun]\n",
            "[freeze] => [stai pe loc]\n",
            "[get up] => [ridicate]\n",
            "[really] => [serios]\n",
            "[ask tom] => [intreabal pe tom]\n",
            "[awesome] => [maxim]\n",
            "[awesome] => [excelent]\n",
            "[call me] => [sunama]\n",
            "[get out] => [pleaca de aici]\n",
            "[get out] => [pleaca de aici]\n",
            "[go away] => [pleaca]\n",
            "[go away] => [lasama]\n",
            "[goodbye] => [la revedere]\n",
            "[hold on] => [rezista]\n",
            "[i agree] => [sunt de acord]\n",
            "[im tom] => [sunt tom]\n",
            "[im ill] => [sunt bolnav]\n",
            "[im sad] => [sunt trist]\n",
            "[its ok] => [e ok]\n",
            "[keep it] => [pastreazo]\n",
            "[keep it] => [pastreazal]\n",
            "[open up] => [deschide]\n",
            "[perfect] => [perfect]\n",
            "[tell me] => [spunemi]\n",
            "[why not] => [de ce nu]\n",
            "[grab him] => [apucate de el]\n",
            "[how cute] => [ce dragut]\n",
            "[how cute] => [ce simpatic]\n",
            "[hurry up] => [grabestete]\n",
            "[ill pay] => [eu voi plati]\n",
            "[im back] => [mam intors]\n",
            "[im back] => [am revenit]\n",
            "[im calm] => [sunt calm]\n",
            "[im free] => [eu sunt liber]\n",
            "[im here] => [sunt aici]\n",
            "[im here] => [eu sunt aici]\n",
            "[im home] => [sunt acasa]\n",
            "[im numb] => [sunt indiferent]\n",
            "[im numb] => [sunt indiferenta]\n",
            "[im sick] => [sunt bolnav]\n",
            "[it hurts] => [ma doare]\n",
            "[its tom] => [este tom]\n",
            "[keep out] => [nu intra]\n",
            "[marry me] => [casatorestete cu mine]\n",
            "[may i go] => [pot sa merg]\n",
            "[may i go] => [pot sa ma duc]\n",
            "[terrific] => [maxim]\n",
            "[terrific] => [teribil]\n",
            "[tom fled] => [tom a fugit]\n",
            "[tom left] => [tom a plecat]\n",
            "[too late] => [prea tarziu]\n",
            "[trust me] => [ai incredere in mine]\n",
            "[use this] => [utilizeaza asta]\n",
            "[who fell] => [cine a cazut]\n",
            "[who paid] => [cine a platit]\n",
            "[bless you] => [noroc]\n",
            "[bless you] => [sanatate]\n",
            "[bless you] => [noroc bun]\n",
            "[calm down] => [calmeazate]\n",
            "[come back] => [intoarcete]\n",
            "[come back] => [revino]\n",
            "[come back] => [reveniti]\n",
            "[come back] => [intoarcetiva]\n",
            "[come home] => [vino acasa]\n",
            "[dont ask] => [nu intreba]\n",
            "[fantastic] => [maxim]\n",
            "[he is ill] => [el este bolnav]\n",
            "[hes fast] => [el e rapid]\n",
            "[i am sick] => [sunt bolnav]\n",
            "[i beg you] => [va rog]\n",
            "[i can ski] => [pot sa schiez]\n",
            "[i can win] => [eu pot castiga]\n",
            "[i hope so] => [sper]\n",
            "[i will go] => [eu voi merge]\n",
            "[ill lose] => [voi pierde]\n",
            "[im bored] => [sunt plictisit]\n",
            "[im bored] => [mam plictisit]\n",
            "[im drunk] => [sunt beat]\n",
            "[im happy] => [sunt fericit]\n",
            "[im sorry] => [imi pare rau]\n",
            "[im tired] => [sunt obosit]\n",
            "[im tired] => [sunt obosita]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKNbh6zgVXjb",
        "outputId": "943fdd61-e0af-4a21-caa2-e0c675cb2697"
      },
      "source": [
        "from pickle import load\n",
        "from pickle import dump\n",
        "from numpy.random import rand\n",
        "from numpy.random import shuffle\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# save a list of clean sentences to file\n",
        "def save_clean_data(sentences, filename):\n",
        "\tdump(sentences, open(filename, 'wb'))\n",
        "\tprint('Saved: %s' % filename)\n",
        "\n",
        "# load dataset\n",
        "raw_dataset = load_clean_sentences('english-romanian.pkl')\n",
        "\n",
        "# reduce dataset size\n",
        "n_sentences = 10000\n",
        "dataset = raw_dataset[:n_sentences, :]\n",
        "# random shuffle\n",
        "shuffle(dataset)\n",
        "# split into train/test\n",
        "train, test = dataset[:9000], dataset[1000:]\n",
        "# save\n",
        "save_clean_data(dataset, 'english-romanian-both.pkl')\n",
        "save_clean_data(train, 'english-romanian-train.pkl')\n",
        "save_clean_data(test, 'english-romanian-test.pkl')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved: english-romanian-both.pkl\n",
            "Saved: english-romanian-train.pkl\n",
            "Saved: english-romanian-test.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6NqPWK-VryP",
        "outputId": "7429242a-05be-4151-d9d8-7b2215c6106f"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import RepeatVector\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# one hot encode target sequence\n",
        "def encode_output(sequences, vocab_size):\n",
        "\tylist = list()\n",
        "\tfor sequence in sequences:\n",
        "\t\tencoded = to_categorical(sequence, num_classes=vocab_size)\n",
        "\t\tylist.append(encoded)\n",
        "\ty = array(ylist)\n",
        "\ty = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
        "\treturn y\n",
        "\n",
        "# define NMT model\n",
        "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
        "\tmodel.add(LSTM(n_units))\n",
        "\tmodel.add(RepeatVector(tar_timesteps))\n",
        "\tmodel.add(LSTM(n_units, return_sequences=True))\n",
        "\tmodel.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
        "\treturn model\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-romanian-both.pkl')\n",
        "train = load_clean_sentences('english-romanian-train.pkl')\n",
        "test = load_clean_sentences('english-romanian-test.pkl')\n",
        "\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:,0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
        "print('English Max Length: %d' % (eng_length))\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "print('Romanian Vocabulary Size: %d' % ger_vocab_size)\n",
        "print('Romanian  Max Length: %d' % (ger_length))\n",
        "\n",
        "# prepare training data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
        "trainY = encode_output(trainY, eng_vocab_size)\n",
        "# prepare validation data\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
        "testY = encode_output(testY, eng_vocab_size)\n",
        "\n",
        "# define model\n",
        "model = define_model(ger_vocab_size, eng_vocab_size, ger_length, eng_length, 256)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "# summarize defined model\n",
        "print(model.summary())\n",
        "plot_model(model, to_file='model.png', show_shapes=True)\n",
        "# fit model\n",
        "filename = 'model.h5'\n",
        "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "model.fit(trainX, trainY, epochs=300, batch_size=64, validation_data=(testX, testY), callbacks=[checkpoint], verbose=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English Vocabulary Size: 4297\n",
            "English Max Length: 10\n",
            "Romanian Vocabulary Size: 6121\n",
            "Romanian  Max Length: 14\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 14, 256)           1566976   \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 256)               525312    \n",
            "_________________________________________________________________\n",
            "repeat_vector (RepeatVector) (None, 10, 256)           0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 10, 256)           525312    \n",
            "_________________________________________________________________\n",
            "time_distributed (TimeDistri (None, 10, 4297)          1104329   \n",
            "=================================================================\n",
            "Total params: 3,721,929\n",
            "Trainable params: 3,721,929\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/300\n",
            "141/141 - 67s - loss: 4.1603 - val_loss: 3.4185\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 3.41850, saving model to model.h5\n",
            "Epoch 2/300\n",
            "141/141 - 60s - loss: 3.3641 - val_loss: 3.2516\n",
            "\n",
            "Epoch 00002: val_loss improved from 3.41850 to 3.25161, saving model to model.h5\n",
            "Epoch 3/300\n",
            "141/141 - 60s - loss: 3.2394 - val_loss: 3.1604\n",
            "\n",
            "Epoch 00003: val_loss improved from 3.25161 to 3.16036, saving model to model.h5\n",
            "Epoch 4/300\n",
            "141/141 - 60s - loss: 3.1685 - val_loss: 3.1307\n",
            "\n",
            "Epoch 00004: val_loss improved from 3.16036 to 3.13071, saving model to model.h5\n",
            "Epoch 5/300\n",
            "141/141 - 61s - loss: 3.0848 - val_loss: 3.0271\n",
            "\n",
            "Epoch 00005: val_loss improved from 3.13071 to 3.02709, saving model to model.h5\n",
            "Epoch 6/300\n",
            "141/141 - 61s - loss: 3.0328 - val_loss: 2.9959\n",
            "\n",
            "Epoch 00006: val_loss improved from 3.02709 to 2.99586, saving model to model.h5\n",
            "Epoch 7/300\n",
            "141/141 - 61s - loss: 2.9993 - val_loss: 2.9628\n",
            "\n",
            "Epoch 00007: val_loss improved from 2.99586 to 2.96277, saving model to model.h5\n",
            "Epoch 8/300\n",
            "141/141 - 60s - loss: 2.9641 - val_loss: 2.9333\n",
            "\n",
            "Epoch 00008: val_loss improved from 2.96277 to 2.93332, saving model to model.h5\n",
            "Epoch 9/300\n",
            "141/141 - 60s - loss: 2.9294 - val_loss: 2.8994\n",
            "\n",
            "Epoch 00009: val_loss improved from 2.93332 to 2.89945, saving model to model.h5\n",
            "Epoch 10/300\n",
            "141/141 - 61s - loss: 2.8864 - val_loss: 2.8530\n",
            "\n",
            "Epoch 00010: val_loss improved from 2.89945 to 2.85298, saving model to model.h5\n",
            "Epoch 11/300\n",
            "141/141 - 60s - loss: 2.8328 - val_loss: 2.7956\n",
            "\n",
            "Epoch 00011: val_loss improved from 2.85298 to 2.79562, saving model to model.h5\n",
            "Epoch 12/300\n",
            "141/141 - 61s - loss: 2.7689 - val_loss: 2.7269\n",
            "\n",
            "Epoch 00012: val_loss improved from 2.79562 to 2.72687, saving model to model.h5\n",
            "Epoch 13/300\n",
            "141/141 - 60s - loss: 2.6960 - val_loss: 2.6547\n",
            "\n",
            "Epoch 00013: val_loss improved from 2.72687 to 2.65472, saving model to model.h5\n",
            "Epoch 14/300\n",
            "141/141 - 60s - loss: 2.6316 - val_loss: 2.6158\n",
            "\n",
            "Epoch 00014: val_loss improved from 2.65472 to 2.61583, saving model to model.h5\n",
            "Epoch 15/300\n",
            "141/141 - 61s - loss: 2.5830 - val_loss: 2.5523\n",
            "\n",
            "Epoch 00015: val_loss improved from 2.61583 to 2.55234, saving model to model.h5\n",
            "Epoch 16/300\n",
            "141/141 - 61s - loss: 2.5202 - val_loss: 2.4926\n",
            "\n",
            "Epoch 00016: val_loss improved from 2.55234 to 2.49256, saving model to model.h5\n",
            "Epoch 17/300\n",
            "141/141 - 61s - loss: 2.4561 - val_loss: 2.4318\n",
            "\n",
            "Epoch 00017: val_loss improved from 2.49256 to 2.43178, saving model to model.h5\n",
            "Epoch 18/300\n",
            "141/141 - 61s - loss: 2.4015 - val_loss: 2.3888\n",
            "\n",
            "Epoch 00018: val_loss improved from 2.43178 to 2.38876, saving model to model.h5\n",
            "Epoch 19/300\n",
            "141/141 - 61s - loss: 2.3438 - val_loss: 2.3468\n",
            "\n",
            "Epoch 00019: val_loss improved from 2.38876 to 2.34681, saving model to model.h5\n",
            "Epoch 20/300\n",
            "141/141 - 61s - loss: 2.2825 - val_loss: 2.2735\n",
            "\n",
            "Epoch 00020: val_loss improved from 2.34681 to 2.27354, saving model to model.h5\n",
            "Epoch 21/300\n",
            "141/141 - 61s - loss: 2.2214 - val_loss: 2.2137\n",
            "\n",
            "Epoch 00021: val_loss improved from 2.27354 to 2.21367, saving model to model.h5\n",
            "Epoch 22/300\n",
            "141/141 - 61s - loss: 2.1600 - val_loss: 2.1555\n",
            "\n",
            "Epoch 00022: val_loss improved from 2.21367 to 2.15555, saving model to model.h5\n",
            "Epoch 23/300\n",
            "141/141 - 61s - loss: 2.0926 - val_loss: 2.0878\n",
            "\n",
            "Epoch 00023: val_loss improved from 2.15555 to 2.08784, saving model to model.h5\n",
            "Epoch 24/300\n",
            "141/141 - 61s - loss: 2.0182 - val_loss: 2.0318\n",
            "\n",
            "Epoch 00024: val_loss improved from 2.08784 to 2.03182, saving model to model.h5\n",
            "Epoch 25/300\n",
            "141/141 - 61s - loss: 1.9473 - val_loss: 1.9530\n",
            "\n",
            "Epoch 00025: val_loss improved from 2.03182 to 1.95302, saving model to model.h5\n",
            "Epoch 26/300\n",
            "141/141 - 61s - loss: 1.8799 - val_loss: 1.8987\n",
            "\n",
            "Epoch 00026: val_loss improved from 1.95302 to 1.89875, saving model to model.h5\n",
            "Epoch 27/300\n",
            "141/141 - 61s - loss: 1.8124 - val_loss: 1.8315\n",
            "\n",
            "Epoch 00027: val_loss improved from 1.89875 to 1.83150, saving model to model.h5\n",
            "Epoch 28/300\n",
            "141/141 - 60s - loss: 1.7404 - val_loss: 1.7661\n",
            "\n",
            "Epoch 00028: val_loss improved from 1.83150 to 1.76611, saving model to model.h5\n",
            "Epoch 29/300\n",
            "141/141 - 60s - loss: 1.6710 - val_loss: 1.7100\n",
            "\n",
            "Epoch 00029: val_loss improved from 1.76611 to 1.70998, saving model to model.h5\n",
            "Epoch 30/300\n",
            "141/141 - 60s - loss: 1.6060 - val_loss: 1.6528\n",
            "\n",
            "Epoch 00030: val_loss improved from 1.70998 to 1.65277, saving model to model.h5\n",
            "Epoch 31/300\n",
            "141/141 - 61s - loss: 1.5403 - val_loss: 1.5926\n",
            "\n",
            "Epoch 00031: val_loss improved from 1.65277 to 1.59262, saving model to model.h5\n",
            "Epoch 32/300\n",
            "141/141 - 61s - loss: 1.4817 - val_loss: 1.5503\n",
            "\n",
            "Epoch 00032: val_loss improved from 1.59262 to 1.55028, saving model to model.h5\n",
            "Epoch 33/300\n",
            "141/141 - 61s - loss: 1.4227 - val_loss: 1.4931\n",
            "\n",
            "Epoch 00033: val_loss improved from 1.55028 to 1.49310, saving model to model.h5\n",
            "Epoch 34/300\n",
            "141/141 - 61s - loss: 1.3581 - val_loss: 1.4312\n",
            "\n",
            "Epoch 00034: val_loss improved from 1.49310 to 1.43122, saving model to model.h5\n",
            "Epoch 35/300\n",
            "141/141 - 61s - loss: 1.2988 - val_loss: 1.3802\n",
            "\n",
            "Epoch 00035: val_loss improved from 1.43122 to 1.38016, saving model to model.h5\n",
            "Epoch 36/300\n",
            "141/141 - 61s - loss: 1.2478 - val_loss: 1.3582\n",
            "\n",
            "Epoch 00036: val_loss improved from 1.38016 to 1.35819, saving model to model.h5\n",
            "Epoch 37/300\n",
            "141/141 - 61s - loss: 1.1906 - val_loss: 1.2793\n",
            "\n",
            "Epoch 00037: val_loss improved from 1.35819 to 1.27932, saving model to model.h5\n",
            "Epoch 38/300\n",
            "141/141 - 60s - loss: 1.1323 - val_loss: 1.2358\n",
            "\n",
            "Epoch 00038: val_loss improved from 1.27932 to 1.23578, saving model to model.h5\n",
            "Epoch 39/300\n",
            "141/141 - 61s - loss: 1.0876 - val_loss: 1.2167\n",
            "\n",
            "Epoch 00039: val_loss improved from 1.23578 to 1.21668, saving model to model.h5\n",
            "Epoch 40/300\n",
            "141/141 - 61s - loss: 1.0450 - val_loss: 1.1628\n",
            "\n",
            "Epoch 00040: val_loss improved from 1.21668 to 1.16278, saving model to model.h5\n",
            "Epoch 41/300\n",
            "141/141 - 61s - loss: 0.9918 - val_loss: 1.1204\n",
            "\n",
            "Epoch 00041: val_loss improved from 1.16278 to 1.12038, saving model to model.h5\n",
            "Epoch 42/300\n",
            "141/141 - 61s - loss: 0.9405 - val_loss: 1.0760\n",
            "\n",
            "Epoch 00042: val_loss improved from 1.12038 to 1.07600, saving model to model.h5\n",
            "Epoch 43/300\n",
            "141/141 - 61s - loss: 0.8925 - val_loss: 1.0428\n",
            "\n",
            "Epoch 00043: val_loss improved from 1.07600 to 1.04279, saving model to model.h5\n",
            "Epoch 44/300\n",
            "141/141 - 61s - loss: 0.8493 - val_loss: 1.0125\n",
            "\n",
            "Epoch 00044: val_loss improved from 1.04279 to 1.01254, saving model to model.h5\n",
            "Epoch 45/300\n",
            "141/141 - 61s - loss: 0.8108 - val_loss: 0.9762\n",
            "\n",
            "Epoch 00045: val_loss improved from 1.01254 to 0.97623, saving model to model.h5\n",
            "Epoch 46/300\n",
            "141/141 - 61s - loss: 0.7714 - val_loss: 0.9399\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.97623 to 0.93986, saving model to model.h5\n",
            "Epoch 47/300\n",
            "141/141 - 61s - loss: 0.7318 - val_loss: 0.9150\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.93986 to 0.91501, saving model to model.h5\n",
            "Epoch 48/300\n",
            "141/141 - 60s - loss: 0.6971 - val_loss: 0.8994\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.91501 to 0.89937, saving model to model.h5\n",
            "Epoch 49/300\n",
            "141/141 - 60s - loss: 0.6656 - val_loss: 0.8582\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.89937 to 0.85820, saving model to model.h5\n",
            "Epoch 50/300\n",
            "141/141 - 61s - loss: 0.6281 - val_loss: 0.8267\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.85820 to 0.82669, saving model to model.h5\n",
            "Epoch 51/300\n",
            "141/141 - 60s - loss: 0.5963 - val_loss: 0.8038\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.82669 to 0.80377, saving model to model.h5\n",
            "Epoch 52/300\n",
            "141/141 - 61s - loss: 0.5721 - val_loss: 0.7845\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.80377 to 0.78452, saving model to model.h5\n",
            "Epoch 53/300\n",
            "141/141 - 60s - loss: 0.5434 - val_loss: 0.7589\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.78452 to 0.75892, saving model to model.h5\n",
            "Epoch 54/300\n",
            "141/141 - 61s - loss: 0.5153 - val_loss: 0.7389\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.75892 to 0.73892, saving model to model.h5\n",
            "Epoch 55/300\n",
            "141/141 - 61s - loss: 0.4866 - val_loss: 0.7228\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.73892 to 0.72278, saving model to model.h5\n",
            "Epoch 56/300\n",
            "141/141 - 61s - loss: 0.4609 - val_loss: 0.7017\n",
            "\n",
            "Epoch 00056: val_loss improved from 0.72278 to 0.70172, saving model to model.h5\n",
            "Epoch 57/300\n",
            "141/141 - 62s - loss: 0.4407 - val_loss: 0.6911\n",
            "\n",
            "Epoch 00057: val_loss improved from 0.70172 to 0.69109, saving model to model.h5\n",
            "Epoch 58/300\n",
            "141/141 - 61s - loss: 0.4216 - val_loss: 0.6697\n",
            "\n",
            "Epoch 00058: val_loss improved from 0.69109 to 0.66973, saving model to model.h5\n",
            "Epoch 59/300\n",
            "141/141 - 61s - loss: 0.3986 - val_loss: 0.6498\n",
            "\n",
            "Epoch 00059: val_loss improved from 0.66973 to 0.64980, saving model to model.h5\n",
            "Epoch 60/300\n",
            "141/141 - 61s - loss: 0.3749 - val_loss: 0.6375\n",
            "\n",
            "Epoch 00060: val_loss improved from 0.64980 to 0.63746, saving model to model.h5\n",
            "Epoch 61/300\n",
            "141/141 - 60s - loss: 0.3563 - val_loss: 0.6160\n",
            "\n",
            "Epoch 00061: val_loss improved from 0.63746 to 0.61602, saving model to model.h5\n",
            "Epoch 62/300\n",
            "141/141 - 61s - loss: 0.3343 - val_loss: 0.6041\n",
            "\n",
            "Epoch 00062: val_loss improved from 0.61602 to 0.60414, saving model to model.h5\n",
            "Epoch 63/300\n",
            "141/141 - 60s - loss: 0.3164 - val_loss: 0.5904\n",
            "\n",
            "Epoch 00063: val_loss improved from 0.60414 to 0.59037, saving model to model.h5\n",
            "Epoch 64/300\n",
            "141/141 - 60s - loss: 0.2962 - val_loss: 0.5774\n",
            "\n",
            "Epoch 00064: val_loss improved from 0.59037 to 0.57743, saving model to model.h5\n",
            "Epoch 65/300\n",
            "141/141 - 60s - loss: 0.2845 - val_loss: 0.5826\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.57743\n",
            "Epoch 66/300\n",
            "141/141 - 60s - loss: 0.2807 - val_loss: 0.5664\n",
            "\n",
            "Epoch 00066: val_loss improved from 0.57743 to 0.56638, saving model to model.h5\n",
            "Epoch 67/300\n",
            "141/141 - 61s - loss: 0.2695 - val_loss: 0.5601\n",
            "\n",
            "Epoch 00067: val_loss improved from 0.56638 to 0.56009, saving model to model.h5\n",
            "Epoch 68/300\n",
            "141/141 - 60s - loss: 0.2502 - val_loss: 0.5390\n",
            "\n",
            "Epoch 00068: val_loss improved from 0.56009 to 0.53901, saving model to model.h5\n",
            "Epoch 69/300\n",
            "141/141 - 60s - loss: 0.2304 - val_loss: 0.5286\n",
            "\n",
            "Epoch 00069: val_loss improved from 0.53901 to 0.52855, saving model to model.h5\n",
            "Epoch 70/300\n",
            "141/141 - 60s - loss: 0.2142 - val_loss: 0.5190\n",
            "\n",
            "Epoch 00070: val_loss improved from 0.52855 to 0.51896, saving model to model.h5\n",
            "Epoch 71/300\n",
            "141/141 - 60s - loss: 0.2021 - val_loss: 0.5078\n",
            "\n",
            "Epoch 00071: val_loss improved from 0.51896 to 0.50779, saving model to model.h5\n",
            "Epoch 72/300\n",
            "141/141 - 60s - loss: 0.1946 - val_loss: 0.5066\n",
            "\n",
            "Epoch 00072: val_loss improved from 0.50779 to 0.50664, saving model to model.h5\n",
            "Epoch 73/300\n",
            "141/141 - 61s - loss: 0.1855 - val_loss: 0.4996\n",
            "\n",
            "Epoch 00073: val_loss improved from 0.50664 to 0.49955, saving model to model.h5\n",
            "Epoch 74/300\n",
            "141/141 - 61s - loss: 0.1744 - val_loss: 0.4923\n",
            "\n",
            "Epoch 00074: val_loss improved from 0.49955 to 0.49234, saving model to model.h5\n",
            "Epoch 75/300\n",
            "141/141 - 61s - loss: 0.1711 - val_loss: 0.4950\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.49234\n",
            "Epoch 76/300\n",
            "141/141 - 61s - loss: 0.1635 - val_loss: 0.4846\n",
            "\n",
            "Epoch 00076: val_loss improved from 0.49234 to 0.48460, saving model to model.h5\n",
            "Epoch 77/300\n",
            "141/141 - 61s - loss: 0.1569 - val_loss: 0.4860\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.48460\n",
            "Epoch 78/300\n",
            "141/141 - 62s - loss: 0.1510 - val_loss: 0.4810\n",
            "\n",
            "Epoch 00078: val_loss improved from 0.48460 to 0.48103, saving model to model.h5\n",
            "Epoch 79/300\n",
            "141/141 - 61s - loss: 0.1418 - val_loss: 0.4679\n",
            "\n",
            "Epoch 00079: val_loss improved from 0.48103 to 0.46787, saving model to model.h5\n",
            "Epoch 80/300\n",
            "141/141 - 61s - loss: 0.1326 - val_loss: 0.4631\n",
            "\n",
            "Epoch 00080: val_loss improved from 0.46787 to 0.46311, saving model to model.h5\n",
            "Epoch 81/300\n",
            "141/141 - 61s - loss: 0.1245 - val_loss: 0.4596\n",
            "\n",
            "Epoch 00081: val_loss improved from 0.46311 to 0.45962, saving model to model.h5\n",
            "Epoch 82/300\n",
            "141/141 - 61s - loss: 0.1169 - val_loss: 0.4514\n",
            "\n",
            "Epoch 00082: val_loss improved from 0.45962 to 0.45139, saving model to model.h5\n",
            "Epoch 83/300\n",
            "141/141 - 62s - loss: 0.1106 - val_loss: 0.4502\n",
            "\n",
            "Epoch 00083: val_loss improved from 0.45139 to 0.45018, saving model to model.h5\n",
            "Epoch 84/300\n",
            "141/141 - 61s - loss: 0.1045 - val_loss: 0.4473\n",
            "\n",
            "Epoch 00084: val_loss improved from 0.45018 to 0.44730, saving model to model.h5\n",
            "Epoch 85/300\n",
            "141/141 - 61s - loss: 0.1010 - val_loss: 0.4480\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.44730\n",
            "Epoch 86/300\n",
            "141/141 - 61s - loss: 0.0990 - val_loss: 0.4468\n",
            "\n",
            "Epoch 00086: val_loss improved from 0.44730 to 0.44684, saving model to model.h5\n",
            "Epoch 87/300\n",
            "141/141 - 61s - loss: 0.0976 - val_loss: 0.4453\n",
            "\n",
            "Epoch 00087: val_loss improved from 0.44684 to 0.44529, saving model to model.h5\n",
            "Epoch 88/300\n",
            "141/141 - 62s - loss: 0.0961 - val_loss: 0.4454\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.44529\n",
            "Epoch 89/300\n",
            "141/141 - 61s - loss: 0.0957 - val_loss: 0.4496\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.44529\n",
            "Epoch 90/300\n",
            "141/141 - 61s - loss: 0.1004 - val_loss: 0.4539\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.44529\n",
            "Epoch 91/300\n",
            "141/141 - 61s - loss: 0.1103 - val_loss: 0.4635\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.44529\n",
            "Epoch 92/300\n",
            "141/141 - 61s - loss: 0.1095 - val_loss: 0.4547\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.44529\n",
            "Epoch 93/300\n",
            "141/141 - 61s - loss: 0.1012 - val_loss: 0.4458\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.44529\n",
            "Epoch 94/300\n",
            "141/141 - 61s - loss: 0.0851 - val_loss: 0.4339\n",
            "\n",
            "Epoch 00094: val_loss improved from 0.44529 to 0.43393, saving model to model.h5\n",
            "Epoch 95/300\n",
            "141/141 - 61s - loss: 0.0728 - val_loss: 0.4356\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.43393\n",
            "Epoch 96/300\n",
            "141/141 - 60s - loss: 0.0688 - val_loss: 0.4306\n",
            "\n",
            "Epoch 00096: val_loss improved from 0.43393 to 0.43057, saving model to model.h5\n",
            "Epoch 97/300\n",
            "141/141 - 61s - loss: 0.0680 - val_loss: 0.4287\n",
            "\n",
            "Epoch 00097: val_loss improved from 0.43057 to 0.42869, saving model to model.h5\n",
            "Epoch 98/300\n",
            "141/141 - 62s - loss: 0.0642 - val_loss: 0.4259\n",
            "\n",
            "Epoch 00098: val_loss improved from 0.42869 to 0.42593, saving model to model.h5\n",
            "Epoch 99/300\n",
            "141/141 - 62s - loss: 0.0596 - val_loss: 0.4246\n",
            "\n",
            "Epoch 00099: val_loss improved from 0.42593 to 0.42458, saving model to model.h5\n",
            "Epoch 100/300\n",
            "141/141 - 61s - loss: 0.0585 - val_loss: 0.4245\n",
            "\n",
            "Epoch 00100: val_loss improved from 0.42458 to 0.42454, saving model to model.h5\n",
            "Epoch 101/300\n",
            "141/141 - 61s - loss: 0.0559 - val_loss: 0.4246\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.42454\n",
            "Epoch 102/300\n",
            "141/141 - 61s - loss: 0.0559 - val_loss: 0.4239\n",
            "\n",
            "Epoch 00102: val_loss improved from 0.42454 to 0.42392, saving model to model.h5\n",
            "Epoch 103/300\n",
            "141/141 - 61s - loss: 0.0564 - val_loss: 0.4265\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.42392\n",
            "Epoch 104/300\n",
            "141/141 - 61s - loss: 0.0557 - val_loss: 0.4293\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.42392\n",
            "Epoch 105/300\n",
            "141/141 - 61s - loss: 0.0625 - val_loss: 0.4388\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.42392\n",
            "Epoch 106/300\n",
            "141/141 - 61s - loss: 0.0942 - val_loss: 0.4829\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.42392\n",
            "Epoch 107/300\n",
            "141/141 - 61s - loss: 0.1496 - val_loss: 0.4898\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.42392\n",
            "Epoch 108/300\n",
            "141/141 - 61s - loss: 0.1189 - val_loss: 0.4580\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.42392\n",
            "Epoch 109/300\n",
            "141/141 - 61s - loss: 0.0867 - val_loss: 0.4397\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.42392\n",
            "Epoch 110/300\n",
            "141/141 - 61s - loss: 0.0617 - val_loss: 0.4257\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.42392\n",
            "Epoch 111/300\n",
            "141/141 - 60s - loss: 0.0510 - val_loss: 0.4203\n",
            "\n",
            "Epoch 00111: val_loss improved from 0.42392 to 0.42033, saving model to model.h5\n",
            "Epoch 112/300\n",
            "141/141 - 61s - loss: 0.0465 - val_loss: 0.4210\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.42033\n",
            "Epoch 113/300\n",
            "141/141 - 61s - loss: 0.0448 - val_loss: 0.4207\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.42033\n",
            "Epoch 114/300\n",
            "141/141 - 61s - loss: 0.0448 - val_loss: 0.4221\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.42033\n",
            "Epoch 115/300\n",
            "141/141 - 60s - loss: 0.0447 - val_loss: 0.4218\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.42033\n",
            "Epoch 116/300\n",
            "141/141 - 61s - loss: 0.0444 - val_loss: 0.4238\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.42033\n",
            "Epoch 117/300\n",
            "141/141 - 60s - loss: 0.0434 - val_loss: 0.4230\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.42033\n",
            "Epoch 118/300\n",
            "141/141 - 60s - loss: 0.0433 - val_loss: 0.4234\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.42033\n",
            "Epoch 119/300\n",
            "141/141 - 61s - loss: 0.0433 - val_loss: 0.4246\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.42033\n",
            "Epoch 120/300\n",
            "141/141 - 61s - loss: 0.0425 - val_loss: 0.4251\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.42033\n",
            "Epoch 121/300\n",
            "141/141 - 61s - loss: 0.0425 - val_loss: 0.4266\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.42033\n",
            "Epoch 122/300\n",
            "141/141 - 61s - loss: 0.0435 - val_loss: 0.4289\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.42033\n",
            "Epoch 123/300\n",
            "141/141 - 61s - loss: 0.0444 - val_loss: 0.4262\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.42033\n",
            "Epoch 124/300\n",
            "141/141 - 60s - loss: 0.0466 - val_loss: 0.4337\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.42033\n",
            "Epoch 125/300\n",
            "141/141 - 61s - loss: 0.0642 - val_loss: 0.4962\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.42033\n",
            "Epoch 126/300\n",
            "141/141 - 60s - loss: 0.1314 - val_loss: 0.5190\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.42033\n",
            "Epoch 127/300\n",
            "141/141 - 60s - loss: 0.1380 - val_loss: 0.4665\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.42033\n",
            "Epoch 128/300\n",
            "141/141 - 60s - loss: 0.0858 - val_loss: 0.4414\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.42033\n",
            "Epoch 129/300\n",
            "141/141 - 60s - loss: 0.0569 - val_loss: 0.4297\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.42033\n",
            "Epoch 130/300\n",
            "141/141 - 60s - loss: 0.0464 - val_loss: 0.4277\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.42033\n",
            "Epoch 131/300\n",
            "141/141 - 60s - loss: 0.0442 - val_loss: 0.4256\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.42033\n",
            "Epoch 132/300\n",
            "141/141 - 60s - loss: 0.0413 - val_loss: 0.4253\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.42033\n",
            "Epoch 133/300\n",
            "141/141 - 60s - loss: 0.0403 - val_loss: 0.4238\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.42033\n",
            "Epoch 134/300\n",
            "141/141 - 60s - loss: 0.0386 - val_loss: 0.4251\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.42033\n",
            "Epoch 135/300\n",
            "141/141 - 60s - loss: 0.0384 - val_loss: 0.4248\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.42033\n",
            "Epoch 136/300\n",
            "141/141 - 60s - loss: 0.0380 - val_loss: 0.4249\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.42033\n",
            "Epoch 137/300\n",
            "141/141 - 60s - loss: 0.0380 - val_loss: 0.4253\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.42033\n",
            "Epoch 138/300\n",
            "141/141 - 61s - loss: 0.0382 - val_loss: 0.4249\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.42033\n",
            "Epoch 139/300\n",
            "141/141 - 60s - loss: 0.0376 - val_loss: 0.4263\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.42033\n",
            "Epoch 140/300\n",
            "141/141 - 60s - loss: 0.0382 - val_loss: 0.4260\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.42033\n",
            "Epoch 141/300\n",
            "141/141 - 60s - loss: 0.0380 - val_loss: 0.4270\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.42033\n",
            "Epoch 142/300\n",
            "141/141 - 60s - loss: 0.0387 - val_loss: 0.4273\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.42033\n",
            "Epoch 143/300\n",
            "141/141 - 60s - loss: 0.0389 - val_loss: 0.4278\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.42033\n",
            "Epoch 144/300\n",
            "141/141 - 60s - loss: 0.0406 - val_loss: 0.4324\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.42033\n",
            "Epoch 145/300\n",
            "141/141 - 60s - loss: 0.0501 - val_loss: 0.4530\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.42033\n",
            "Epoch 146/300\n",
            "141/141 - 61s - loss: 0.0921 - val_loss: 0.4958\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.42033\n",
            "Epoch 147/300\n",
            "141/141 - 61s - loss: 0.1074 - val_loss: 0.4700\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.42033\n",
            "Epoch 148/300\n",
            "141/141 - 60s - loss: 0.0778 - val_loss: 0.4477\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.42033\n",
            "Epoch 149/300\n",
            "141/141 - 60s - loss: 0.0560 - val_loss: 0.4389\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.42033\n",
            "Epoch 150/300\n",
            "141/141 - 60s - loss: 0.0454 - val_loss: 0.4334\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.42033\n",
            "Epoch 151/300\n",
            "141/141 - 61s - loss: 0.0410 - val_loss: 0.4319\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.42033\n",
            "Epoch 152/300\n",
            "141/141 - 60s - loss: 0.0382 - val_loss: 0.4284\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.42033\n",
            "Epoch 153/300\n",
            "141/141 - 61s - loss: 0.0366 - val_loss: 0.4290\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.42033\n",
            "Epoch 154/300\n",
            "141/141 - 60s - loss: 0.0360 - val_loss: 0.4310\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.42033\n",
            "Epoch 155/300\n",
            "141/141 - 60s - loss: 0.0357 - val_loss: 0.4288\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.42033\n",
            "Epoch 156/300\n",
            "141/141 - 61s - loss: 0.0355 - val_loss: 0.4290\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.42033\n",
            "Epoch 157/300\n",
            "141/141 - 62s - loss: 0.0355 - val_loss: 0.4296\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.42033\n",
            "Epoch 158/300\n",
            "141/141 - 63s - loss: 0.0355 - val_loss: 0.4294\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.42033\n",
            "Epoch 159/300\n",
            "141/141 - 62s - loss: 0.0361 - val_loss: 0.4300\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.42033\n",
            "Epoch 160/300\n",
            "141/141 - 62s - loss: 0.0357 - val_loss: 0.4305\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.42033\n",
            "Epoch 161/300\n",
            "141/141 - 63s - loss: 0.0359 - val_loss: 0.4307\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.42033\n",
            "Epoch 162/300\n",
            "141/141 - 61s - loss: 0.0365 - val_loss: 0.4325\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.42033\n",
            "Epoch 163/300\n",
            "141/141 - 63s - loss: 0.0370 - val_loss: 0.4318\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.42033\n",
            "Epoch 164/300\n",
            "141/141 - 61s - loss: 0.0386 - val_loss: 0.4369\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.42033\n",
            "Epoch 165/300\n",
            "141/141 - 62s - loss: 0.0522 - val_loss: 0.4608\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.42033\n",
            "Epoch 166/300\n",
            "141/141 - 60s - loss: 0.0881 - val_loss: 0.4864\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.42033\n",
            "Epoch 167/300\n",
            "141/141 - 60s - loss: 0.0986 - val_loss: 0.4666\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.42033\n",
            "Epoch 168/300\n",
            "141/141 - 61s - loss: 0.0836 - val_loss: 0.4508\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.42033\n",
            "Epoch 169/300\n",
            "141/141 - 61s - loss: 0.0522 - val_loss: 0.4401\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.42033\n",
            "Epoch 170/300\n",
            "141/141 - 62s - loss: 0.0415 - val_loss: 0.4328\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.42033\n",
            "Epoch 171/300\n",
            "141/141 - 63s - loss: 0.0366 - val_loss: 0.4329\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.42033\n",
            "Epoch 172/300\n",
            "141/141 - 63s - loss: 0.0355 - val_loss: 0.4336\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.42033\n",
            "Epoch 173/300\n",
            "141/141 - 64s - loss: 0.0345 - val_loss: 0.4321\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.42033\n",
            "Epoch 174/300\n",
            "141/141 - 63s - loss: 0.0339 - val_loss: 0.4323\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.42033\n",
            "Epoch 175/300\n",
            "141/141 - 63s - loss: 0.0340 - val_loss: 0.4329\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.42033\n",
            "Epoch 176/300\n",
            "141/141 - 64s - loss: 0.0340 - val_loss: 0.4327\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.42033\n",
            "Epoch 177/300\n",
            "141/141 - 64s - loss: 0.0339 - val_loss: 0.4339\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.42033\n",
            "Epoch 178/300\n",
            "141/141 - 64s - loss: 0.0338 - val_loss: 0.4333\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.42033\n",
            "Epoch 179/300\n",
            "141/141 - 63s - loss: 0.0338 - val_loss: 0.4329\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.42033\n",
            "Epoch 180/300\n",
            "141/141 - 63s - loss: 0.0344 - val_loss: 0.4336\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.42033\n",
            "Epoch 181/300\n",
            "141/141 - 64s - loss: 0.0344 - val_loss: 0.4329\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.42033\n",
            "Epoch 182/300\n",
            "141/141 - 63s - loss: 0.0345 - val_loss: 0.4343\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.42033\n",
            "Epoch 183/300\n",
            "141/141 - 64s - loss: 0.0346 - val_loss: 0.4345\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.42033\n",
            "Epoch 184/300\n",
            "141/141 - 61s - loss: 0.0346 - val_loss: 0.4356\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.42033\n",
            "Epoch 185/300\n",
            "141/141 - 60s - loss: 0.0360 - val_loss: 0.4396\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.42033\n",
            "Epoch 186/300\n",
            "141/141 - 61s - loss: 0.0434 - val_loss: 0.4588\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.42033\n",
            "Epoch 187/300\n",
            "141/141 - 61s - loss: 0.0858 - val_loss: 0.4875\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.42033\n",
            "Epoch 188/300\n",
            "141/141 - 61s - loss: 0.1013 - val_loss: 0.4722\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.42033\n",
            "Epoch 189/300\n",
            "141/141 - 60s - loss: 0.0720 - val_loss: 0.4534\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.42033\n",
            "Epoch 190/300\n",
            "141/141 - 60s - loss: 0.0473 - val_loss: 0.4392\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.42033\n",
            "Epoch 191/300\n",
            "141/141 - 61s - loss: 0.0380 - val_loss: 0.4359\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.42033\n",
            "Epoch 192/300\n",
            "141/141 - 61s - loss: 0.0345 - val_loss: 0.4330\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.42033\n",
            "Epoch 193/300\n",
            "141/141 - 61s - loss: 0.0330 - val_loss: 0.4338\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.42033\n",
            "Epoch 194/300\n",
            "141/141 - 60s - loss: 0.0327 - val_loss: 0.4339\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.42033\n",
            "Epoch 195/300\n",
            "141/141 - 60s - loss: 0.0327 - val_loss: 0.4339\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.42033\n",
            "Epoch 196/300\n",
            "141/141 - 60s - loss: 0.0326 - val_loss: 0.4345\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.42033\n",
            "Epoch 197/300\n",
            "141/141 - 61s - loss: 0.0323 - val_loss: 0.4339\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.42033\n",
            "Epoch 198/300\n",
            "141/141 - 61s - loss: 0.0327 - val_loss: 0.4338\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.42033\n",
            "Epoch 199/300\n",
            "141/141 - 61s - loss: 0.0325 - val_loss: 0.4346\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.42033\n",
            "Epoch 200/300\n",
            "141/141 - 60s - loss: 0.0331 - val_loss: 0.4351\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.42033\n",
            "Epoch 201/300\n",
            "141/141 - 60s - loss: 0.0332 - val_loss: 0.4361\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.42033\n",
            "Epoch 202/300\n",
            "141/141 - 61s - loss: 0.0334 - val_loss: 0.4358\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.42033\n",
            "Epoch 203/300\n",
            "141/141 - 60s - loss: 0.0333 - val_loss: 0.4361\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.42033\n",
            "Epoch 204/300\n",
            "141/141 - 60s - loss: 0.0335 - val_loss: 0.4363\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.42033\n",
            "Epoch 205/300\n",
            "141/141 - 61s - loss: 0.0336 - val_loss: 0.4380\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.42033\n",
            "Epoch 206/300\n",
            "141/141 - 60s - loss: 0.0336 - val_loss: 0.4364\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.42033\n",
            "Epoch 207/300\n",
            "141/141 - 61s - loss: 0.0340 - val_loss: 0.4375\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.42033\n",
            "Epoch 208/300\n",
            "141/141 - 61s - loss: 0.0348 - val_loss: 0.4371\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.42033\n",
            "Epoch 209/300\n",
            "141/141 - 60s - loss: 0.0350 - val_loss: 0.4384\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.42033\n",
            "Epoch 210/300\n",
            "141/141 - 61s - loss: 0.0361 - val_loss: 0.4413\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.42033\n",
            "Epoch 211/300\n",
            "141/141 - 60s - loss: 0.0522 - val_loss: 0.4834\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.42033\n",
            "Epoch 212/300\n",
            "141/141 - 61s - loss: 0.1012 - val_loss: 0.4941\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.42033\n",
            "Epoch 213/300\n",
            "141/141 - 61s - loss: 0.0886 - val_loss: 0.4616\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.42033\n",
            "Epoch 214/300\n",
            "141/141 - 61s - loss: 0.0562 - val_loss: 0.4501\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.42033\n",
            "Epoch 215/300\n",
            "141/141 - 60s - loss: 0.0440 - val_loss: 0.4419\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.42033\n",
            "Epoch 216/300\n",
            "141/141 - 60s - loss: 0.0385 - val_loss: 0.4381\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.42033\n",
            "Epoch 217/300\n",
            "141/141 - 60s - loss: 0.0343 - val_loss: 0.4387\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.42033\n",
            "Epoch 218/300\n",
            "141/141 - 61s - loss: 0.0328 - val_loss: 0.4361\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.42033\n",
            "Epoch 219/300\n",
            "141/141 - 60s - loss: 0.0319 - val_loss: 0.4363\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.42033\n",
            "Epoch 220/300\n",
            "141/141 - 60s - loss: 0.0318 - val_loss: 0.4367\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.42033\n",
            "Epoch 221/300\n",
            "141/141 - 61s - loss: 0.0316 - val_loss: 0.4371\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.42033\n",
            "Epoch 222/300\n",
            "141/141 - 61s - loss: 0.0315 - val_loss: 0.4375\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.42033\n",
            "Epoch 223/300\n",
            "141/141 - 61s - loss: 0.0315 - val_loss: 0.4372\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.42033\n",
            "Epoch 224/300\n",
            "141/141 - 60s - loss: 0.0314 - val_loss: 0.4385\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.42033\n",
            "Epoch 225/300\n",
            "141/141 - 60s - loss: 0.0317 - val_loss: 0.4390\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.42033\n",
            "Epoch 226/300\n",
            "141/141 - 60s - loss: 0.0319 - val_loss: 0.4395\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.42033\n",
            "Epoch 227/300\n",
            "141/141 - 61s - loss: 0.0321 - val_loss: 0.4395\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.42033\n",
            "Epoch 228/300\n",
            "141/141 - 61s - loss: 0.0322 - val_loss: 0.4383\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.42033\n",
            "Epoch 229/300\n",
            "141/141 - 61s - loss: 0.0326 - val_loss: 0.4394\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.42033\n",
            "Epoch 230/300\n",
            "141/141 - 60s - loss: 0.0326 - val_loss: 0.4389\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.42033\n",
            "Epoch 231/300\n",
            "141/141 - 60s - loss: 0.0327 - val_loss: 0.4400\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.42033\n",
            "Epoch 232/300\n",
            "141/141 - 61s - loss: 0.0331 - val_loss: 0.4388\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.42033\n",
            "Epoch 233/300\n",
            "141/141 - 61s - loss: 0.0332 - val_loss: 0.4401\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.42033\n",
            "Epoch 234/300\n",
            "141/141 - 61s - loss: 0.0332 - val_loss: 0.4412\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.42033\n",
            "Epoch 235/300\n",
            "141/141 - 61s - loss: 0.0351 - val_loss: 0.4441\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.42033\n",
            "Epoch 236/300\n",
            "141/141 - 61s - loss: 0.0505 - val_loss: 0.4874\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.42033\n",
            "Epoch 237/300\n",
            "141/141 - 61s - loss: 0.0910 - val_loss: 0.4945\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.42033\n",
            "Epoch 238/300\n",
            "141/141 - 61s - loss: 0.0874 - val_loss: 0.4649\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.42033\n",
            "Epoch 239/300\n",
            "141/141 - 61s - loss: 0.0570 - val_loss: 0.4479\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.42033\n",
            "Epoch 240/300\n",
            "141/141 - 61s - loss: 0.0392 - val_loss: 0.4407\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.42033\n",
            "Epoch 241/300\n",
            "141/141 - 61s - loss: 0.0342 - val_loss: 0.4406\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.42033\n",
            "Epoch 242/300\n",
            "141/141 - 61s - loss: 0.0324 - val_loss: 0.4400\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.42033\n",
            "Epoch 243/300\n",
            "141/141 - 61s - loss: 0.0316 - val_loss: 0.4407\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.42033\n",
            "Epoch 244/300\n",
            "141/141 - 62s - loss: 0.0313 - val_loss: 0.4410\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.42033\n",
            "Epoch 245/300\n",
            "141/141 - 61s - loss: 0.0312 - val_loss: 0.4408\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.42033\n",
            "Epoch 246/300\n",
            "141/141 - 61s - loss: 0.0311 - val_loss: 0.4419\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.42033\n",
            "Epoch 247/300\n",
            "141/141 - 61s - loss: 0.0315 - val_loss: 0.4409\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.42033\n",
            "Epoch 248/300\n",
            "141/141 - 61s - loss: 0.0314 - val_loss: 0.4412\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.42033\n",
            "Epoch 249/300\n",
            "141/141 - 61s - loss: 0.0313 - val_loss: 0.4436\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.42033\n",
            "Epoch 250/300\n",
            "141/141 - 60s - loss: 0.0314 - val_loss: 0.4433\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.42033\n",
            "Epoch 251/300\n",
            "141/141 - 61s - loss: 0.0317 - val_loss: 0.4421\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.42033\n",
            "Epoch 252/300\n",
            "141/141 - 61s - loss: 0.0317 - val_loss: 0.4439\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.42033\n",
            "Epoch 253/300\n",
            "141/141 - 61s - loss: 0.0317 - val_loss: 0.4443\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.42033\n",
            "Epoch 254/300\n",
            "141/141 - 61s - loss: 0.0322 - val_loss: 0.4430\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.42033\n",
            "Epoch 255/300\n",
            "141/141 - 61s - loss: 0.0321 - val_loss: 0.4436\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.42033\n",
            "Epoch 256/300\n",
            "141/141 - 61s - loss: 0.0321 - val_loss: 0.4441\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.42033\n",
            "Epoch 257/300\n",
            "141/141 - 61s - loss: 0.0326 - val_loss: 0.4446\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.42033\n",
            "Epoch 258/300\n",
            "141/141 - 61s - loss: 0.0330 - val_loss: 0.4436\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.42033\n",
            "Epoch 259/300\n",
            "141/141 - 61s - loss: 0.0335 - val_loss: 0.4429\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.42033\n",
            "Epoch 260/300\n",
            "141/141 - 60s - loss: 0.0338 - val_loss: 0.4463\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.42033\n",
            "Epoch 261/300\n",
            "141/141 - 60s - loss: 0.0354 - val_loss: 0.4514\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.42033\n",
            "Epoch 262/300\n",
            "141/141 - 60s - loss: 0.0716 - val_loss: 0.5165\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.42033\n",
            "Epoch 263/300\n",
            "141/141 - 60s - loss: 0.0976 - val_loss: 0.4754\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.42033\n",
            "Epoch 264/300\n",
            "141/141 - 61s - loss: 0.0632 - val_loss: 0.4571\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.42033\n",
            "Epoch 265/300\n",
            "141/141 - 60s - loss: 0.0439 - val_loss: 0.4495\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.42033\n",
            "Epoch 266/300\n",
            "141/141 - 60s - loss: 0.0362 - val_loss: 0.4446\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.42033\n",
            "Epoch 267/300\n",
            "141/141 - 60s - loss: 0.0329 - val_loss: 0.4443\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.42033\n",
            "Epoch 268/300\n",
            "141/141 - 60s - loss: 0.0325 - val_loss: 0.4451\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.42033\n",
            "Epoch 269/300\n",
            "141/141 - 60s - loss: 0.0317 - val_loss: 0.4441\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.42033\n",
            "Epoch 270/300\n",
            "141/141 - 61s - loss: 0.0308 - val_loss: 0.4440\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.42033\n",
            "Epoch 271/300\n",
            "141/141 - 61s - loss: 0.0305 - val_loss: 0.4450\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.42033\n",
            "Epoch 272/300\n",
            "141/141 - 61s - loss: 0.0305 - val_loss: 0.4445\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.42033\n",
            "Epoch 273/300\n",
            "141/141 - 61s - loss: 0.0310 - val_loss: 0.4452\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.42033\n",
            "Epoch 274/300\n",
            "141/141 - 61s - loss: 0.0309 - val_loss: 0.4448\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.42033\n",
            "Epoch 275/300\n",
            "141/141 - 61s - loss: 0.0306 - val_loss: 0.4454\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.42033\n",
            "Epoch 276/300\n",
            "141/141 - 60s - loss: 0.0309 - val_loss: 0.4453\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.42033\n",
            "Epoch 277/300\n",
            "141/141 - 60s - loss: 0.0308 - val_loss: 0.4465\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.42033\n",
            "Epoch 278/300\n",
            "141/141 - 60s - loss: 0.0312 - val_loss: 0.4474\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.42033\n",
            "Epoch 279/300\n",
            "141/141 - 61s - loss: 0.0314 - val_loss: 0.4462\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.42033\n",
            "Epoch 280/300\n",
            "141/141 - 60s - loss: 0.0316 - val_loss: 0.4467\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.42033\n",
            "Epoch 281/300\n",
            "141/141 - 60s - loss: 0.0319 - val_loss: 0.4479\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.42033\n",
            "Epoch 282/300\n",
            "141/141 - 60s - loss: 0.0315 - val_loss: 0.4454\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.42033\n",
            "Epoch 283/300\n",
            "141/141 - 60s - loss: 0.0315 - val_loss: 0.4483\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.42033\n",
            "Epoch 284/300\n",
            "141/141 - 60s - loss: 0.0319 - val_loss: 0.4463\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.42033\n",
            "Epoch 285/300\n",
            "141/141 - 60s - loss: 0.0322 - val_loss: 0.4474\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.42033\n",
            "Epoch 286/300\n",
            "141/141 - 60s - loss: 0.0338 - val_loss: 0.4536\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.42033\n",
            "Epoch 287/300\n",
            "141/141 - 60s - loss: 0.0690 - val_loss: 0.5086\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.42033\n",
            "Epoch 288/300\n",
            "141/141 - 60s - loss: 0.0906 - val_loss: 0.4766\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.42033\n",
            "Epoch 289/300\n",
            "141/141 - 60s - loss: 0.0572 - val_loss: 0.4572\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.42033\n",
            "Epoch 290/300\n",
            "141/141 - 60s - loss: 0.0402 - val_loss: 0.4473\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.42033\n",
            "Epoch 291/300\n",
            "141/141 - 61s - loss: 0.0333 - val_loss: 0.4470\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.42033\n",
            "Epoch 292/300\n",
            "141/141 - 60s - loss: 0.0321 - val_loss: 0.4474\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.42033\n",
            "Epoch 293/300\n",
            "141/141 - 60s - loss: 0.0312 - val_loss: 0.4466\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.42033\n",
            "Epoch 294/300\n",
            "141/141 - 60s - loss: 0.0304 - val_loss: 0.4467\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.42033\n",
            "Epoch 295/300\n",
            "141/141 - 60s - loss: 0.0305 - val_loss: 0.4465\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.42033\n",
            "Epoch 296/300\n",
            "141/141 - 61s - loss: 0.0301 - val_loss: 0.4487\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.42033\n",
            "Epoch 297/300\n",
            "141/141 - 61s - loss: 0.0305 - val_loss: 0.4478\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.42033\n",
            "Epoch 298/300\n",
            "141/141 - 60s - loss: 0.0305 - val_loss: 0.4477\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.42033\n",
            "Epoch 299/300\n",
            "141/141 - 60s - loss: 0.0303 - val_loss: 0.4474\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.42033\n",
            "Epoch 300/300\n",
            "141/141 - 60s - loss: 0.0302 - val_loss: 0.4486\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.42033\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f1944d38ef0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJBwK47nOCkK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3515e4e0-1371-478d-9ab8-1b518336753f"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# map an integer to a word\n",
        "def word_for_id(integer, tokenizer):\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == integer:\n",
        "\t\t\treturn word\n",
        "\treturn None\n",
        "\n",
        "# generate target given source sequence\n",
        "def predict_sequence(model, tokenizer, source):\n",
        "\tprediction = model.predict(source, verbose=0)[0]\n",
        "\tintegers = [argmax(vector) for vector in prediction]\n",
        "\ttarget = list()\n",
        "\tfor i in integers:\n",
        "\t\tword = word_for_id(i, tokenizer)\n",
        "\t\tif word is None:\n",
        "\t\t\tbreak\n",
        "\t\ttarget.append(word)\n",
        "\treturn ' '.join(target)\n",
        "\n",
        "# evaluate the skill of the model\n",
        "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
        "\tactual, predicted = list(), list()\n",
        "\tfor i, source in enumerate(sources):\n",
        "\t\t# translate encoded source text\n",
        "\t\tsource = source.reshape((1, source.shape[0]))\n",
        "\t\ttranslation = predict_sequence(model, eng_tokenizer, source)\n",
        "\t\traw_target, raw_src,test = raw_dataset[i]\n",
        "\t\tif i < 10:\n",
        "\t\t\tprint('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
        "\t\tactual.append([raw_target.split()])\n",
        "\t\tpredicted.append(translation.split())\n",
        "\t# calculate BLEU score\n",
        "\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-romanian-both.pkl')\n",
        "train = load_clean_sentences('english-romanian-train.pkl')\n",
        "test = load_clean_sentences('english-romanian-test.pkl')\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "# prepare data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "\n",
        "# load model\n",
        "model = load_model('model.h5')\n",
        "# test on some training sequences\n",
        "print('train')\n",
        "evaluate_model(model, eng_tokenizer, trainX, train)\n",
        "# test on some test sequences\n",
        "print('test')\n",
        "evaluate_model(model, eng_tokenizer, testX, test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train\n",
            "src=[nu pot sa o gasesc], target=[i cant find it], predicted=[i cant find it]\n",
            "src=[acest ceas trebuie reparat], target=[this watch needs to be fixed], predicted=[this watch needs to be fixed]\n",
            "src=[ajutor], target=[help], predicted=[help]\n",
            "src=[tom a mancat prea multa vata de zahar], target=[tom ate too much cotton candy], predicted=[tom ate too much cotton candy]\n",
            "src=[supa este groasa], target=[the soup is thick], predicted=[the soup is thick]\n",
            "src=[stai deoparte de acel loc], target=[keep away from there], predicted=[keep away from there]\n",
            "src=[cainele a venit alergand la ea], target=[the dog came running to her], predicted=[the dog came running to her]\n",
            "src=[nu trebuie sa o pierzi], target=[you ought not to miss it], predicted=[you ought not to miss it]\n",
            "src=[ai grija de tine], target=[take care], predicted=[take care]\n",
            "src=[casa aceea este a mea], target=[that house is mine], predicted=[that house is mine]\n",
            "BLEU-1: 0.979353\n",
            "BLEU-2: 0.971890\n",
            "BLEU-3: 0.966778\n",
            "BLEU-4: 0.943036\n",
            "test\n",
            "src=[asta e o propozitie ciudata], target=[this is a weird sentence], predicted=[this is a strange sentence]\n",
            "src=[lasama sa intru], target=[let me come in], predicted=[let me come in]\n",
            "src=[mam nascut in kyoto in], target=[i was born in kyoto in], predicted=[i was born in kyoto in]\n",
            "src=[sunt uimita de indrazneala ta], target=[i am amazed at your audacity], predicted=[i am amazed at your audacity]\n",
            "src=[cat de adanc este lacul asta], target=[how deep is this lake], predicted=[how deep is this lake]\n",
            "src=[totul este o greseala], target=[this is all a mistake], predicted=[this is all a mistake]\n",
            "src=[preturile variaza in functie de locatie], target=[prices vary by location], predicted=[prices vary by location]\n",
            "src=[voi conduce chiar eu], target=[ill drive myself], predicted=[ill drive myself]\n",
            "src=[inveti ceva nou in fiecare zi], target=[you learn something new every day], predicted=[you learn something new every day]\n",
            "src=[imi pare rau], target=[im sorry], predicted=[im sorry]\n",
            "BLEU-1: 0.913682\n",
            "BLEU-2: 0.895274\n",
            "BLEU-3: 0.891656\n",
            "BLEU-4: 0.858230\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkK4Ms3vVyxk"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lVCPgHMsIbe",
        "outputId": "b681b52e-3ce2-48e8-d01b-b83002038378"
      },
      "source": [
        "import string\n",
        "import re\n",
        "from pickle import dump\n",
        "from unicodedata import normalize\n",
        "from numpy import array\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, mode='rt', encoding='utf-8')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "# split a loaded document into sentences\n",
        "def to_pairs(doc):\n",
        "\tlines = doc.strip().split('\\n')\n",
        "\tpairs = [line.split('\\t') for line in  lines]\n",
        "\treturn pairs\n",
        "\n",
        "# clean a list of lines\n",
        "def clean_pairs(lines):\n",
        "\tcleaned = list()\n",
        "\t# prepare regex for char filtering\n",
        "\tre_print = re.compile('[^%s]' % re.escape(string.printable))\n",
        "\t# prepare translation table for removing punctuation\n",
        "\ttable = str.maketrans('', '', string.punctuation)\n",
        "\tfor pair in lines:\n",
        "\t\tclean_pair = list()\n",
        "\t\tfor line in pair:\n",
        "\t\t\t# normalize unicode characters\n",
        "\t\t\tline = normalize('NFD', line).encode('ascii', 'ignore')\n",
        "\t\t\tline = line.decode('UTF-8')\n",
        "\t\t\t# tokenize on white space\n",
        "\t\t\tline = line.split()\n",
        "\t\t\t# convert to lowercase\n",
        "\t\t\tline = [word.lower() for word in line]\n",
        "\t\t\t# remove punctuation from each token\n",
        "\t\t\tline = [word.translate(table) for word in line]\n",
        "\t\t\t# remove non-printable chars form each token\n",
        "\t\t\tline = [re_print.sub('', w) for w in line]\n",
        "\t\t\t# remove tokens with numbers in them\n",
        "\t\t\tline = [word for word in line if word.isalpha()]\n",
        "\t\t\t# store as string\n",
        "\t\t\tclean_pair.append(' '.join(line))\n",
        "\t\tcleaned.append(clean_pair)\n",
        "\treturn array(cleaned)\n",
        "\n",
        "# save a list of clean sentences to file\n",
        "def save_clean_data(sentences, filename):\n",
        "\tdump(sentences, open(filename, 'wb'))\n",
        "\tprint('Saved: %s' % filename)\n",
        "\n",
        "# load dataset\n",
        "filename = 'ron.txt'\n",
        "doc = load_doc(filename)\n",
        "# split into english-german pairs\n",
        "pairs = to_pairs(doc)\n",
        "# clean sentences\n",
        "clean_pairs = clean_pairs(pairs)\n",
        "# save clean pairs to file\n",
        "save_clean_data(clean_pairs, 'english-romanian.pkl')\n",
        "# spot check\n",
        "for i in range(100):\n",
        "\tprint('[%s] => [%s]' % (clean_pairs[i,0], clean_pairs[i,1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved: english-romanian.pkl\n",
            "[hi] => [buna]\n",
            "[run] => [fugi]\n",
            "[who] => [cine]\n",
            "[fire] => [foc]\n",
            "[help] => [ajutor]\n",
            "[jump] => [sari]\n",
            "[stop] => [oprestete]\n",
            "[wait] => [asteapta]\n",
            "[hello] => [salut]\n",
            "[hello] => [buna]\n",
            "[hurry] => [haide]\n",
            "[hurry] => [grabestete]\n",
            "[relax] => [calmeazate]\n",
            "[smile] => [zambeste]\n",
            "[attack] => [ataca]\n",
            "[attack] => [atacati]\n",
            "[cheers] => [noroc]\n",
            "[cheers] => [sanatate]\n",
            "[cheers] => [noroc bun]\n",
            "[freeze] => [stai pe loc]\n",
            "[get up] => [ridicate]\n",
            "[really] => [serios]\n",
            "[ask tom] => [intreabal pe tom]\n",
            "[awesome] => [maxim]\n",
            "[awesome] => [excelent]\n",
            "[call me] => [sunama]\n",
            "[get out] => [pleaca de aici]\n",
            "[get out] => [pleaca de aici]\n",
            "[go away] => [pleaca]\n",
            "[go away] => [lasama]\n",
            "[goodbye] => [la revedere]\n",
            "[hold on] => [rezista]\n",
            "[i agree] => [sunt de acord]\n",
            "[im tom] => [sunt tom]\n",
            "[im ill] => [sunt bolnav]\n",
            "[im sad] => [sunt trist]\n",
            "[its ok] => [e ok]\n",
            "[keep it] => [pastreazo]\n",
            "[keep it] => [pastreazal]\n",
            "[open up] => [deschide]\n",
            "[perfect] => [perfect]\n",
            "[tell me] => [spunemi]\n",
            "[why not] => [de ce nu]\n",
            "[grab him] => [apucate de el]\n",
            "[how cute] => [ce dragut]\n",
            "[how cute] => [ce simpatic]\n",
            "[hurry up] => [grabestete]\n",
            "[ill pay] => [eu voi plati]\n",
            "[im back] => [mam intors]\n",
            "[im back] => [am revenit]\n",
            "[im calm] => [sunt calm]\n",
            "[im free] => [eu sunt liber]\n",
            "[im here] => [sunt aici]\n",
            "[im here] => [eu sunt aici]\n",
            "[im home] => [sunt acasa]\n",
            "[im numb] => [sunt indiferent]\n",
            "[im numb] => [sunt indiferenta]\n",
            "[im sick] => [sunt bolnav]\n",
            "[it hurts] => [ma doare]\n",
            "[its tom] => [este tom]\n",
            "[keep out] => [nu intra]\n",
            "[marry me] => [casatorestete cu mine]\n",
            "[may i go] => [pot sa merg]\n",
            "[may i go] => [pot sa ma duc]\n",
            "[terrific] => [maxim]\n",
            "[terrific] => [teribil]\n",
            "[tom fled] => [tom a fugit]\n",
            "[tom left] => [tom a plecat]\n",
            "[too late] => [prea tarziu]\n",
            "[trust me] => [ai incredere in mine]\n",
            "[use this] => [utilizeaza asta]\n",
            "[who fell] => [cine a cazut]\n",
            "[who paid] => [cine a platit]\n",
            "[bless you] => [noroc]\n",
            "[bless you] => [sanatate]\n",
            "[bless you] => [noroc bun]\n",
            "[calm down] => [calmeazate]\n",
            "[come back] => [intoarcete]\n",
            "[come back] => [revino]\n",
            "[come back] => [reveniti]\n",
            "[come back] => [intoarcetiva]\n",
            "[come home] => [vino acasa]\n",
            "[dont ask] => [nu intreba]\n",
            "[fantastic] => [maxim]\n",
            "[he is ill] => [el este bolnav]\n",
            "[hes fast] => [el e rapid]\n",
            "[i am sick] => [sunt bolnav]\n",
            "[i beg you] => [va rog]\n",
            "[i can ski] => [pot sa schiez]\n",
            "[i can win] => [eu pot castiga]\n",
            "[i hope so] => [sper]\n",
            "[i will go] => [eu voi merge]\n",
            "[ill lose] => [voi pierde]\n",
            "[im bored] => [sunt plictisit]\n",
            "[im bored] => [mam plictisit]\n",
            "[im drunk] => [sunt beat]\n",
            "[im happy] => [sunt fericit]\n",
            "[im sorry] => [imi pare rau]\n",
            "[im tired] => [sunt obosit]\n",
            "[im tired] => [sunt obosita]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "guGmY4mvsU4k",
        "outputId": "6d9381cd-7f66-4c5c-bdb0-dc7bbbc86386"
      },
      "source": [
        "from pickle import load\n",
        "from pickle import dump\n",
        "from numpy.random import rand\n",
        "from numpy.random import shuffle\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# save a list of clean sentences to file\n",
        "def save_clean_data(sentences, filename):\n",
        "\tdump(sentences, open(filename, 'wb'))\n",
        "\tprint('Saved: %s' % filename)\n",
        "\n",
        "# load dataset\n",
        "raw_dataset = load_clean_sentences('english-romanian.pkl')\n",
        "\n",
        "# reduce dataset size\n",
        "n_sentences = 10000\n",
        "dataset = raw_dataset[:n_sentences, :]\n",
        "# random shuffle\n",
        "shuffle(dataset)\n",
        "# split into train/test\n",
        "train, test = dataset[:9000], dataset[1000:]\n",
        "# save\n",
        "save_clean_data(dataset, 'english-romanian-both.pkl')\n",
        "save_clean_data(train, 'english-romanian-train.pkl')\n",
        "save_clean_data(test, 'english-romanian-test.pkl')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved: english-romanian-both.pkl\n",
            "Saved: english-romanian-train.pkl\n",
            "Saved: english-romanian-test.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8m1ZdDI2Pf_",
        "outputId": "14deca78-b6d7-41b0-8e5e-5137a94fe3a5"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import RepeatVector\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# one hot encode target sequence\n",
        "def encode_output(sequences, vocab_size):\n",
        "\tylist = list()\n",
        "\tfor sequence in sequences:\n",
        "\t\tencoded = to_categorical(sequence, num_classes=vocab_size)\n",
        "\t\tylist.append(encoded)\n",
        "\ty = array(ylist)\n",
        "\ty = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
        "\treturn y\n",
        "\n",
        "# define NMT model\n",
        "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
        "\tmodel.add(LSTM(n_units))\n",
        "\tmodel.add(RepeatVector(tar_timesteps))\n",
        "\tmodel.add(LSTM(n_units, return_sequences=True))\n",
        "\tmodel.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
        "\treturn model\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-romanian-both.pkl')\n",
        "train = load_clean_sentences('english-romanian-train.pkl')\n",
        "test = load_clean_sentences('english-romanian-test.pkl')\n",
        "\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:,0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
        "print('English Max Length: %d' % (eng_length))\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "print('Romanian Vocabulary Size: %d' % ger_vocab_size)\n",
        "print('Romanian  Max Length: %d' % (ger_length))\n",
        "\n",
        "# prepare training data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
        "trainY = encode_output(trainY, eng_vocab_size)\n",
        "# prepare validation data\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
        "testY = encode_output(testY, eng_vocab_size)\n",
        "\n",
        "# define model\n",
        "model = define_model(ger_vocab_size, eng_vocab_size, ger_length, eng_length, 256)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "# summarize defined model\n",
        "print(model.summary())\n",
        "plot_model(model, to_file='model.png', show_shapes=True)\n",
        "# fit model\n",
        "filename = 'model.h5'\n",
        "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "model.fit(trainX, trainY, epochs=100, batch_size=64, validation_data=(testX, testY), callbacks=[checkpoint], verbose=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English Vocabulary Size: 4297\n",
            "English Max Length: 10\n",
            "Romanian Vocabulary Size: 6121\n",
            "Romanian  Max Length: 14\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 14, 256)           1566976   \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 256)               525312    \n",
            "_________________________________________________________________\n",
            "repeat_vector (RepeatVector) (None, 10, 256)           0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 10, 256)           525312    \n",
            "_________________________________________________________________\n",
            "time_distributed (TimeDistri (None, 10, 4297)          1104329   \n",
            "=================================================================\n",
            "Total params: 3,721,929\n",
            "Trainable params: 3,721,929\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "141/141 - 67s - loss: 4.1655 - val_loss: 3.4402\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 3.44020, saving model to model.h5\n",
            "Epoch 2/100\n",
            "141/141 - 59s - loss: 3.3705 - val_loss: 3.2456\n",
            "\n",
            "Epoch 00002: val_loss improved from 3.44020 to 3.24559, saving model to model.h5\n",
            "Epoch 3/100\n",
            "141/141 - 59s - loss: 3.2285 - val_loss: 3.1661\n",
            "\n",
            "Epoch 00003: val_loss improved from 3.24559 to 3.16607, saving model to model.h5\n",
            "Epoch 4/100\n",
            "141/141 - 59s - loss: 3.1356 - val_loss: 3.0918\n",
            "\n",
            "Epoch 00004: val_loss improved from 3.16607 to 3.09178, saving model to model.h5\n",
            "Epoch 5/100\n",
            "141/141 - 59s - loss: 3.0746 - val_loss: 3.0413\n",
            "\n",
            "Epoch 00005: val_loss improved from 3.09178 to 3.04134, saving model to model.h5\n",
            "Epoch 6/100\n",
            "141/141 - 60s - loss: 3.0322 - val_loss: 2.9869\n",
            "\n",
            "Epoch 00006: val_loss improved from 3.04134 to 2.98695, saving model to model.h5\n",
            "Epoch 7/100\n",
            "141/141 - 59s - loss: 2.9887 - val_loss: 2.9514\n",
            "\n",
            "Epoch 00007: val_loss improved from 2.98695 to 2.95145, saving model to model.h5\n",
            "Epoch 8/100\n",
            "141/141 - 59s - loss: 2.9371 - val_loss: 2.8863\n",
            "\n",
            "Epoch 00008: val_loss improved from 2.95145 to 2.88632, saving model to model.h5\n",
            "Epoch 9/100\n",
            "141/141 - 59s - loss: 2.8705 - val_loss: 2.8367\n",
            "\n",
            "Epoch 00009: val_loss improved from 2.88632 to 2.83669, saving model to model.h5\n",
            "Epoch 10/100\n",
            "141/141 - 59s - loss: 2.8076 - val_loss: 2.7690\n",
            "\n",
            "Epoch 00010: val_loss improved from 2.83669 to 2.76897, saving model to model.h5\n",
            "Epoch 11/100\n",
            "141/141 - 59s - loss: 2.7503 - val_loss: 2.7198\n",
            "\n",
            "Epoch 00011: val_loss improved from 2.76897 to 2.71979, saving model to model.h5\n",
            "Epoch 12/100\n",
            "141/141 - 60s - loss: 2.6859 - val_loss: 2.6540\n",
            "\n",
            "Epoch 00012: val_loss improved from 2.71979 to 2.65403, saving model to model.h5\n",
            "Epoch 13/100\n",
            "141/141 - 59s - loss: 2.6202 - val_loss: 2.5864\n",
            "\n",
            "Epoch 00013: val_loss improved from 2.65403 to 2.58645, saving model to model.h5\n",
            "Epoch 14/100\n",
            "141/141 - 59s - loss: 2.5552 - val_loss: 2.5288\n",
            "\n",
            "Epoch 00014: val_loss improved from 2.58645 to 2.52883, saving model to model.h5\n",
            "Epoch 15/100\n",
            "141/141 - 59s - loss: 2.4834 - val_loss: 2.4530\n",
            "\n",
            "Epoch 00015: val_loss improved from 2.52883 to 2.45295, saving model to model.h5\n",
            "Epoch 16/100\n",
            "141/141 - 60s - loss: 2.4099 - val_loss: 2.3756\n",
            "\n",
            "Epoch 00016: val_loss improved from 2.45295 to 2.37559, saving model to model.h5\n",
            "Epoch 17/100\n",
            "141/141 - 60s - loss: 2.3314 - val_loss: 2.3148\n",
            "\n",
            "Epoch 00017: val_loss improved from 2.37559 to 2.31476, saving model to model.h5\n",
            "Epoch 18/100\n",
            "141/141 - 59s - loss: 2.2506 - val_loss: 2.2364\n",
            "\n",
            "Epoch 00018: val_loss improved from 2.31476 to 2.23641, saving model to model.h5\n",
            "Epoch 19/100\n",
            "141/141 - 59s - loss: 2.1714 - val_loss: 2.1564\n",
            "\n",
            "Epoch 00019: val_loss improved from 2.23641 to 2.15638, saving model to model.h5\n",
            "Epoch 20/100\n",
            "141/141 - 60s - loss: 2.0938 - val_loss: 2.0774\n",
            "\n",
            "Epoch 00020: val_loss improved from 2.15638 to 2.07743, saving model to model.h5\n",
            "Epoch 21/100\n",
            "141/141 - 59s - loss: 2.0126 - val_loss: 2.0115\n",
            "\n",
            "Epoch 00021: val_loss improved from 2.07743 to 2.01151, saving model to model.h5\n",
            "Epoch 22/100\n",
            "141/141 - 60s - loss: 1.9326 - val_loss: 1.9248\n",
            "\n",
            "Epoch 00022: val_loss improved from 2.01151 to 1.92477, saving model to model.h5\n",
            "Epoch 23/100\n",
            "141/141 - 59s - loss: 1.8501 - val_loss: 1.8656\n",
            "\n",
            "Epoch 00023: val_loss improved from 1.92477 to 1.86564, saving model to model.h5\n",
            "Epoch 24/100\n",
            "141/141 - 59s - loss: 1.7733 - val_loss: 1.7885\n",
            "\n",
            "Epoch 00024: val_loss improved from 1.86564 to 1.78848, saving model to model.h5\n",
            "Epoch 25/100\n",
            "141/141 - 59s - loss: 1.7031 - val_loss: 1.7187\n",
            "\n",
            "Epoch 00025: val_loss improved from 1.78848 to 1.71871, saving model to model.h5\n",
            "Epoch 26/100\n",
            "141/141 - 59s - loss: 1.6278 - val_loss: 1.6459\n",
            "\n",
            "Epoch 00026: val_loss improved from 1.71871 to 1.64588, saving model to model.h5\n",
            "Epoch 27/100\n",
            "141/141 - 60s - loss: 1.5501 - val_loss: 1.5910\n",
            "\n",
            "Epoch 00027: val_loss improved from 1.64588 to 1.59097, saving model to model.h5\n",
            "Epoch 28/100\n",
            "141/141 - 60s - loss: 1.4806 - val_loss: 1.5234\n",
            "\n",
            "Epoch 00028: val_loss improved from 1.59097 to 1.52341, saving model to model.h5\n",
            "Epoch 29/100\n",
            "141/141 - 59s - loss: 1.4115 - val_loss: 1.4599\n",
            "\n",
            "Epoch 00029: val_loss improved from 1.52341 to 1.45991, saving model to model.h5\n",
            "Epoch 30/100\n",
            "141/141 - 59s - loss: 1.3405 - val_loss: 1.3981\n",
            "\n",
            "Epoch 00030: val_loss improved from 1.45991 to 1.39809, saving model to model.h5\n",
            "Epoch 31/100\n",
            "141/141 - 59s - loss: 1.2750 - val_loss: 1.3539\n",
            "\n",
            "Epoch 00031: val_loss improved from 1.39809 to 1.35389, saving model to model.h5\n",
            "Epoch 32/100\n",
            "141/141 - 59s - loss: 1.2122 - val_loss: 1.3029\n",
            "\n",
            "Epoch 00032: val_loss improved from 1.35389 to 1.30291, saving model to model.h5\n",
            "Epoch 33/100\n",
            "141/141 - 59s - loss: 1.1496 - val_loss: 1.2452\n",
            "\n",
            "Epoch 00033: val_loss improved from 1.30291 to 1.24519, saving model to model.h5\n",
            "Epoch 34/100\n",
            "141/141 - 59s - loss: 1.0858 - val_loss: 1.1885\n",
            "\n",
            "Epoch 00034: val_loss improved from 1.24519 to 1.18847, saving model to model.h5\n",
            "Epoch 35/100\n",
            "141/141 - 59s - loss: 1.0275 - val_loss: 1.1351\n",
            "\n",
            "Epoch 00035: val_loss improved from 1.18847 to 1.13512, saving model to model.h5\n",
            "Epoch 36/100\n",
            "141/141 - 59s - loss: 0.9713 - val_loss: 1.0993\n",
            "\n",
            "Epoch 00036: val_loss improved from 1.13512 to 1.09933, saving model to model.h5\n",
            "Epoch 37/100\n",
            "141/141 - 59s - loss: 0.9199 - val_loss: 1.0435\n",
            "\n",
            "Epoch 00037: val_loss improved from 1.09933 to 1.04347, saving model to model.h5\n",
            "Epoch 38/100\n",
            "141/141 - 60s - loss: 0.8702 - val_loss: 1.0050\n",
            "\n",
            "Epoch 00038: val_loss improved from 1.04347 to 1.00501, saving model to model.h5\n",
            "Epoch 39/100\n",
            "141/141 - 59s - loss: 0.8237 - val_loss: 0.9700\n",
            "\n",
            "Epoch 00039: val_loss improved from 1.00501 to 0.97002, saving model to model.h5\n",
            "Epoch 40/100\n",
            "141/141 - 59s - loss: 0.7804 - val_loss: 0.9344\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.97002 to 0.93435, saving model to model.h5\n",
            "Epoch 41/100\n",
            "141/141 - 59s - loss: 0.7338 - val_loss: 0.8927\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.93435 to 0.89266, saving model to model.h5\n",
            "Epoch 42/100\n",
            "141/141 - 59s - loss: 0.6925 - val_loss: 0.8598\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.89266 to 0.85979, saving model to model.h5\n",
            "Epoch 43/100\n",
            "141/141 - 59s - loss: 0.6524 - val_loss: 0.8304\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.85979 to 0.83043, saving model to model.h5\n",
            "Epoch 44/100\n",
            "141/141 - 60s - loss: 0.6205 - val_loss: 0.8144\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.83043 to 0.81441, saving model to model.h5\n",
            "Epoch 45/100\n",
            "141/141 - 59s - loss: 0.5835 - val_loss: 0.7766\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.81441 to 0.77662, saving model to model.h5\n",
            "Epoch 46/100\n",
            "141/141 - 60s - loss: 0.5492 - val_loss: 0.7471\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.77662 to 0.74714, saving model to model.h5\n",
            "Epoch 47/100\n",
            "141/141 - 59s - loss: 0.5171 - val_loss: 0.7246\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.74714 to 0.72457, saving model to model.h5\n",
            "Epoch 48/100\n",
            "141/141 - 60s - loss: 0.4918 - val_loss: 0.7019\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.72457 to 0.70187, saving model to model.h5\n",
            "Epoch 49/100\n",
            "141/141 - 60s - loss: 0.4625 - val_loss: 0.6777\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.70187 to 0.67772, saving model to model.h5\n",
            "Epoch 50/100\n",
            "141/141 - 59s - loss: 0.4352 - val_loss: 0.6643\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.67772 to 0.66427, saving model to model.h5\n",
            "Epoch 51/100\n",
            "141/141 - 59s - loss: 0.4135 - val_loss: 0.6382\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.66427 to 0.63815, saving model to model.h5\n",
            "Epoch 52/100\n",
            "141/141 - 59s - loss: 0.3906 - val_loss: 0.6210\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.63815 to 0.62103, saving model to model.h5\n",
            "Epoch 53/100\n",
            "141/141 - 60s - loss: 0.3672 - val_loss: 0.6132\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.62103 to 0.61319, saving model to model.h5\n",
            "Epoch 54/100\n",
            "141/141 - 60s - loss: 0.3478 - val_loss: 0.5919\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.61319 to 0.59186, saving model to model.h5\n",
            "Epoch 55/100\n",
            "141/141 - 59s - loss: 0.3209 - val_loss: 0.5747\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.59186 to 0.57470, saving model to model.h5\n",
            "Epoch 56/100\n",
            "141/141 - 59s - loss: 0.3000 - val_loss: 0.5544\n",
            "\n",
            "Epoch 00056: val_loss improved from 0.57470 to 0.55437, saving model to model.h5\n",
            "Epoch 57/100\n",
            "141/141 - 59s - loss: 0.2854 - val_loss: 0.5560\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.55437\n",
            "Epoch 58/100\n",
            "141/141 - 60s - loss: 0.2718 - val_loss: 0.5352\n",
            "\n",
            "Epoch 00058: val_loss improved from 0.55437 to 0.53515, saving model to model.h5\n",
            "Epoch 59/100\n",
            "141/141 - 59s - loss: 0.2538 - val_loss: 0.5269\n",
            "\n",
            "Epoch 00059: val_loss improved from 0.53515 to 0.52691, saving model to model.h5\n",
            "Epoch 60/100\n",
            "141/141 - 60s - loss: 0.2454 - val_loss: 0.5176\n",
            "\n",
            "Epoch 00060: val_loss improved from 0.52691 to 0.51761, saving model to model.h5\n",
            "Epoch 61/100\n",
            "141/141 - 59s - loss: 0.2317 - val_loss: 0.5053\n",
            "\n",
            "Epoch 00061: val_loss improved from 0.51761 to 0.50528, saving model to model.h5\n",
            "Epoch 62/100\n",
            "141/141 - 59s - loss: 0.2159 - val_loss: 0.4927\n",
            "\n",
            "Epoch 00062: val_loss improved from 0.50528 to 0.49275, saving model to model.h5\n",
            "Epoch 63/100\n",
            "141/141 - 59s - loss: 0.1995 - val_loss: 0.4862\n",
            "\n",
            "Epoch 00063: val_loss improved from 0.49275 to 0.48619, saving model to model.h5\n",
            "Epoch 64/100\n",
            "141/141 - 60s - loss: 0.1881 - val_loss: 0.4790\n",
            "\n",
            "Epoch 00064: val_loss improved from 0.48619 to 0.47896, saving model to model.h5\n",
            "Epoch 65/100\n",
            "141/141 - 60s - loss: 0.1761 - val_loss: 0.4676\n",
            "\n",
            "Epoch 00065: val_loss improved from 0.47896 to 0.46763, saving model to model.h5\n",
            "Epoch 66/100\n",
            "141/141 - 59s - loss: 0.1643 - val_loss: 0.4631\n",
            "\n",
            "Epoch 00066: val_loss improved from 0.46763 to 0.46314, saving model to model.h5\n",
            "Epoch 67/100\n",
            "141/141 - 60s - loss: 0.1604 - val_loss: 0.4588\n",
            "\n",
            "Epoch 00067: val_loss improved from 0.46314 to 0.45880, saving model to model.h5\n",
            "Epoch 68/100\n",
            "141/141 - 60s - loss: 0.1531 - val_loss: 0.4568\n",
            "\n",
            "Epoch 00068: val_loss improved from 0.45880 to 0.45679, saving model to model.h5\n",
            "Epoch 69/100\n",
            "141/141 - 60s - loss: 0.1442 - val_loss: 0.4500\n",
            "\n",
            "Epoch 00069: val_loss improved from 0.45679 to 0.45004, saving model to model.h5\n",
            "Epoch 70/100\n",
            "141/141 - 60s - loss: 0.1400 - val_loss: 0.4519\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.45004\n",
            "Epoch 71/100\n",
            "141/141 - 60s - loss: 0.1343 - val_loss: 0.4435\n",
            "\n",
            "Epoch 00071: val_loss improved from 0.45004 to 0.44348, saving model to model.h5\n",
            "Epoch 72/100\n",
            "141/141 - 60s - loss: 0.1265 - val_loss: 0.4399\n",
            "\n",
            "Epoch 00072: val_loss improved from 0.44348 to 0.43991, saving model to model.h5\n",
            "Epoch 73/100\n",
            "141/141 - 59s - loss: 0.1201 - val_loss: 0.4325\n",
            "\n",
            "Epoch 00073: val_loss improved from 0.43991 to 0.43253, saving model to model.h5\n",
            "Epoch 74/100\n",
            "141/141 - 60s - loss: 0.1147 - val_loss: 0.4382\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.43253\n",
            "Epoch 75/100\n",
            "141/141 - 59s - loss: 0.1096 - val_loss: 0.4338\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.43253\n",
            "Epoch 76/100\n",
            "141/141 - 60s - loss: 0.1058 - val_loss: 0.4249\n",
            "\n",
            "Epoch 00076: val_loss improved from 0.43253 to 0.42491, saving model to model.h5\n",
            "Epoch 77/100\n",
            "141/141 - 59s - loss: 0.0958 - val_loss: 0.4205\n",
            "\n",
            "Epoch 00077: val_loss improved from 0.42491 to 0.42050, saving model to model.h5\n",
            "Epoch 78/100\n",
            "141/141 - 60s - loss: 0.0918 - val_loss: 0.4177\n",
            "\n",
            "Epoch 00078: val_loss improved from 0.42050 to 0.41769, saving model to model.h5\n",
            "Epoch 79/100\n",
            "141/141 - 60s - loss: 0.0855 - val_loss: 0.4156\n",
            "\n",
            "Epoch 00079: val_loss improved from 0.41769 to 0.41560, saving model to model.h5\n",
            "Epoch 80/100\n",
            "141/141 - 60s - loss: 0.0828 - val_loss: 0.4161\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.41560\n",
            "Epoch 81/100\n",
            "141/141 - 60s - loss: 0.0817 - val_loss: 0.4199\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.41560\n",
            "Epoch 82/100\n",
            "141/141 - 60s - loss: 0.0826 - val_loss: 0.4140\n",
            "\n",
            "Epoch 00082: val_loss improved from 0.41560 to 0.41404, saving model to model.h5\n",
            "Epoch 83/100\n",
            "141/141 - 59s - loss: 0.0815 - val_loss: 0.4234\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.41404\n",
            "Epoch 84/100\n",
            "141/141 - 60s - loss: 0.0854 - val_loss: 0.4292\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.41404\n",
            "Epoch 85/100\n",
            "141/141 - 59s - loss: 0.0942 - val_loss: 0.4341\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.41404\n",
            "Epoch 86/100\n",
            "141/141 - 59s - loss: 0.1064 - val_loss: 0.4362\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.41404\n",
            "Epoch 87/100\n",
            "141/141 - 60s - loss: 0.1080 - val_loss: 0.4427\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.41404\n",
            "Epoch 88/100\n",
            "141/141 - 60s - loss: 0.0953 - val_loss: 0.4217\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.41404\n",
            "Epoch 89/100\n",
            "141/141 - 60s - loss: 0.0802 - val_loss: 0.4160\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.41404\n",
            "Epoch 90/100\n",
            "141/141 - 59s - loss: 0.0680 - val_loss: 0.4083\n",
            "\n",
            "Epoch 00090: val_loss improved from 0.41404 to 0.40832, saving model to model.h5\n",
            "Epoch 91/100\n",
            "141/141 - 59s - loss: 0.0597 - val_loss: 0.4044\n",
            "\n",
            "Epoch 00091: val_loss improved from 0.40832 to 0.40444, saving model to model.h5\n",
            "Epoch 92/100\n",
            "141/141 - 60s - loss: 0.0553 - val_loss: 0.4025\n",
            "\n",
            "Epoch 00092: val_loss improved from 0.40444 to 0.40248, saving model to model.h5\n",
            "Epoch 93/100\n",
            "141/141 - 59s - loss: 0.0527 - val_loss: 0.4020\n",
            "\n",
            "Epoch 00093: val_loss improved from 0.40248 to 0.40196, saving model to model.h5\n",
            "Epoch 94/100\n",
            "141/141 - 60s - loss: 0.0518 - val_loss: 0.4019\n",
            "\n",
            "Epoch 00094: val_loss improved from 0.40196 to 0.40185, saving model to model.h5\n",
            "Epoch 95/100\n",
            "141/141 - 59s - loss: 0.0508 - val_loss: 0.4017\n",
            "\n",
            "Epoch 00095: val_loss improved from 0.40185 to 0.40169, saving model to model.h5\n",
            "Epoch 96/100\n",
            "141/141 - 60s - loss: 0.0520 - val_loss: 0.4067\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.40169\n",
            "Epoch 97/100\n",
            "141/141 - 60s - loss: 0.0543 - val_loss: 0.4097\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.40169\n",
            "Epoch 98/100\n",
            "141/141 - 59s - loss: 0.0584 - val_loss: 0.4165\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.40169\n",
            "Epoch 99/100\n",
            "141/141 - 59s - loss: 0.0660 - val_loss: 0.4262\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.40169\n",
            "Epoch 100/100\n",
            "141/141 - 60s - loss: 0.0830 - val_loss: 0.4394\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.40169\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f701ae5d4e0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSgMd_erI7st",
        "outputId": "7eab265c-9aaf-4c05-a12e-07e8b8454546"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import RepeatVector\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# one hot encode target sequence\n",
        "def encode_output(sequences, vocab_size):\n",
        "\tylist = list()\n",
        "\tfor sequence in sequences:\n",
        "\t\tencoded = to_categorical(sequence, num_classes=vocab_size)\n",
        "\t\tylist.append(encoded)\n",
        "\ty = array(ylist)\n",
        "\ty = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
        "\treturn y\n",
        "\n",
        "# define NMT model\n",
        "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
        "\tmodel.add(LSTM(n_units))\n",
        "\tmodel.add(RepeatVector(tar_timesteps))\n",
        "\tmodel.add(LSTM(n_units, return_sequences=True))\n",
        "\tmodel.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
        "\treturn model\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-romanian-both.pkl')\n",
        "train = load_clean_sentences('english-romanian-train.pkl')\n",
        "test = load_clean_sentences('english-romanian-test.pkl')\n",
        "\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:,0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
        "print('English Max Length: %d' % (eng_length))\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "print('Romanian Vocabulary Size: %d' % ger_vocab_size)\n",
        "print('Romanian  Max Length: %d' % (ger_length))\n",
        "\n",
        "# prepare training data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
        "trainY = encode_output(trainY, eng_vocab_size)\n",
        "# prepare validation data\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
        "testY = encode_output(testY, eng_vocab_size)\n",
        "\n",
        "# define model\n",
        "model = define_model(ger_vocab_size, eng_vocab_size, ger_length, eng_length, 256)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "# summarize defined model\n",
        "print(model.summary())\n",
        "plot_model(model, to_file='model.png', show_shapes=True)\n",
        "# fit model\n",
        "filename = 'model.h5'\n",
        "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "model.fit(trainX, trainY, epochs=100, batch_size=64, validation_data=(testX, testY), callbacks=[checkpoint], verbose=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English Vocabulary Size: 4297\n",
            "English Max Length: 10\n",
            "Romanian Vocabulary Size: 6121\n",
            "Romanian  Max Length: 14\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 14, 256)           1566976   \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 256)               525312    \n",
            "_________________________________________________________________\n",
            "repeat_vector_1 (RepeatVecto (None, 10, 256)           0         \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 10, 256)           525312    \n",
            "_________________________________________________________________\n",
            "time_distributed_1 (TimeDist (None, 10, 4297)          1104329   \n",
            "=================================================================\n",
            "Total params: 3,721,929\n",
            "Trainable params: 3,721,929\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "141/141 - 51s - loss: 4.1972 - val_loss: 3.4536\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 3.45356, saving model to model.h5\n",
            "Epoch 2/100\n",
            "141/141 - 45s - loss: 3.3764 - val_loss: 3.2439\n",
            "\n",
            "Epoch 00002: val_loss improved from 3.45356 to 3.24386, saving model to model.h5\n",
            "Epoch 3/100\n",
            "141/141 - 45s - loss: 3.2373 - val_loss: 3.1659\n",
            "\n",
            "Epoch 00003: val_loss improved from 3.24386 to 3.16588, saving model to model.h5\n",
            "Epoch 4/100\n",
            "141/141 - 45s - loss: 3.1489 - val_loss: 3.0771\n",
            "\n",
            "Epoch 00004: val_loss improved from 3.16588 to 3.07708, saving model to model.h5\n",
            "Epoch 5/100\n",
            "141/141 - 45s - loss: 3.0866 - val_loss: 3.0295\n",
            "\n",
            "Epoch 00005: val_loss improved from 3.07708 to 3.02951, saving model to model.h5\n",
            "Epoch 6/100\n",
            "141/141 - 45s - loss: 3.0382 - val_loss: 3.0063\n",
            "\n",
            "Epoch 00006: val_loss improved from 3.02951 to 3.00633, saving model to model.h5\n",
            "Epoch 7/100\n",
            "141/141 - 45s - loss: 3.0058 - val_loss: 2.9675\n",
            "\n",
            "Epoch 00007: val_loss improved from 3.00633 to 2.96746, saving model to model.h5\n",
            "Epoch 8/100\n",
            "141/141 - 45s - loss: 2.9695 - val_loss: 2.9286\n",
            "\n",
            "Epoch 00008: val_loss improved from 2.96746 to 2.92860, saving model to model.h5\n",
            "Epoch 9/100\n",
            "141/141 - 45s - loss: 2.9327 - val_loss: 2.9004\n",
            "\n",
            "Epoch 00009: val_loss improved from 2.92860 to 2.90045, saving model to model.h5\n",
            "Epoch 10/100\n",
            "141/141 - 45s - loss: 2.8884 - val_loss: 2.8410\n",
            "\n",
            "Epoch 00010: val_loss improved from 2.90045 to 2.84103, saving model to model.h5\n",
            "Epoch 11/100\n",
            "141/141 - 45s - loss: 2.8187 - val_loss: 2.7675\n",
            "\n",
            "Epoch 00011: val_loss improved from 2.84103 to 2.76748, saving model to model.h5\n",
            "Epoch 12/100\n",
            "141/141 - 45s - loss: 2.7495 - val_loss: 2.7033\n",
            "\n",
            "Epoch 00012: val_loss improved from 2.76748 to 2.70331, saving model to model.h5\n",
            "Epoch 13/100\n",
            "141/141 - 45s - loss: 2.6815 - val_loss: 2.6424\n",
            "\n",
            "Epoch 00013: val_loss improved from 2.70331 to 2.64235, saving model to model.h5\n",
            "Epoch 14/100\n",
            "141/141 - 45s - loss: 2.6199 - val_loss: 2.5829\n",
            "\n",
            "Epoch 00014: val_loss improved from 2.64235 to 2.58294, saving model to model.h5\n",
            "Epoch 15/100\n",
            "141/141 - 45s - loss: 2.5547 - val_loss: 2.5190\n",
            "\n",
            "Epoch 00015: val_loss improved from 2.58294 to 2.51899, saving model to model.h5\n",
            "Epoch 16/100\n",
            "141/141 - 45s - loss: 2.4851 - val_loss: 2.4557\n",
            "\n",
            "Epoch 00016: val_loss improved from 2.51899 to 2.45571, saving model to model.h5\n",
            "Epoch 17/100\n",
            "141/141 - 45s - loss: 2.4188 - val_loss: 2.3969\n",
            "\n",
            "Epoch 00017: val_loss improved from 2.45571 to 2.39689, saving model to model.h5\n",
            "Epoch 18/100\n",
            "141/141 - 45s - loss: 2.3496 - val_loss: 2.3252\n",
            "\n",
            "Epoch 00018: val_loss improved from 2.39689 to 2.32520, saving model to model.h5\n",
            "Epoch 19/100\n",
            "141/141 - 45s - loss: 2.2761 - val_loss: 2.2594\n",
            "\n",
            "Epoch 00019: val_loss improved from 2.32520 to 2.25937, saving model to model.h5\n",
            "Epoch 20/100\n",
            "141/141 - 45s - loss: 2.2088 - val_loss: 2.1921\n",
            "\n",
            "Epoch 00020: val_loss improved from 2.25937 to 2.19205, saving model to model.h5\n",
            "Epoch 21/100\n",
            "141/141 - 45s - loss: 2.1390 - val_loss: 2.1354\n",
            "\n",
            "Epoch 00021: val_loss improved from 2.19205 to 2.13538, saving model to model.h5\n",
            "Epoch 22/100\n",
            "141/141 - 45s - loss: 2.0683 - val_loss: 2.0622\n",
            "\n",
            "Epoch 00022: val_loss improved from 2.13538 to 2.06221, saving model to model.h5\n",
            "Epoch 23/100\n",
            "141/141 - 45s - loss: 1.9931 - val_loss: 1.9896\n",
            "\n",
            "Epoch 00023: val_loss improved from 2.06221 to 1.98957, saving model to model.h5\n",
            "Epoch 24/100\n",
            "141/141 - 45s - loss: 1.9232 - val_loss: 1.9574\n",
            "\n",
            "Epoch 00024: val_loss improved from 1.98957 to 1.95735, saving model to model.h5\n",
            "Epoch 25/100\n",
            "141/141 - 45s - loss: 1.8536 - val_loss: 1.8653\n",
            "\n",
            "Epoch 00025: val_loss improved from 1.95735 to 1.86528, saving model to model.h5\n",
            "Epoch 26/100\n",
            "141/141 - 45s - loss: 1.7815 - val_loss: 1.8003\n",
            "\n",
            "Epoch 00026: val_loss improved from 1.86528 to 1.80033, saving model to model.h5\n",
            "Epoch 27/100\n",
            "141/141 - 45s - loss: 1.7117 - val_loss: 1.7325\n",
            "\n",
            "Epoch 00027: val_loss improved from 1.80033 to 1.73253, saving model to model.h5\n",
            "Epoch 28/100\n",
            "141/141 - 45s - loss: 1.6450 - val_loss: 1.6793\n",
            "\n",
            "Epoch 00028: val_loss improved from 1.73253 to 1.67928, saving model to model.h5\n",
            "Epoch 29/100\n",
            "141/141 - 45s - loss: 1.5783 - val_loss: 1.6174\n",
            "\n",
            "Epoch 00029: val_loss improved from 1.67928 to 1.61739, saving model to model.h5\n",
            "Epoch 30/100\n",
            "141/141 - 45s - loss: 1.5132 - val_loss: 1.5649\n",
            "\n",
            "Epoch 00030: val_loss improved from 1.61739 to 1.56489, saving model to model.h5\n",
            "Epoch 31/100\n",
            "141/141 - 45s - loss: 1.4506 - val_loss: 1.5028\n",
            "\n",
            "Epoch 00031: val_loss improved from 1.56489 to 1.50285, saving model to model.h5\n",
            "Epoch 32/100\n",
            "141/141 - 45s - loss: 1.3893 - val_loss: 1.4480\n",
            "\n",
            "Epoch 00032: val_loss improved from 1.50285 to 1.44802, saving model to model.h5\n",
            "Epoch 33/100\n",
            "141/141 - 45s - loss: 1.3277 - val_loss: 1.3923\n",
            "\n",
            "Epoch 00033: val_loss improved from 1.44802 to 1.39233, saving model to model.h5\n",
            "Epoch 34/100\n",
            "141/141 - 45s - loss: 1.2611 - val_loss: 1.3346\n",
            "\n",
            "Epoch 00034: val_loss improved from 1.39233 to 1.33462, saving model to model.h5\n",
            "Epoch 35/100\n",
            "141/141 - 45s - loss: 1.2033 - val_loss: 1.3121\n",
            "\n",
            "Epoch 00035: val_loss improved from 1.33462 to 1.31210, saving model to model.h5\n",
            "Epoch 36/100\n",
            "141/141 - 45s - loss: 1.1514 - val_loss: 1.2409\n",
            "\n",
            "Epoch 00036: val_loss improved from 1.31210 to 1.24087, saving model to model.h5\n",
            "Epoch 37/100\n",
            "141/141 - 45s - loss: 1.0973 - val_loss: 1.2026\n",
            "\n",
            "Epoch 00037: val_loss improved from 1.24087 to 1.20258, saving model to model.h5\n",
            "Epoch 38/100\n",
            "141/141 - 45s - loss: 1.0402 - val_loss: 1.1581\n",
            "\n",
            "Epoch 00038: val_loss improved from 1.20258 to 1.15814, saving model to model.h5\n",
            "Epoch 39/100\n",
            "141/141 - 45s - loss: 0.9872 - val_loss: 1.1034\n",
            "\n",
            "Epoch 00039: val_loss improved from 1.15814 to 1.10338, saving model to model.h5\n",
            "Epoch 40/100\n",
            "141/141 - 45s - loss: 0.9347 - val_loss: 1.0653\n",
            "\n",
            "Epoch 00040: val_loss improved from 1.10338 to 1.06530, saving model to model.h5\n",
            "Epoch 41/100\n",
            "141/141 - 45s - loss: 0.8878 - val_loss: 1.0291\n",
            "\n",
            "Epoch 00041: val_loss improved from 1.06530 to 1.02912, saving model to model.h5\n",
            "Epoch 42/100\n",
            "141/141 - 45s - loss: 0.8453 - val_loss: 0.9878\n",
            "\n",
            "Epoch 00042: val_loss improved from 1.02912 to 0.98781, saving model to model.h5\n",
            "Epoch 43/100\n",
            "141/141 - 46s - loss: 0.8026 - val_loss: 0.9545\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.98781 to 0.95451, saving model to model.h5\n",
            "Epoch 44/100\n",
            "141/141 - 47s - loss: 0.7606 - val_loss: 0.9163\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.95451 to 0.91626, saving model to model.h5\n",
            "Epoch 45/100\n",
            "141/141 - 46s - loss: 0.7182 - val_loss: 0.8987\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.91626 to 0.89873, saving model to model.h5\n",
            "Epoch 46/100\n",
            "141/141 - 45s - loss: 0.6820 - val_loss: 0.8600\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.89873 to 0.85997, saving model to model.h5\n",
            "Epoch 47/100\n",
            "141/141 - 45s - loss: 0.6448 - val_loss: 0.8341\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.85997 to 0.83413, saving model to model.h5\n",
            "Epoch 48/100\n",
            "141/141 - 45s - loss: 0.6115 - val_loss: 0.7982\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.83413 to 0.79825, saving model to model.h5\n",
            "Epoch 49/100\n",
            "141/141 - 45s - loss: 0.5786 - val_loss: 0.7768\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.79825 to 0.77679, saving model to model.h5\n",
            "Epoch 50/100\n",
            "141/141 - 45s - loss: 0.5454 - val_loss: 0.7468\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.77679 to 0.74681, saving model to model.h5\n",
            "Epoch 51/100\n",
            "141/141 - 45s - loss: 0.5138 - val_loss: 0.7240\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.74681 to 0.72396, saving model to model.h5\n",
            "Epoch 52/100\n",
            "141/141 - 45s - loss: 0.4858 - val_loss: 0.7014\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.72396 to 0.70143, saving model to model.h5\n",
            "Epoch 53/100\n",
            "141/141 - 45s - loss: 0.4619 - val_loss: 0.6830\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.70143 to 0.68299, saving model to model.h5\n",
            "Epoch 54/100\n",
            "141/141 - 45s - loss: 0.4390 - val_loss: 0.6643\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.68299 to 0.66430, saving model to model.h5\n",
            "Epoch 55/100\n",
            "141/141 - 45s - loss: 0.4136 - val_loss: 0.6441\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.66430 to 0.64412, saving model to model.h5\n",
            "Epoch 56/100\n",
            "141/141 - 45s - loss: 0.3912 - val_loss: 0.6254\n",
            "\n",
            "Epoch 00056: val_loss improved from 0.64412 to 0.62541, saving model to model.h5\n",
            "Epoch 57/100\n",
            "141/141 - 45s - loss: 0.3653 - val_loss: 0.6086\n",
            "\n",
            "Epoch 00057: val_loss improved from 0.62541 to 0.60864, saving model to model.h5\n",
            "Epoch 58/100\n",
            "141/141 - 45s - loss: 0.3449 - val_loss: 0.5940\n",
            "\n",
            "Epoch 00058: val_loss improved from 0.60864 to 0.59396, saving model to model.h5\n",
            "Epoch 59/100\n",
            "141/141 - 45s - loss: 0.3257 - val_loss: 0.5796\n",
            "\n",
            "Epoch 00059: val_loss improved from 0.59396 to 0.57957, saving model to model.h5\n",
            "Epoch 60/100\n",
            "141/141 - 45s - loss: 0.3108 - val_loss: 0.5669\n",
            "\n",
            "Epoch 00060: val_loss improved from 0.57957 to 0.56685, saving model to model.h5\n",
            "Epoch 61/100\n",
            "141/141 - 45s - loss: 0.2942 - val_loss: 0.5603\n",
            "\n",
            "Epoch 00061: val_loss improved from 0.56685 to 0.56028, saving model to model.h5\n",
            "Epoch 62/100\n",
            "141/141 - 45s - loss: 0.2819 - val_loss: 0.5463\n",
            "\n",
            "Epoch 00062: val_loss improved from 0.56028 to 0.54629, saving model to model.h5\n",
            "Epoch 63/100\n",
            "141/141 - 45s - loss: 0.2642 - val_loss: 0.5341\n",
            "\n",
            "Epoch 00063: val_loss improved from 0.54629 to 0.53411, saving model to model.h5\n",
            "Epoch 64/100\n",
            "141/141 - 45s - loss: 0.2482 - val_loss: 0.5224\n",
            "\n",
            "Epoch 00064: val_loss improved from 0.53411 to 0.52240, saving model to model.h5\n",
            "Epoch 65/100\n",
            "141/141 - 45s - loss: 0.2331 - val_loss: 0.5096\n",
            "\n",
            "Epoch 00065: val_loss improved from 0.52240 to 0.50957, saving model to model.h5\n",
            "Epoch 66/100\n",
            "141/141 - 45s - loss: 0.2180 - val_loss: 0.5012\n",
            "\n",
            "Epoch 00066: val_loss improved from 0.50957 to 0.50118, saving model to model.h5\n",
            "Epoch 67/100\n",
            "141/141 - 45s - loss: 0.2081 - val_loss: 0.4975\n",
            "\n",
            "Epoch 00067: val_loss improved from 0.50118 to 0.49749, saving model to model.h5\n",
            "Epoch 68/100\n",
            "141/141 - 45s - loss: 0.1960 - val_loss: 0.4907\n",
            "\n",
            "Epoch 00068: val_loss improved from 0.49749 to 0.49066, saving model to model.h5\n",
            "Epoch 69/100\n",
            "141/141 - 45s - loss: 0.1895 - val_loss: 0.4889\n",
            "\n",
            "Epoch 00069: val_loss improved from 0.49066 to 0.48891, saving model to model.h5\n",
            "Epoch 70/100\n",
            "141/141 - 45s - loss: 0.1812 - val_loss: 0.4755\n",
            "\n",
            "Epoch 00070: val_loss improved from 0.48891 to 0.47555, saving model to model.h5\n",
            "Epoch 71/100\n",
            "141/141 - 45s - loss: 0.1699 - val_loss: 0.4682\n",
            "\n",
            "Epoch 00071: val_loss improved from 0.47555 to 0.46821, saving model to model.h5\n",
            "Epoch 72/100\n",
            "141/141 - 45s - loss: 0.1587 - val_loss: 0.4635\n",
            "\n",
            "Epoch 00072: val_loss improved from 0.46821 to 0.46349, saving model to model.h5\n",
            "Epoch 73/100\n",
            "141/141 - 45s - loss: 0.1503 - val_loss: 0.4575\n",
            "\n",
            "Epoch 00073: val_loss improved from 0.46349 to 0.45749, saving model to model.h5\n",
            "Epoch 74/100\n",
            "141/141 - 45s - loss: 0.1456 - val_loss: 0.4635\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.45749\n",
            "Epoch 75/100\n",
            "141/141 - 45s - loss: 0.1473 - val_loss: 0.4514\n",
            "\n",
            "Epoch 00075: val_loss improved from 0.45749 to 0.45144, saving model to model.h5\n",
            "Epoch 76/100\n",
            "141/141 - 45s - loss: 0.1339 - val_loss: 0.4462\n",
            "\n",
            "Epoch 00076: val_loss improved from 0.45144 to 0.44624, saving model to model.h5\n",
            "Epoch 77/100\n",
            "141/141 - 45s - loss: 0.1288 - val_loss: 0.4459\n",
            "\n",
            "Epoch 00077: val_loss improved from 0.44624 to 0.44590, saving model to model.h5\n",
            "Epoch 78/100\n",
            "141/141 - 45s - loss: 0.1210 - val_loss: 0.4392\n",
            "\n",
            "Epoch 00078: val_loss improved from 0.44590 to 0.43924, saving model to model.h5\n",
            "Epoch 79/100\n",
            "141/141 - 45s - loss: 0.1163 - val_loss: 0.4313\n",
            "\n",
            "Epoch 00079: val_loss improved from 0.43924 to 0.43133, saving model to model.h5\n",
            "Epoch 80/100\n",
            "141/141 - 45s - loss: 0.1106 - val_loss: 0.4305\n",
            "\n",
            "Epoch 00080: val_loss improved from 0.43133 to 0.43051, saving model to model.h5\n",
            "Epoch 81/100\n",
            "141/141 - 45s - loss: 0.1042 - val_loss: 0.4337\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.43051\n",
            "Epoch 82/100\n",
            "141/141 - 45s - loss: 0.1006 - val_loss: 0.4264\n",
            "\n",
            "Epoch 00082: val_loss improved from 0.43051 to 0.42644, saving model to model.h5\n",
            "Epoch 83/100\n",
            "141/141 - 45s - loss: 0.0953 - val_loss: 0.4233\n",
            "\n",
            "Epoch 00083: val_loss improved from 0.42644 to 0.42329, saving model to model.h5\n",
            "Epoch 84/100\n",
            "141/141 - 45s - loss: 0.0897 - val_loss: 0.4211\n",
            "\n",
            "Epoch 00084: val_loss improved from 0.42329 to 0.42111, saving model to model.h5\n",
            "Epoch 85/100\n",
            "141/141 - 45s - loss: 0.0845 - val_loss: 0.4187\n",
            "\n",
            "Epoch 00085: val_loss improved from 0.42111 to 0.41869, saving model to model.h5\n",
            "Epoch 86/100\n",
            "141/141 - 45s - loss: 0.0811 - val_loss: 0.4177\n",
            "\n",
            "Epoch 00086: val_loss improved from 0.41869 to 0.41769, saving model to model.h5\n",
            "Epoch 87/100\n",
            "141/141 - 45s - loss: 0.0795 - val_loss: 0.4196\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.41769\n",
            "Epoch 88/100\n",
            "141/141 - 45s - loss: 0.0804 - val_loss: 0.4192\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.41769\n",
            "Epoch 89/100\n",
            "141/141 - 45s - loss: 0.0828 - val_loss: 0.4250\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.41769\n",
            "Epoch 90/100\n",
            "141/141 - 45s - loss: 0.0867 - val_loss: 0.4278\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.41769\n",
            "Epoch 91/100\n",
            "141/141 - 45s - loss: 0.0911 - val_loss: 0.4254\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.41769\n",
            "Epoch 92/100\n",
            "141/141 - 45s - loss: 0.0908 - val_loss: 0.4309\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.41769\n",
            "Epoch 93/100\n",
            "141/141 - 45s - loss: 0.0896 - val_loss: 0.4259\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.41769\n",
            "Epoch 94/100\n",
            "141/141 - 46s - loss: 0.0829 - val_loss: 0.4286\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.41769\n",
            "Epoch 95/100\n",
            "141/141 - 43s - loss: 0.0753 - val_loss: 0.4162\n",
            "\n",
            "Epoch 00095: val_loss improved from 0.41769 to 0.41623, saving model to model.h5\n",
            "Epoch 96/100\n",
            "141/141 - 43s - loss: 0.0683 - val_loss: 0.4116\n",
            "\n",
            "Epoch 00096: val_loss improved from 0.41623 to 0.41163, saving model to model.h5\n",
            "Epoch 97/100\n",
            "141/141 - 43s - loss: 0.0629 - val_loss: 0.4095\n",
            "\n",
            "Epoch 00097: val_loss improved from 0.41163 to 0.40951, saving model to model.h5\n",
            "Epoch 98/100\n",
            "141/141 - 44s - loss: 0.0574 - val_loss: 0.4054\n",
            "\n",
            "Epoch 00098: val_loss improved from 0.40951 to 0.40537, saving model to model.h5\n",
            "Epoch 99/100\n",
            "141/141 - 44s - loss: 0.0532 - val_loss: 0.4059\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.40537\n",
            "Epoch 100/100\n",
            "141/141 - 42s - loss: 0.0503 - val_loss: 0.4044\n",
            "\n",
            "Epoch 00100: val_loss improved from 0.40537 to 0.40438, saving model to model.h5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f8232426630>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h17z8uAgOJrS",
        "outputId": "ca6edfa0-a41f-4b0d-aa66-29ad312aa3f5"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# map an integer to a word\n",
        "def word_for_id(integer, tokenizer):\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == integer:\n",
        "\t\t\treturn word\n",
        "\treturn None\n",
        "\n",
        "# generate target given source sequence\n",
        "def predict_sequence(model, tokenizer, source):\n",
        "\tprediction = model.predict(source, verbose=0)[0]\n",
        "\tintegers = [argmax(vector) for vector in prediction]\n",
        "\ttarget = list()\n",
        "\tfor i in integers:\n",
        "\t\tword = word_for_id(i, tokenizer)\n",
        "\t\tif word is None:\n",
        "\t\t\tbreak\n",
        "\t\ttarget.append(word)\n",
        "\treturn ' '.join(target)\n",
        "\n",
        "# evaluate the skill of the model\n",
        "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
        "\tactual, predicted = list(), list()\n",
        "\tfor i, source in enumerate(sources):\n",
        "\t\t# translate encoded source text\n",
        "\t\tsource = source.reshape((1, source.shape[0]))\n",
        "\t\ttranslation = predict_sequence(model, eng_tokenizer, source)\n",
        "\t\traw_target, raw_src,test = raw_dataset[i]\n",
        "\t\tif i < 10:\n",
        "\t\t\tprint('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
        "\t\tactual.append([raw_target.split()])\n",
        "\t\tpredicted.append(translation.split())\n",
        "\t# calculate BLEU score\n",
        "\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-romanian-both.pkl')\n",
        "train = load_clean_sentences('english-romanian-train.pkl')\n",
        "test = load_clean_sentences('english-romanian-test.pkl')\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "# prepare data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "\n",
        "# load model\n",
        "model = load_model('model.h5')\n",
        "# test on some training sequences\n",
        "print('train')\n",
        "evaluate_model(model, eng_tokenizer, trainX, train)\n",
        "# test on some test sequences\n",
        "print('test')\n",
        "evaluate_model(model, eng_tokenizer, testX, test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train\n",
            "src=[poti sa calaresti un cal], target=[can you ride a horse], predicted=[can you ride a horse]\n",
            "src=[tom nu a citit raportul pe care lai scris], target=[tom hasnt read the report you wrote], predicted=[tom hasnt read the report you wrote]\n",
            "src=[aceasta problema merita discutata], target=[this problem is worth discussing], predicted=[this problem is worth discussing]\n",
            "src=[esti casatorit], target=[are you married], predicted=[are you married]\n",
            "src=[esti pregatit sa faci asta], target=[are you prepared to do this], predicted=[are you prepared to do this]\n",
            "src=[mai facut sami pierd mintile], target=[you made me lose my mind], predicted=[you made me lose my mind]\n",
            "src=[nuti pot spune cine a facut asta], target=[i cant tell you who did that], predicted=[i cant tell you who did that]\n",
            "src=[ai cheltuit multi bani], target=[you spent a lot of money], predicted=[you spent a lot of money]\n",
            "src=[tom usuca farfuriile], target=[tom is drying the dishes], predicted=[tom is drying the dishes]\n",
            "src=[dansul meu favorit este tangoul], target=[my favorite dance is the tango], predicted=[my favorite dance is the tango]\n",
            "BLEU-1: 0.978171\n",
            "BLEU-2: 0.970369\n",
            "BLEU-3: 0.964908\n",
            "BLEU-4: 0.940530\n",
            "test\n",
            "src=[dam o petrecere sambata viitoare], target=[were having a party next saturday], predicted=[were having a party next saturday]\n",
            "src=[uitate sub pat], target=[look under the bed], predicted=[look under the bed]\n",
            "src=[am facut primul pas], target=[ive taken the first step], predicted=[ive taken the first step]\n",
            "src=[ii vom opri], target=[well stop them], predicted=[well stop them]\n",
            "src=[putem sa intram], target=[can we come in], predicted=[can we come in]\n",
            "src=[tatal meu a murit in vietnam], target=[my father died in vietnam], predicted=[my father died in vietnam]\n",
            "src=[am fost prezent ieri la scoala], target=[i was present at school yesterday], predicted=[i was present at school yesterday]\n",
            "src=[crezi ca acest lucru este relevant], target=[do you think this is pertinent], predicted=[do you think this is pertinent]\n",
            "src=[pulsul meu e slab], target=[my pulse is weak], predicted=[my pulse is weak]\n",
            "src=[a facut totul din bunatate], target=[he did it all out of kindness], predicted=[he did it all out of kindness]\n",
            "BLEU-1: 0.914298\n",
            "BLEU-2: 0.895632\n",
            "BLEU-3: 0.891519\n",
            "BLEU-4: 0.857968\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7y1FhcXn4eI",
        "outputId": "3dd81d13-04bc-4de3-f493-2ca8fc049f94"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# map an integer to a word\n",
        "def word_for_id(integer, tokenizer):\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == integer:\n",
        "\t\t\treturn word\n",
        "\treturn None\n",
        "\n",
        "# generate target given source sequence\n",
        "def predict_sequence(model, tokenizer, source):\n",
        "\tprediction = model.predict(source, verbose=0)[0]\n",
        "\tintegers = [argmax(vector) for vector in prediction]\n",
        "\ttarget = list()\n",
        "\tfor i in integers:\n",
        "\t\tword = word_for_id(i, tokenizer)\n",
        "\t\tif word is None:\n",
        "\t\t\tbreak\n",
        "\t\ttarget.append(word)\n",
        "\treturn ' '.join(target)\n",
        "\n",
        "# evaluate the skill of the model\n",
        "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
        "\tactual, predicted = list(), list()\n",
        "\tfor i, source in enumerate(sources):\n",
        "\t\t# translate encoded source text\n",
        "\t\tsource = source.reshape((1, source.shape[0]))\n",
        "\t\ttranslation = predict_sequence(model, eng_tokenizer, source)\n",
        "\t\traw_target, raw_src,test = raw_dataset[i]\n",
        "\t\tif i < 100:\n",
        "\t\t\tprint('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
        "\t\tactual.append([raw_target.split()])\n",
        "\t\tpredicted.append(translation.split())\n",
        "\t# calculate BLEU score\n",
        "\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-romanian-both.pkl')\n",
        "train = load_clean_sentences('english-romanian-train.pkl')\n",
        "test = load_clean_sentences('english-romanian-test.pkl')\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "# prepare data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "\n",
        "# load model\n",
        "model = load_model('model.h5')\n",
        "# test on some training sequences\n",
        "print('train')\n",
        "evaluate_model(model, eng_tokenizer, trainX, train)\n",
        "# test on some test sequences\n",
        "print('test')\n",
        "evaluate_model(model, eng_tokenizer, testX, test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train\n",
            "src=[ea are obiceiul sasi roada unghiile], target=[she has a habit of biting her nails], predicted=[she has a habit of biting her nails]\n",
            "src=[pot sa intru], target=[am i allowed to enter], predicted=[can i come in]\n",
            "src=[lui tom nui place sa fie deranjat], target=[tom doesnt like to be disturbed], predicted=[tom doesnt like to be disturbed]\n",
            "src=[nu vreau sa te induc in eroare], target=[i dont want to mislead you], predicted=[i dont want to mislead you]\n",
            "src=[avem nevoie de legi mai stricte asupra armelor], target=[we need stricter gun laws], predicted=[we need stricter gun laws]\n",
            "src=[am crezut ca tom sa pierdut], target=[i thought tom was lost], predicted=[i thought tom was lost]\n",
            "src=[fa o alta alegere], target=[make another choice], predicted=[make another choice]\n",
            "src=[tom a decedat], target=[tom passed away], predicted=[tom passed passed away]\n",
            "src=[uneori el lipseste de la scoala], target=[he is sometimes absent from school], predicted=[he is sometimes absent from school]\n",
            "src=[el a fost o povara pentru parintii sai], target=[he was a burden to his parents], predicted=[he was a burden to his parents]\n",
            "src=[ce este sub patura], target=[whats under the blanket], predicted=[whats under the blanket]\n",
            "src=[tatal meu e doctor], target=[my father is a doctor], predicted=[my father is a doctor]\n",
            "src=[tom a fost invins], target=[tom was defeated], predicted=[tom was defeated]\n",
            "src=[de ce anume ai vrea sa mergi], target=[why would you even want to go], predicted=[why would you even want to go]\n",
            "src=[tom a continuat sa vorbeasca], target=[tom continued talking], predicted=[tom continued talking]\n",
            "src=[de ce intrebi], target=[why do you ask], predicted=[why do you ask]\n",
            "src=[sunt asa de plictisit], target=[im so bored], predicted=[im so bored]\n",
            "src=[tom a fost singur in majoritatea timpului], target=[tom was alone most of the time], predicted=[tom was alone most of the time]\n",
            "src=[ne vedem luni la scoala], target=[see you monday at school], predicted=[see you monday at school]\n",
            "src=[nu am chef sa studiez acum], target=[i am unwilling to study now], predicted=[i am unwilling to study now]\n",
            "src=[imi aduc aminte de acel loc], target=[i remember that place], predicted=[i remember that place]\n",
            "src=[chiar acum nu am timp], target=[right now i dont have any time], predicted=[right now i dont have any time]\n",
            "src=[esti inginer], target=[are you an engineer], predicted=[are you an engineer]\n",
            "src=[tom a fost prins furand din magazin], target=[tom was caught shoplifting], predicted=[tom was caught shoplifting]\n",
            "src=[tom a plecat in graba], target=[tom hurried away], predicted=[tom hurried away]\n",
            "src=[lam gasit pe baiat deja adormit], target=[i found the boy fast asleep], predicted=[i found the boy fast asleep]\n",
            "src=[el este un pictor], target=[he is a painter], predicted=[he is a painter]\n",
            "src=[aceasta este proprietatea mea], target=[this is my property], predicted=[this is my property]\n",
            "src=[imi era teama ca poate am intarziat], target=[i was afraid i might be late], predicted=[i was afraid i might be late]\n",
            "src=[aceasta este o migdala], target=[this is an almond], predicted=[this is an almond]\n",
            "src=[aceasta usa permite accesul catre gradina], target=[this gate allows access to the garden], predicted=[this gate allows access to the garden]\n",
            "src=[nu putem schimba ceea ce sa intamplat], target=[we cant change what has happened], predicted=[we cant change what has happened]\n",
            "src=[esti pregatit sa faci asta], target=[are you prepared to do this], predicted=[are you prepared to do this]\n",
            "src=[nu avem nevoie de banii tai], target=[we dont need your money], predicted=[we dont need your money]\n",
            "src=[nu ma bag], target=[i am not getting involved], predicted=[i am not getting involved]\n",
            "src=[pot sami parchez masina aici], target=[can i park my car here], predicted=[can i park my car here]\n",
            "src=[fiecare caz difera], target=[every case is different], predicted=[every case is different]\n",
            "src=[fa o lista], target=[make a list], predicted=[make a list]\n",
            "src=[trebuie sa ma duc la spital], target=[i have to go to the hospital], predicted=[i have to go to the hospital]\n",
            "src=[lam cautat prin toate colturile], target=[we looked for it high and low], predicted=[we looked for it high and low]\n",
            "src=[nu esti dispus sa ajuti], target=[arent you willing to help], predicted=[arent you willing to help]\n",
            "src=[tom ia cerut lui mary sai faca un sandvis], target=[tom had mary make him a sandwich], predicted=[tom had mary make him a sandwich]\n",
            "src=[il cunosc], target=[i know him], predicted=[i know him]\n",
            "src=[ai aproape dreptate], target=[youre almost right], predicted=[youre almost right]\n",
            "src=[ce face matusa ta], target=[what does your aunt do], predicted=[what does your aunt do]\n",
            "src=[sper ca il pot convinge pe tom sa ne ajute], target=[i hope i can convince tom to help us], predicted=[i hope i can convince tom to help us]\n",
            "src=[el mia recunoscut prezenta cu o inclinare a capului], target=[he acknowledged my presence with a nod], predicted=[he acknowledged my presence with a nod]\n",
            "src=[lui i sa refuzat tratamentul medical], target=[he was refused medical treatment], predicted=[he was refused medical treatment]\n",
            "src=[a traversat frontiera], target=[he passed across the border], predicted=[he passed across the border]\n",
            "src=[cine a clipit], target=[who winked], predicted=[who winked]\n",
            "src=[a nins noaptea trecuta], target=[it snowed last night], predicted=[it snowed last night]\n",
            "src=[de ce nu se misca nava], target=[why isnt the ship moving], predicted=[why isnt the ship moving]\n",
            "src=[avem nevoie de ajutorul tau], target=[we need your help], predicted=[we need your help]\n",
            "src=[vin], target=[i am coming], predicted=[im coming coming]\n",
            "src=[nu a fost un accident], target=[it was not an accident], predicted=[it was not accident accident]\n",
            "src=[sunt destul de sigur ca am facut lucrul potrivit], target=[im pretty sure i did the right thing], predicted=[im pretty sure i did the right thing]\n",
            "src=[sunt uimita de indrazneala ta], target=[i am amazed at your audacity], predicted=[i am amazed at your audacity]\n",
            "src=[tom a fost ranit grav], target=[tom was severely injured], predicted=[tom was severely injured]\n",
            "src=[vreau sa cunosc detaliile], target=[i want to know the details], predicted=[i want to know the details]\n",
            "src=[sunt intru totul de acord], target=[i agree completely], predicted=[i totally completely]\n",
            "src=[trebuie sa plec imediat], target=[i must go at once], predicted=[i must go right away]\n",
            "src=[viata merge inainte], target=[life goes on], predicted=[life goes on]\n",
            "src=[tom va avea nevoie de un ajutor], target=[tom will need some support], predicted=[tom will need some support]\n",
            "src=[vinul a fost excelent], target=[the wine was excellent], predicted=[the wine was excellent]\n",
            "src=[cunosc boston], target=[i know boston], predicted=[i know boston]\n",
            "src=[merita], target=[its worth it], predicted=[its worth it]\n",
            "src=[ar trebui sa vorbiti direct cu tom], target=[you should talk directly to tom], predicted=[you should talk directly directly tom]\n",
            "src=[sper ca esti bine], target=[i hope youre fine], predicted=[i hope youre fine]\n",
            "src=[tom este indiferent], target=[tom is numb], predicted=[tom is numb]\n",
            "src=[tom nu este la fel de tanar ca mine], target=[tom is not as young as i am], predicted=[tom is not as young as i am]\n",
            "src=[sa minti e gresit], target=[lying is wrong], predicted=[lying is wrong]\n",
            "src=[vacanta sa terminat acum], target=[the vacation is over now], predicted=[the vacation is over now]\n",
            "src=[stiu ca am facut o greseala], target=[i know i made a mistake], predicted=[i know i made a mistake]\n",
            "src=[lui tom ia placut compania lui mary], target=[tom enjoyed marys company], predicted=[tom enjoyed marys company]\n",
            "src=[ne bucuram ca vii], target=[we are glad you are coming], predicted=[we are glad you are coming]\n",
            "src=[am construit aceasta companie], target=[i built this company], predicted=[i built this company]\n",
            "src=[nuti voi irosi timpul], target=[i wont waste your time], predicted=[i wont waste your time]\n",
            "src=[este vremea ideala pentru un picnic], target=[its ideal weather for a picnic], predicted=[its ideal weather for a picnic]\n",
            "src=[as vrea sa fiu tanar din nou], target=[i wish i was young again], predicted=[i wish i was young again]\n",
            "src=[tom a pus intrebari], target=[tom asked questions], predicted=[tom asked questions]\n",
            "src=[inginerii rezolva probleme], target=[engineers are problems solvers], predicted=[engineers are problems solvers]\n",
            "src=[tiai indeplinit obiectivele], target=[did you accomplish your goals], predicted=[did you accomplish your goals]\n",
            "src=[ei ar trebui ignorati], target=[they should be ignored], predicted=[they should be ignored]\n",
            "src=[ce vreti sa beti], target=[whatll you have to drink], predicted=[what would you like to drink]\n",
            "src=[trebuie sa invatati toata viata], target=[you must study your whole life], predicted=[you must study your whole life]\n",
            "src=[mia cazut o plomba], target=[ive lost my filling], predicted=[ive lost my filling]\n",
            "src=[asta ar fi o decizie inteleapta], target=[that would be a wise decision], predicted=[that would be a wise decision]\n",
            "src=[voi merge oriunde doresti], target=[ill go anywhere you want], predicted=[ill go anywhere you want]\n",
            "src=[aici ninge foarte rar iarna], target=[it rarely snows here in the winter], predicted=[it rarely snows here in the winter]\n",
            "src=[imi pare bine de cunostinta], target=[nice to meet you], predicted=[nice to meet you]\n",
            "src=[miam platit taxele], target=[i paid my taxes], predicted=[i paid my taxes]\n",
            "src=[ea a luat un taxi pana la muzeu], target=[she went to the museum by cab], predicted=[she went to the museum by cab]\n",
            "src=[am fost impresionat], target=[i was impressed], predicted=[i was impressed]\n",
            "src=[tom are nevoie de o operatie], target=[tom needs an operation], predicted=[tom needs an operation]\n",
            "src=[tom stie ca lui mary ii place regae], target=[tom knows mary likes reggae], predicted=[tom knows mary likes reggae]\n",
            "src=[cand este ziua ta], target=[when is your birthday], predicted=[when is your birthday]\n",
            "src=[cainele lui tom a innebunit], target=[toms dog went crazy], predicted=[toms dog went crazy]\n",
            "src=[ceata devine mai densa], target=[the fog is getting thicker], predicted=[the fog is getting thicker]\n",
            "src=[tom pare sovaitor], target=[tom looks reluctant], predicted=[tom looks reluctant]\n",
            "src=[vacanta de vara incepe in iulie], target=[the summer vacation begins in july], predicted=[summer holiday begins in july july]\n",
            "BLEU-1: 0.980668\n",
            "BLEU-2: 0.972408\n",
            "BLEU-3: 0.967054\n",
            "BLEU-4: 0.943184\n",
            "test\n",
            "src=[esti canadian], target=[are you canadian], predicted=[are you canadian]\n",
            "src=[acuzatia dumneavoastra este absurda], target=[your accusation is preposterous], predicted=[your accusation is preposterous]\n",
            "src=[tom spune ca poate face asta pentru tine], target=[tom says he can do that for you], predicted=[tom says he can do that for you]\n",
            "src=[ce curcubeu frumos], target=[what a beautiful rainbow], predicted=[what a beautiful rainbow]\n",
            "src=[nimeni nu este lipsit de griji lumesti], target=[no one is free from worldly cares], predicted=[no one is free from worldly cares]\n",
            "src=[mama mea este in bucatarie], target=[my mother is in the kitchen], predicted=[my mother is in the kitchen]\n",
            "src=[asta este o poveste frumoasa], target=[this is a beautiful story], predicted=[this is a beautiful story]\n",
            "src=[lui tom iar fi placut sa stea mai mult], target=[tom wouldve liked to stay longer], predicted=[tom wouldve liked to stay longer]\n",
            "src=[nui pot uita bunatatea], target=[i cant forget his kindness], predicted=[i cant forget his kindness]\n",
            "src=[tom este introvertit], target=[tom is introverted], predicted=[tom is introverted]\n",
            "src=[ei sunt cantareti], target=[they are singers], predicted=[they are singers]\n",
            "src=[nu intarzia la scoala], target=[dont be late for school], predicted=[dont be late for school]\n",
            "src=[unde este toaleta], target=[where are the restrooms], predicted=[where the the]\n",
            "src=[simtete ca acasa], target=[make yourself at home], predicted=[make yourself at home]\n",
            "src=[il evit pe tom], target=[im avoiding tom], predicted=[im avoiding tom]\n",
            "src=[tom isi ia serviciul in serios], target=[tom takes his job seriously], predicted=[tom takes his job seriously]\n",
            "src=[eu locuiesc la parter], target=[i live on the bottom floor], predicted=[i live on the bottom floor]\n",
            "src=[noi ii stim], target=[we know them], predicted=[we know them]\n",
            "src=[mia fost frig], target=[i felt cold], predicted=[i felt cold]\n",
            "src=[lasama sa ma gandesc un minut], target=[let me think for a minute], predicted=[let me think for a minute]\n",
            "src=[am doua pisici], target=[i have two cats], predicted=[i have two cats]\n",
            "src=[a fost joaca de copil], target=[it was childs play], predicted=[it was childs play]\n",
            "src=[stia ca nu va putea castiga], target=[he knew he could not win], predicted=[he knew he could not win]\n",
            "src=[am avut o febra foarte mare], target=[i had a very high fever], predicted=[i had a very high fever]\n",
            "src=[biciclistul a urcat dealul], target=[the cyclist climbed the hill], predicted=[the cyclist climbed the hill]\n",
            "src=[tom a ras la toate glumele mariei], target=[tom laughed at all of marys jokes], predicted=[tom laughed at all of marys jokes]\n",
            "src=[trebuie sa pastram traditiile familiei], target=[we must keep up the family traditions], predicted=[we must keep up the family traditions]\n",
            "src=[doar falsifical], target=[just fake it], predicted=[just fake it]\n",
            "src=[tom se confrunta cu o dilema], target=[tom faces a conundrum], predicted=[tom faces a conundrum]\n",
            "src=[ei cautau locuri de munca], target=[they were looking for jobs], predicted=[they were looking for jobs]\n",
            "src=[nimeni nu poate evita moartea], target=[nobody can escape death], predicted=[nobody can escape death]\n",
            "src=[tom a dorit ca asta sa fie asa], target=[tom wanted it this way], predicted=[tom wanted it this way]\n",
            "src=[tom a aplaudat], target=[tom clapped his hands], predicted=[tom clapped his hands]\n",
            "src=[nu pot vorbi franceza bine], target=[i dont speak french well], predicted=[i dont speak french well]\n",
            "src=[care este destinatia ta], target=[what is your destination], predicted=[what is your destination]\n",
            "src=[tom este un barbat chipes], target=[tom is a handsome man], predicted=[tom is a handsome man]\n",
            "src=[nu ai spalat masina inca], target=[have you washed the car yet], predicted=[have you washed the car yet]\n",
            "src=[trebuie sa ne prioritizam nevoile], target=[we have to prioritize our needs], predicted=[we have to prioritize our needs]\n",
            "src=[ignoranta intotdeauna creeaza frica], target=[ignorance always creates fear], predicted=[ignorance always creates fear]\n",
            "src=[trebuie sa am mai multa rabdare], target=[i need to be more patient], predicted=[i need to be more patient]\n",
            "src=[aveti un dictionar], target=[do you have a dictionary], predicted=[do you have a dictionary]\n",
            "src=[unde e banca], target=[wheres the bank], predicted=[wheres the bank]\n",
            "src=[poti sa inoti asai], target=[you can swim cant you], predicted=[you can swim cant you]\n",
            "src=[ne vedem maine], target=[see you tomorrow], predicted=[see you tomorrow]\n",
            "src=[tom mia spus ca nu are prea mult timp], target=[tom told me he didnt have much time], predicted=[tom told me he didnt have much time]\n",
            "src=[numi e frica de moarte], target=[i dont fear death], predicted=[i dont afraid death death]\n",
            "src=[negocierile vor tine trei zile], target=[the talks will last three days], predicted=[the talks will last three days]\n",
            "src=[autobuzul nu vine intotdeauna la timp], target=[the bus doesnt always come on time], predicted=[the bus doesnt always come on time]\n",
            "src=[chiar crezi ca tom este afara], target=[do you really think tom is outside], predicted=[do you really think tom is outside]\n",
            "src=[cineva trebuie sa fie tras la raspundere], target=[someones got to be held accountable], predicted=[someones got to be held accountable]\n",
            "src=[tom vorbeste in somn], target=[tom is talking in his sleep], predicted=[tom is talking in his sleep]\n",
            "src=[anul trecut neam dus la londra], target=[we went to london last year], predicted=[we went to london last year]\n",
            "src=[a fost extrem de istovitor], target=[it was extremely grueling], predicted=[it was extremely grueling]\n",
            "src=[noi am asteptat], target=[we waited], predicted=[we waited]\n",
            "src=[tom nu a mai zburat cu avionul], target=[tom has never been on a plane before], predicted=[tom has never been on a plane before]\n",
            "src=[ma indeamna ceva sa fac cumparaturi], target=[i have an urge to buy something], predicted=[i have an urge to buy something]\n",
            "src=[niciodata nam vazut asa ceva], target=[never have i seen such a thing], predicted=[never have i seen such a thing]\n",
            "src=[nu te retrage], target=[dont back away], predicted=[dont back away]\n",
            "src=[ei au ars hartia], target=[they burned the paper], predicted=[they burned the paper]\n",
            "src=[nu pot sa ajung devreme], target=[i can come in early], predicted=[i can come in early]\n",
            "src=[de ce il intrebi pe tom], target=[why are you asking tom], predicted=[why are you asking tom]\n",
            "src=[tom abia isi putea ascunde dezgustul], target=[tom could barely conceal his disgust], predicted=[tom could barely conceal his disgust]\n",
            "src=[cine a creat universul], target=[who created the universe], predicted=[who created the universe]\n",
            "src=[noi traim in orase diferite], target=[we live in different cities], predicted=[we live in different cities]\n",
            "src=[e prea tare], target=[its too loud], predicted=[its too loud]\n",
            "src=[esti casatorit], target=[are you married], predicted=[are you married]\n",
            "src=[sa inceapa sarbatoarea], target=[let the celebration start], predicted=[let the celebration start]\n",
            "src=[unde sa dus toata painea], target=[where did all the bread go], predicted=[where did all the bread go]\n",
            "src=[tom a observat zgaraieturi pe mainile lui mary], target=[tom noticed scratches on marys hands], predicted=[tom noticed scratches on marys hands]\n",
            "src=[teai simtit bine la cina], target=[did you have a good time at the dinner], predicted=[did you have a good time at the dinner]\n",
            "src=[mama ta stie], target=[does your mother know], predicted=[does your mother know]\n",
            "src=[nu mai pot continua sa ignor problema], target=[i cant continue to ignore the problem], predicted=[i cant continue to ignore the problem]\n",
            "src=[nu mai lucram pentru tom], target=[were no longer working for tom], predicted=[were no longer working for tom]\n",
            "src=[toata lumea a inceput sa intre in panica], target=[everybody started to panic], predicted=[everybody started to panic]\n",
            "src=[tom ia cumparat cateva flori lui mary], target=[tom bought mary some flowers], predicted=[tom bought mary some flowers]\n",
            "src=[ati putea sa o spuneti pe litere pentru mine va rog], target=[could you spell that for me please], predicted=[could you spell that for me please]\n",
            "src=[ea gateste foarte bine], target=[she cooks very well], predicted=[she cooks very well]\n",
            "src=[pentru ce este destinat acel lucru], target=[whats that thing supposed to do], predicted=[whats that thing supposed to do]\n",
            "src=[ai vrea sa mergi la o plimbare], target=[would you like to go for a walk], predicted=[would you like to go for a walk]\n",
            "src=[astazi este ziua de nastere a prietenului meu], target=[today is my friends birthday], predicted=[today is my friends birthday]\n",
            "src=[porumbelul simbolizeaza pacea], target=[the dove symbolizes peace], predicted=[the dove symbolizes peace]\n",
            "src=[el nu a existat], target=[he never existed], predicted=[he never existed]\n",
            "src=[unde e scoala ta], target=[wheres your school], predicted=[wheres is school school]\n",
            "src=[fa loc], target=[just back away], predicted=[just back away]\n",
            "src=[este un secret], target=[is it a secret], predicted=[is that a secret]\n",
            "src=[daca tu esti fericit eu sunt fericit], target=[if youre happy im happy], predicted=[if youre happy im i am]\n",
            "src=[nu e castig fara osteneala], target=[no pain no gain], predicted=[no pain no gain]\n",
            "src=[nar trebui sa fii la scoala], target=[arent you supposed to be at school], predicted=[arent you supposed to be at school]\n",
            "src=[e nebunatica], target=[shes crazy], predicted=[shes crazy]\n",
            "src=[tom stia ca mary este ocupata], target=[tom knew mary was busy], predicted=[tom knew mary was busy]\n",
            "src=[tom are gura mare], target=[toms got a big mouth], predicted=[toms got a big mouth]\n",
            "src=[de ce esti atat de nervos], target=[why are you so angry], predicted=[why are you so angry]\n",
            "src=[sunt de acord cu evaluarea dumneavoastra], target=[i agree with your assessment], predicted=[i agree with your assessment]\n",
            "src=[el poate sa vorbeasca si franceza], target=[he can also speak french], predicted=[he can also speak french]\n",
            "src=[somajul a crescut brusc], target=[unemployment rose sharply], predicted=[unemployment rose sharply]\n",
            "src=[tom e slab], target=[tom is weak], predicted=[toms weak weak]\n",
            "src=[tom nu este obisnuit cu viata de la oras], target=[tom isnt accustomed to city life], predicted=[tom isnt accustomed to city life]\n",
            "src=[beau lapte], target=[i drink milk], predicted=[i drink milk]\n",
            "src=[intotdeauna team iubit], target=[ive always loved you], predicted=[ive always loved you]\n",
            "src=[sora mea e draguta], target=[my sister is pretty], predicted=[my sister is pretty]\n",
            "BLEU-1: 0.919384\n",
            "BLEU-2: 0.900646\n",
            "BLEU-3: 0.896908\n",
            "BLEU-4: 0.864281\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zDyKJyQD6fH",
        "outputId": "1c9a72cc-1c3c-4bf6-cc46-f8de0f2d128a"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# map an integer to a word\n",
        "def word_for_id(integer, tokenizer):\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == integer:\n",
        "\t\t\treturn word\n",
        "\treturn None\n",
        "\n",
        "# generate target given source sequence\n",
        "def predict_sequence(model, tokenizer, source):\n",
        "\tprediction = model.predict(source, verbose=0)[0]\n",
        "\tintegers = [argmax(vector) for vector in prediction]\n",
        "\ttarget = list()\n",
        "\tfor i in integers:\n",
        "\t\tword = word_for_id(i, tokenizer)\n",
        "\t\tif word is None:\n",
        "\t\t\tbreak\n",
        "\t\ttarget.append(word)\n",
        "\treturn ' '.join(target)\n",
        "\n",
        "# evaluate the skill of the model\n",
        "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
        "\tactual, predicted = list(), list()\n",
        "\tfor i, source in enumerate(sources):\n",
        "\t\t# translate encoded source text\n",
        "\t\tsource = source.reshape((1, source.shape[0]))\n",
        "\t\ttranslation = predict_sequence(model, eng_tokenizer, source)\n",
        "\t\traw_target, raw_src,test = raw_dataset[i]\n",
        "\t\tif i < 10:\n",
        "\t\t\tprint('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
        "\t\tactual.append([raw_target.split()])\n",
        "\t\tpredicted.append(translation.split())\n",
        "\t# calculate BLEU score\n",
        "\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-romanian-both.pkl')\n",
        "train = load_clean_sentences('english-romanian-train.pkl')\n",
        "test = load_clean_sentences('english-romanian-test.pkl')\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "# prepare data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "\n",
        "# load model\n",
        "model = load_model('model.h5')\n",
        "# test on some training sequences\n",
        "print('train')\n",
        "evaluate_model(model, eng_tokenizer, trainX, train)\n",
        "# test on some test sequences\n",
        "print('test')\n",
        "evaluate_model(model, eng_tokenizer, testX, test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train\n",
            "src=[ea are obiceiul sasi roada unghiile], target=[she has a habit of biting her nails], predicted=[she has a habit of biting her nails]\n",
            "src=[pot sa intru], target=[am i allowed to enter], predicted=[might i come in]\n",
            "src=[lui tom nui place sa fie deranjat], target=[tom doesnt like to be disturbed], predicted=[tom doesnt like to be disturbed]\n",
            "src=[nu vreau sa te induc in eroare], target=[i dont want to mislead you], predicted=[i dont want to mislead you]\n",
            "src=[avem nevoie de legi mai stricte asupra armelor], target=[we need stricter gun laws], predicted=[we need stricter gun laws]\n",
            "src=[am crezut ca tom sa pierdut], target=[i thought tom was lost], predicted=[i thought tom was lost]\n",
            "src=[fa o alta alegere], target=[make another choice], predicted=[make another choice]\n",
            "src=[tom a decedat], target=[tom passed away], predicted=[tom has passed]\n",
            "src=[uneori el lipseste de la scoala], target=[he is sometimes absent from school], predicted=[he is sometimes absent from school]\n",
            "src=[el a fost o povara pentru parintii sai], target=[he was a burden to his parents], predicted=[he was a burden to his parents]\n",
            "BLEU-1: 0.979510\n",
            "BLEU-2: 0.971432\n",
            "BLEU-3: 0.965954\n",
            "BLEU-4: 0.941844\n",
            "test\n",
            "src=[esti canadian], target=[are you canadian], predicted=[are you canadian]\n",
            "src=[acuzatia dumneavoastra este absurda], target=[your accusation is preposterous], predicted=[your accusation is preposterous]\n",
            "src=[tom spune ca poate face asta pentru tine], target=[tom says he can do that for you], predicted=[tom says he can do that for you]\n",
            "src=[ce curcubeu frumos], target=[what a beautiful rainbow], predicted=[what a beautiful rainbow]\n",
            "src=[nimeni nu este lipsit de griji lumesti], target=[no one is free from worldly cares], predicted=[no one is free from worldly cares]\n",
            "src=[mama mea este in bucatarie], target=[my mother is in the kitchen], predicted=[my mother is in the kitchen]\n",
            "src=[asta este o poveste frumoasa], target=[this is a beautiful story], predicted=[this is a beautiful story]\n",
            "src=[lui tom iar fi placut sa stea mai mult], target=[tom wouldve liked to stay longer], predicted=[tom wouldve liked to stay longer]\n",
            "src=[nui pot uita bunatatea], target=[i cant forget his kindness], predicted=[i cant forget his kindness]\n",
            "src=[tom este introvertit], target=[tom is introverted], predicted=[tom is introverted]\n",
            "BLEU-1: 0.917704\n",
            "BLEU-2: 0.899233\n",
            "BLEU-3: 0.895238\n",
            "BLEU-4: 0.862303\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}